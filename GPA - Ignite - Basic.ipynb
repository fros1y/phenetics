{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from catalyst import dl\n",
    "from catalyst import dl, utils\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import Accuracy, Loss, RunningAverage\n",
    "from ignite.handlers import ModelCheckpoint, EarlyStopping\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "from ignite.utils import convert_tensor\n",
    "warnings.simplefilter('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import logging\n",
    "from util import *\n",
    "import torch.nn.functional as F\n",
    "from pytorch_metric_learning import miners, losses\n",
    "import catalyst.contrib as contrib\n",
    "from datetime import datetime\n",
    "import pprint\n",
    "from ignite.engine import _prepare_batch\n",
    "from ignite.engine import create_supervised_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_json(\"training_set.json.gz\", lines=True, orient=\"records\")\n",
    "testing_set = pd.read_json(\"testing_set.json.gz\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = list(sorted(set(all_tiers_100)-set([\"PersonalizedProduct\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"/home/martin/IdeaProjects/phenetics/bertForPatents/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModel.from_pretrained(model_name, gradient_checkpointing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['labels']=training_set[subset].astype(int).values.tolist()\n",
    "testing_set['labels']=testing_set[subset].astype(int).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokenizer, text, max_len):\n",
    "        text = str(text)\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            f\"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            f\"attention_mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            f\"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN_CLAIMS = 512\n",
    "MAX_LEN_ABSTRACT = 160\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-5\n",
    "SEED = 17\n",
    "PRED_THRES = 0.4\n",
    "ACCUM_STEPS = 8\n",
    "NUM_LABELS = len(subset)\n",
    "\n",
    "device = utils.get_device()\n",
    "logdir=\"/var/patentmark/logdir/fit2/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        \n",
    "        self.claims = dataframe.claims\n",
    "        self.abstracts = dataframe.abstract\n",
    "        \n",
    "        self.labels = dataframe.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        abstract = tokenize(tokenizer, self.abstracts[index], max_len=MAX_LEN_ABSTRACT)\n",
    "        claims = tokenize(tokenizer, self.claims[index], MAX_LEN_CLAIMS)\n",
    "        labels = torch.tensor(np.array(self.labels[index]), dtype=torch.\n",
    "                              float)                \n",
    "        return {\"abstract\": abstract, \n",
    "                \"claims\": claims,\n",
    "                'labels': labels}\n",
    "    \n",
    "training_dataset = MultiLabelDataset(training_set, tokenizer)\n",
    "testing_dataset = MultiLabelDataset(testing_set, tokenizer)\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_dataset, **train_params)\n",
    "testing_loader = DataLoader(testing_dataset, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.text_embedder = AutoModel.from_pretrained(model_name, gradient_checkpointing=True)\n",
    "        \n",
    "        total_embedding_size = self.text_embedder.pooler.dense.out_features*2\n",
    "        output_size = NUM_LABELS\n",
    "        bottleneck = 768\n",
    "    \n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dense1 = nn.Linear(total_embedding_size, bottleneck)\n",
    "        self.classifier = nn.Linear(bottleneck, output_size)\n",
    "\n",
    "            \n",
    "    def forward(self, abstract, claims):\n",
    "        abstract_emb = self.text_embedder(input_ids=abstract[\"input_ids\"], attention_mask=abstract[\"attention_mask\"])\n",
    "        abstract_emb = abstract_emb[0][:, 0]\n",
    "        \n",
    "        claim_emb = self.text_embedder(input_ids=claims[\"input_ids\"], attention_mask=claims[\"attention_mask\"])\n",
    "        claim_emb = claim_emb[0][:, 0]\n",
    "           \n",
    "        x = torch.cat((abstract_emb, claim_emb), 1)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.elu(self.dense1(x))\n",
    "        x = self.classifier(x)\n",
    "        x = F.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = BasicModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = torch.nn.BCELoss() #WithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.metrics import Accuracy, Precision, Recall, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-04 08:34:59 - Engine run starting with max_epochs=5.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be4da1fc05d4584b21464892d906262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=243.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-04 08:37:10 - Engine run starting with max_epochs=1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1] Loss: 0.67\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=61.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-04 08:37:35 - Epoch[1] Complete. Time taken: 00:00:25\n",
      "2020-12-04 08:37:35 - Engine run complete. Time taken: 00:00:25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Val Results - Epoch: 1 \n",
      "Metrics\n",
      "{'val_accuracy': 0.0, 'val_loss': 0.5321308877242446}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-04 08:39:50 - Engine run starting with max_epochs=1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1] Loss: 0.49\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=61.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-04 08:40:15 - Epoch[1] Complete. Time taken: 00:00:25\n",
      "2020-12-04 08:40:15 - Engine run complete. Time taken: 00:00:25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Val Results - Epoch: 1 \n",
      "Metrics\n",
      "{'val_accuracy': 0.0, 'val_loss': 0.5325767942416815}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-04 08:41:14 - Epoch[1] Complete. Time taken: 00:06:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246a9ee225cd48848e3d745960f9e32f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=243.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-04 08:42:29 - Engine run starting with max_epochs=1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[2] Loss: 0.43\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=61.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-04 08:42:54 - Epoch[1] Complete. Time taken: 00:00:25\n",
      "2020-12-04 08:42:54 - Engine run complete. Time taken: 00:00:25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Val Results - Epoch: 2 \n",
      "Metrics\n",
      "{'val_accuracy': 0.0, 'val_loss': 0.5320584902792801}\n"
     ]
    }
   ],
   "source": [
    "def basic_train_step(engine, batch):\n",
    "    model.train()\n",
    "    \n",
    "    abstract = convert_tensor(batch['abstract'], device=device)\n",
    "    claims = convert_tensor(batch['claims'], device=device)\n",
    "    labels = convert_tensor(batch['labels'], device=device)\n",
    "\n",
    "    predictions = model(abstract=abstract, claims=claims)\n",
    "    loss = loss_function(predictions, labels)\n",
    "    loss.backward()\n",
    "    \n",
    "    if engine.state.iteration % ACCUM_STEPS == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def basic_validation_step(engine, batch):\n",
    "    model.eval()\n",
    "    abstract = convert_tensor(batch['abstract'], device=device)\n",
    "    claims = convert_tensor(batch['claims'], device=device)\n",
    "    labels = convert_tensor(batch['labels'], device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        raw_predictions = model(abstract=abstract, claims=claims)\n",
    "        return raw_predictions, labels\n",
    "    \n",
    "def thresholded_output_transform(output):\n",
    "    y_pred, y = output\n",
    "    y_pred = torch.round(y_pred)\n",
    "    return y_pred, y\n",
    "\n",
    "trainer = Engine(basic_train_step)\n",
    "\n",
    "RunningAverage(output_transform=lambda x: x).attach(trainer, 'loss')\n",
    "pbar = ProgressBar(persist=True, bar_format=\"\")\n",
    "pbar.attach(trainer, ['loss'])\n",
    "\n",
    "log_interval = 100\n",
    "@trainer.on(Events.ITERATION_COMPLETED(every=log_interval))\n",
    "def log_training_loss(trainer):\n",
    "    print(\"Epoch[{}] Loss: {:.2f}\".format(trainer.state.epoch, trainer.state.output))\n",
    "\n",
    "evaluator = Engine(basic_validation_step)\n",
    "Accuracy(output_transform=thresholded_output_transform, is_multilabel=True).attach(evaluator, 'val_accuracy')\n",
    "Loss(loss_function).attach(evaluator, 'val_loss')\n",
    "\n",
    "pbar = ProgressBar(persist=False, bar_format=\"\")\n",
    "pbar.attach(evaluator)\n",
    "# Precision(output_transform=thresholded_output_transform, is_multilabel=True, average=True).attach(evaluator, 'val_precision')\n",
    "# #Accuracy(output_transform=thresholded_output_transform, is_multilabel=True, labelwise=True).attach(evaluator, 'val_label_acc')\n",
    "\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED(every=log_interval))\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(testing_loader)\n",
    "    pbar.log_message(\"Running validation\")\n",
    "    \n",
    "    metrics = evaluator.state.metrics\n",
    "    pbar.log_message(\n",
    "        \"Val Results - Epoch: {} \\nMetrics\\n{}\"\n",
    "        .format(engine.state.epoch, pprint.pformat(metrics)))\n",
    "\n",
    "trainer.run(training_loader, max_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# #train_evaluator = Engine(validation_step)\n",
    "# test_evaluator = Engine(validation_step)\n",
    "\n",
    "# #Accuracy(is_multilabel=True, output_transform=thresholded_output_transform).attach(train_evaluator, 'train_accuracy')\n",
    "# #Loss(classifier_function).attach(train_evaluator, 'train_loss')\n",
    "# Accuracy(is_multilabel=True, output_transform=thresholded_output_transform).attach(test_evaluator, 'val_accuracy')\n",
    "# Loss(classifier_function).attach(test_evaluator, 'val_loss')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def log_validation_results(engine):\n",
    "#     validation_evaluator.run(valid_iterator)\n",
    "#     metrics = validation_evaluator.state.metrics\n",
    "#     avg_accuracy = metrics['accuracy']\n",
    "#     avg_bce = metrics['bce']\n",
    "#     pbar.log_message(\n",
    "#         \"Validation Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\"\n",
    "#         .format(engine.state.epoch, avg_accuracy, avg_bce))\n",
    "#     pbar.n = pbar.last_print_n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# log_interval = 10\n",
    "# @trainer.on(Events.ITERATION_COMPLETED(every=log_interval))\n",
    "# def log_validation_results(trainer):\n",
    "#     print(\"running validation\")\n",
    "#     evaluator.run(testing_loader)\n",
    "#     metrics = evaluator.state.metrics\n",
    "#     print(\"Validation Results - Epoch: {}  Avg accuracy: {:.2f}\"\n",
    "#           .format(trainer.state.epoch, metrics[\"accuracy\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
