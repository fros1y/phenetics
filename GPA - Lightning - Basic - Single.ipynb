{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import logging\n",
    "from util import *\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "import pprint\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import util\n",
    "#from catalyst.metrics.functional import process_multilabel_components\n",
    "#from catalyst.metrics import multi_label_accuracy\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.core.decorators import auto_move_data\n",
    "from sklearn.metrics import label_ranking_average_precision_score, accuracy_score, f1_score\n",
    "#from ignite.utils import convert_tensor\n",
    "#from pytorch_metric_learning.miners import MultiSimilarityMiner\n",
    "#from pytorch_metric_learning.losses import TripletMarginLoss\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set = pd.read_json(\"training_set.json.gz\", lines=True, orient=\"records\")\n",
    "# testing_set = pd.read_json(\"testing_set.json.gz\", lines=True, orient=\"records\")\n",
    "\n",
    "# subset = list(sorted(set(all_tiers_100)-set([\"PersonalizedProduct\"])))\n",
    "\n",
    "# nice_subset = [tier_translations[x] for x in subset]\n",
    "\n",
    "# #model_name = \"/home/martin/IdeaProjects/phenetics/bertForPatents/\"\n",
    "# model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# #base_model = AutoModel.from_pretrained(model_name, gradient_checkpointing=True)\n",
    "\n",
    "# import util\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "\n",
    "\n",
    "# training_labels = training_set[subset].apply(util.array_labels_textual, axis=1)\n",
    "# testing_labels = testing_set[subset].apply(util.array_labels_textual, axis=1)\n",
    "# all_labels = np.concatenate((training_labels, testing_labels))\n",
    "# enc = LabelEncoder()\n",
    "# enc.fit(all_labels)\n",
    "# training_set['labels'] = enc.transform(training_labels).astype(np.int64).tolist()\n",
    "# testing_set['labels'] = enc.transform(testing_labels).astype(np.int64).tolist()\n",
    "\n",
    "\n",
    "\n",
    "# training_set['labels']\n",
    "\n",
    "# mlb = MultiLabelBinarizer()\n",
    "\n",
    "# #training_set['labels']=\n",
    "# training_labels = mlb.fit_transform(training_set[subset].apply(util.array_labels, axis=1)).astype(np.float32)\n",
    "# testing_labels = mlb.transform(testing_set[subset].apply(util.array_labels, axis=1)).astype(np.float32)\n",
    "# #testing_set['labels']=testing_set[subset].astype(int).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set['labels'] = training_labels.tolist()\n",
    "# testing_set['labels'] = testing_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatentDataset(Dataset):\n",
    "\n",
    "    \n",
    "    def __init__(self, dataframe, tokenizer, abstract_max_len=160, claims_max_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.claims_max_len = claims_max_len\n",
    "        self.abstract_max_len = abstract_max_len\n",
    "\n",
    "        self.claims = dataframe.claims\n",
    "        self.abstracts = dataframe.abstract        \n",
    "        self.labels = dataframe.labels\n",
    "\n",
    "    def tokenize(self, text, max_len):\n",
    "        text = str(text)\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            f\"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            f\"attention_mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            f\"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        abstract = self.tokenize(self.abstracts[index], max_len=self.abstract_max_len)\n",
    "        claims = self.tokenize(self.claims[index], max_len=self.claims_max_len)\n",
    "        labels = torch.tensor(self.labels[index])\n",
    "        return {\"abstract\": abstract, \n",
    "                \"claims\": claims,\n",
    "                'labels': labels}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack(listOfDicts):\n",
    "    initDict = listOfDicts[0]\n",
    "    finalDict = {}\n",
    "    for key in initDict.keys():\n",
    "        tensors = tuple(d[key] for d in listOfDicts)\n",
    "        finalDict[key] = torch.stack(tensors)\n",
    "    return finalDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AnalysisAndModeling',\n",
       " 'AnatomicalTarget',\n",
       " 'Imaging',\n",
       " 'Manufacturing',\n",
       " 'SpecificationofUse',\n",
       " 'SurgicalMethod'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(tier1) & set(all_tiers_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.metrics import functional as FM\n",
    "\n",
    "\n",
    "class BasicSystem(pl.LightningModule):\n",
    "    def __init__(self, batch_size=16, learning_rate = 1e-5):\n",
    "        super().__init__()\n",
    "        \n",
    "        #self.model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "        self.model_name = \"bertForPatents/\"\n",
    "    \n",
    "        filtered = set(all_tiers_100)-set([\"PersonalizedProduct\"])\n",
    "        self.subset = list(sorted(set(tier1) & set(filtered)))\n",
    "        #self.subset = list(sorted(set(all_tiers_100)-set([\"PersonalizedProduct\"])))16\n",
    "        \n",
    "        \n",
    "        self.loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        #torch.nn.MSELoss() #torch.nn.MultiLabelSoftMarginLoss()  #torch.nn.BCEWithLogitsLoss() #torch.nn.MultiLabelSoftMarginLoss() #torch.nn.CrossEntropyLoss()\n",
    "        #self.softmax = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "        self.train_accuracy = pl.metrics.Accuracy()\n",
    "        self.val_accuracy = pl.metrics.Accuracy()\n",
    "        \n",
    "        self.train_accuracy = pl.metrics.Accuracy()\n",
    "        self.train_f1 = pl.metrics.F1()\n",
    "        \n",
    "        self.val_accuracy = pl.metrics.Accuracy()\n",
    "        self.val_f1 = pl.metrics.F1()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        logdir=\"/var/patentmark/logdir/fit2/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        self.training_set = pd.read_json(\"training_set.json.gz\", lines=True, orient=\"records\")\n",
    "        self.testing_set = pd.read_json(\"testing_set.json.gz\", lines=True, orient=\"records\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        \n",
    "    def setup(self,stage):\n",
    "\n",
    "        training_labels = self.training_set[self.subset].apply(util.array_labels, axis=1)\n",
    "        testing_labels = self.testing_set[self.subset].apply(util.array_labels, axis=1)\n",
    "        all_labels = np.concatenate((training_labels, testing_labels))\n",
    "        \n",
    "        self.label_encoder = MultiLabelBinarizer()\n",
    "        self.label_encoder.fit(all_labels)\n",
    "        print(f\"Labels: {self.label_encoder.classes_}\")\n",
    "        \n",
    "        self.training_set['labels'] = self.label_encoder.transform(training_labels).astype(np.float).tolist()\n",
    "        self.testing_set['labels'] = self.label_encoder.transform(testing_labels).astype(np.float).tolist()\n",
    "        \n",
    "        self.training_dataset = PatentDataset(self.training_set, self.tokenizer)\n",
    "        self.testing_dataset = PatentDataset(self.testing_set, self.tokenizer)\n",
    "        \n",
    "        self.text_embedder = AutoModel.from_pretrained(self.model_name, gradient_checkpointing=True)\n",
    "        \n",
    "        total_embedding_size = self.text_embedder.pooler.dense.out_features*2\n",
    "        output_size = len(self.label_encoder.classes_)\n",
    "        bottleneck = 256\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.bottleneck = nn.Linear(total_embedding_size, bottleneck)\n",
    "        self.classifier = nn.Linear(bottleneck, output_size)\n",
    "\n",
    "    \n",
    "    def embed_patent(self, abstract, claims):\n",
    "        abstract_emb = self.text_embedder(input_ids=abstract[\"input_ids\"], attention_mask=abstract[\"attention_mask\"])\n",
    "        abstract_emb = abstract_emb[1]\n",
    "        \n",
    "        claim_emb = self.text_embedder(input_ids=claims[\"input_ids\"], attention_mask=claims[\"attention_mask\"])\n",
    "        claim_emb = claim_emb[1]\n",
    "    \n",
    "        x = torch.cat((abstract_emb, claim_emb), 1)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.bottleneck(x)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def classify_patent(self, embedding):\n",
    "        x = embedding\n",
    "        x = self.dropout1(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    @auto_move_data\n",
    "    def forward(self, abstract, claims):\n",
    "        x = self.embed_patent(abstract, claims)\n",
    "        x = self.classify_patent(x)\n",
    "        return x\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.training_dataset, batch_size=self.batch_size, shuffle=True, num_workers=32)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.testing_dataset, batch_size=self.batch_size, shuffle=False, num_workers=32)\n",
    "            \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        abstract = batch['abstract']\n",
    "        claims = batch['claims']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        embedding = self.embed_patent(abstract, claims)\n",
    "        logits = self.classify_patent(embedding)\n",
    "        loss = self.loss_function(logits, labels)\n",
    "        \n",
    "        #acc = FM.accuracy(F.sigmoilogits, labels)\n",
    "        #self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        #self.log(\"train_acc\", acc, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        \n",
    "        #self.log(\"train_loss\", loss, prog_bar=True)       \n",
    "        \n",
    "        #predictions = torch.argmax(logits, dim=1)\n",
    "        #self.log(\"train_f1\", f1_score(labels.detach().cpu(), predictions.detach().cpu(), average=\"weighted\"), prog_bar=True, on_epoch=True, on_step=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        abstract = batch['abstract']\n",
    "        claims = batch['claims']\n",
    "        labels = batch['labels']\n",
    "    \n",
    "        embedding = self.embed_patent(abstract, claims)\n",
    "        logits = self.classify_patent(embedding)\n",
    "        loss = self.loss_function(logits, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        \n",
    "\n",
    "        #acc = FM.accuracy(F.sigmoid(logits), labels)\n",
    "        #self.log(\"val_acc\", acc, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        \n",
    "        predictions = (F.sigmoid(logits).cpu().detach().numpy() >= 0.5).tolist()\n",
    "        labels = labels.cpu().detach().numpy().astype(np.bool).tolist()\n",
    "        \n",
    "        return predictions, labels\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        predictions = list(f.cat(x[0] for x in outputs))\n",
    "        labels = list(f.cat(x[1] for x in outputs))\n",
    "        print(classification_report(labels, predictions, target_names=self.label_encoder.classes_))\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "#         param_optimizer = list(self.named_parameters())\n",
    "#         no_decay = [\"bias\", \"gamma\", \"beta\"]\n",
    "#         optimizer_grouped_parameters = [\n",
    "#                 {\n",
    "#                     \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "#                     \"weight_decay_rate\": 0.01\n",
    "#                     },\n",
    "#                 {\n",
    "#                     \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "#                     \"weight_decay_rate\": 0.0\n",
    "#                     },\n",
    "#                 ]\n",
    "#         optimizer = torch.optim.AdamW(\n",
    "#                 optimizer_grouped_parameters,\n",
    "#                 lr=self.learning_rate,\n",
    "#                 )\n",
    "#         return optimizer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "2020-12-05 14:52:01 - GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "2020-12-05 14:52:01 - TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2020-12-05 14:52:01 - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Using native 16bit precision.\n",
      "2020-12-05 14:52:01 - Using native 16bit precision.\n"
     ]
    }
   ],
   "source": [
    "model = BasicSystem()\n",
    "early_stopping = EarlyStopping('val_loss')\n",
    "trainer = pl.Trainer(gpus=1, accumulate_grad_batches=1,\n",
    "                     callbacks=[early_stopping],\n",
    "                     precision=16,\n",
    "                     auto_scale_batch_size=True,\n",
    "                     #auto_lr_find=True,\n",
    "                     log_every_n_steps=1, \n",
    "                     flush_logs_every_n_steps=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.tune(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['Analysis and Modeling' 'Anatomical Target' 'Imaging' 'Manufacturing'\n",
      " 'Specification of Use' 'Surgical Method']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name           | Type              | Params\n",
      "-----------------------------------------------------\n",
      "0 | loss_function  | BCEWithLogitsLoss | 0     \n",
      "1 | train_accuracy | Accuracy          | 0     \n",
      "2 | val_accuracy   | Accuracy          | 0     \n",
      "3 | train_f1       | F1                | 0     \n",
      "4 | val_f1         | F1                | 0     \n",
      "5 | text_embedder  | BertModel         | 344 M \n",
      "6 | dropout1       | Dropout           | 0     \n",
      "7 | bottleneck     | Linear            | 524 K \n",
      "8 | classifier     | Linear            | 1.5 K \n",
      "2020-12-05 14:52:17 - \n",
      "  | Name           | Type              | Params\n",
      "-----------------------------------------------------\n",
      "0 | loss_function  | BCEWithLogitsLoss | 0     \n",
      "1 | train_accuracy | Accuracy          | 0     \n",
      "2 | val_accuracy   | Accuracy          | 0     \n",
      "3 | train_f1       | F1                | 0     \n",
      "4 | val_f1         | F1                | 0     \n",
      "5 | text_embedder  | BertModel         | 344 M \n",
      "6 | dropout1       | Dropout           | 0     \n",
      "7 | bottleneck     | Linear            | 524 K \n",
      "8 | classifier     | Linear            | 1.5 K \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "Analysis and Modeling       0.44      1.00      0.61        14\n",
      "    Anatomical Target       0.00      0.00      0.00        22\n",
      "              Imaging       0.00      0.00      0.00        18\n",
      "        Manufacturing       0.00      0.00      0.00        12\n",
      " Specification of Use       0.31      1.00      0.48        10\n",
      "      Surgical Method       0.60      0.38      0.46         8\n",
      "\n",
      "            micro avg       0.39      0.32      0.35        84\n",
      "            macro avg       0.23      0.40      0.26        84\n",
      "         weighted avg       0.17      0.32      0.20        84\n",
      "          samples avg       0.40      0.31      0.33        84\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b59765d0cd4afb87147f5264458b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "Analysis and Modeling       0.00      0.00      0.00        84\n",
      "    Anatomical Target       0.67      1.00      0.81       164\n",
      "              Imaging       0.58      0.96      0.72       133\n",
      "        Manufacturing       0.00      0.00      0.00        83\n",
      " Specification of Use       0.00      0.00      0.00        79\n",
      "      Surgical Method       0.00      0.00      0.00        40\n",
      "\n",
      "            micro avg       0.63      0.50      0.56       583\n",
      "            macro avg       0.21      0.33      0.25       583\n",
      "         weighted avg       0.32      0.50      0.39       583\n",
      "          samples avg       0.64      0.53      0.55       583\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "Analysis and Modeling       0.00      0.00      0.00        84\n",
      "    Anatomical Target       0.68      0.99      0.80       164\n",
      "              Imaging       0.57      0.96      0.72       133\n",
      "        Manufacturing       0.33      0.07      0.12        83\n",
      " Specification of Use       0.00      0.00      0.00        79\n",
      "      Surgical Method       0.00      0.00      0.00        40\n",
      "\n",
      "            micro avg       0.62      0.51      0.56       583\n",
      "            macro avg       0.26      0.34      0.27       583\n",
      "         weighted avg       0.37      0.51      0.41       583\n",
      "          samples avg       0.62      0.54      0.54       583\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "Analysis and Modeling       0.44      0.10      0.16        84\n",
      "    Anatomical Target       0.71      0.93      0.80       164\n",
      "              Imaging       0.58      0.83      0.68       133\n",
      "        Manufacturing       0.37      0.34      0.35        83\n",
      " Specification of Use       0.00      0.00      0.00        79\n",
      "      Surgical Method       1.00      0.17      0.30        40\n",
      "\n",
      "            micro avg       0.60      0.52      0.56       583\n",
      "            macro avg       0.52      0.39      0.38       583\n",
      "         weighted avg       0.52      0.52      0.47       583\n",
      "          samples avg       0.62      0.56      0.54       583\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "Analysis and Modeling       0.00      0.00      0.00        84\n",
      "    Anatomical Target       0.67      1.00      0.81       164\n",
      "              Imaging       0.62      0.70      0.65       133\n",
      "        Manufacturing       0.43      0.18      0.25        83\n",
      " Specification of Use       0.00      0.00      0.00        79\n",
      "      Surgical Method       0.33      0.20      0.25        40\n",
      "\n",
      "            micro avg       0.62      0.48      0.54       583\n",
      "            macro avg       0.34      0.35      0.33       583\n",
      "         weighted avg       0.41      0.48      0.43       583\n",
      "          samples avg       0.65      0.52      0.54       583\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstracts = stack(list(tokenize(tokenizer, x, max_len=MAX_LEN_ABSTRACT) for x in testing_set.abstract))\n",
    "\n",
    "# claims = stack(list(tokenize(tokenizer, x, max_len=MAX_LEN_CLAIMS) for x in testing_set.claims))\n",
    "\n",
    "# predictions = model.forward(abstract=abstracts, claims=claims)\n",
    "\n",
    "# binarized = predictions.detach().numpy() > 0.5\n",
    "\n",
    "# from sklearn.metrics import classification_report\n",
    "# testing_labels = testing_set[subset]\n",
    "# print(classification_report(testing_labels, binarized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
