{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel, Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from sklearn import metrics\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import logging\n",
    "from util import *\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "import pprint\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import util\n",
    "#from catalyst.metrics.functional import process_multilabel_components\n",
    "#from catalyst.metrics import multi_label_accuracy\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.core.decorators import auto_move_data\n",
    "from sklearn.metrics import label_ranking_average_precision_score, accuracy_score, f1_score\n",
    "#from ignite.utils import convert_tensor\n",
    "#from pytorch_metric_learning.miners import MultiSimilarityMiner\n",
    "#from pytorch_metric_learning.losses import TripletMarginLoss\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokenizer, text, label, max_len):\n",
    "        text = str(text)\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            label,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            f\"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            f\"attention_mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            f\"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "        }\n",
    "    \n",
    "\n",
    "class PatentDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, abstract_max_len=160, claims_max_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.claims_max_len = claims_max_len\n",
    "        self.abstract_max_len = abstract_max_len\n",
    "\n",
    "        self.claims = dataframe.claims\n",
    "        self.abstracts = dataframe.abstract        \n",
    "        self.labels = dataframe.labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        abstract = self.abstracts[index] #self.tokenize(self.abstracts[index], max_len=self.abstract_max_len)\n",
    "        claims = self.claims[index] #self.tokenize(self.claims[index], max_len=self.claims_max_len)\n",
    "        labels = self.labels[index]\n",
    "        #embedded_cpc = torch.tensor(np.array(self.data.embedded_cpc[index]), dtype=torch.float)\n",
    "        \n",
    "        #labels = torch.tensor(self.labels[index])\n",
    "        return {\"abstract\": abstract, \n",
    "                \"claims\": claims,\n",
    "                #'embedded_cpc': embedded_cpc,\n",
    "                'labels': labels}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "# def linear_combination(x, y, epsilon): \n",
    "#     return epsilon*x + (1-epsilon)*y\n",
    "\n",
    "# def reduce_loss(loss, reduction='mean'):\n",
    "#     return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss\n",
    "\n",
    "# class LabelSmoothingCrossEntropy(nn.Module):\n",
    "#     def __init__(self, pos_weights, epsilon:float=0.1, reduction='mean'):\n",
    "#         super().__init__()\n",
    "#         self.epsilon = epsilon\n",
    "#         self.reduction = reduction\n",
    "#         self.pos_weights = pos_weights\n",
    "    \n",
    "#     def forward(self, preds, target):\n",
    "#         n = preds.size()[-1]\n",
    "#         #log_preds = F.log_softmax(preds, dim=-1)\n",
    "#         loss = reduce_loss(-preds.sum(dim=-1), self.reduction)\n",
    "#         nll = F.binary_cross_entropy_with_logits(preds, target, pos_weight=self.pos_weights.to(\"cuda:0\"), reduction=self.reduction)\n",
    "#         return linear_combination(loss/n, nll, self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.metrics import functional as FM\n",
    "\n",
    "\n",
    "class BasicSystem(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                     batch_size=16, \n",
    "                     model_name=\"bertForPatents/\", \n",
    "                     gradient_checkpointing=True, \n",
    "                     learning_rate = 3e-5, \n",
    "                     subset=list(sorted(set(all_tiers_100)-set([\"PersonalizedProduct\"])))):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.subset = subset\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        self.training_set = pd.read_json(\"training_set.json.gz\", lines=True, orient=\"records\")\n",
    "        self.testing_set = pd.read_json(\"testing_set.json.gz\", lines=True, orient=\"records\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        \n",
    "#         self.cpc_embeddings = np.fromfile(\"/home/martin/patentmark/cpc.node2vec.emb.32d.bin\", dtype=np.float32).reshape((-1,32))\n",
    "#         self.cpc_labelizer = joblib.load('./node2id.joblib')\n",
    "#         self.cpc_lookup = {c: n for n, c in enumerate(self.cpc_labelizer.classes_)}\n",
    "\n",
    "#     @f.collecting\n",
    "#     def convert_cpc_codes(self, codes):\n",
    "#         for code in codes:\n",
    "#             if code in self.cpc_lookup:\n",
    "#                 yield self.cpc_lookup[code]\n",
    "    \n",
    "#     def embed_cpc_codes(self,codes):\n",
    "#         embedding = np.zeros(32)\n",
    "#         converted = self.convert_cpc_codes(codes)\n",
    "\n",
    "#         if not converted:\n",
    "#             return embedding\n",
    "\n",
    "#         for code_id in converted:\n",
    "#             embedding = embedding + self.cpc_embeddings[code_id]\n",
    "\n",
    "#         return embedding / len(converted)\n",
    "    \n",
    "        \n",
    "    def setup(self,stage):\n",
    "\n",
    "        training_labels = self.training_set[self.subset].apply(util.array_labels, axis=1)\n",
    "        testing_labels = self.testing_set[self.subset].apply(util.array_labels, axis=1)\n",
    "        \n",
    "        self.training_set['labels'] = training_labels\n",
    "        self.testing_set['labels'] = testing_labels\n",
    "        \n",
    "        #self.training_set['embedded_cpc'] = self.training_set.cpc_codes.apply(self.embed_cpc_codes)\n",
    "        #self.testing_set['embedded_cpc'] = self.testing_set.cpc_codes.apply(self.embed_cpc_codes)\n",
    "        \n",
    "        self.training_dataset = PatentDataset(self.training_set, self.tokenizer)\n",
    "        self.testing_dataset = PatentDataset(self.testing_set, self.tokenizer)\n",
    "        \n",
    "        self.setup_classifier()\n",
    "        \n",
    "\n",
    "    def test_label(self, text, label):\n",
    "        nice_label = tier_translations[label]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            nice_label,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        cont_reps, pooler_output = self.text_embedder(inputs['input_ids'], inputs['attn_masks'], inputs['token_type_ids'])\n",
    "        logits = self.cls_layer(self.dropout(pooler_output))\n",
    "\n",
    "    def setup_classifier(self):\n",
    "        self.text_embedder = AutoModel.from_pretrained(self.model_name, gradient_checkpointing=self.gradient_checkpointing)\n",
    "        self.cls_layer = nn.Linear(1024, 1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    \n",
    "    @auto_move_data\n",
    "    def forward(self, abstract, claims):\n",
    "        \n",
    "        \n",
    "        for label in subset:\n",
    "            predictions\n",
    "            \n",
    "        x = self.embed_patent(abstract, claims, embedded_cpc)\n",
    "        x = self.classify_patent(x)\n",
    "        return x\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.training_dataset, batch_size=self.batch_size, shuffle=True, num_workers=32)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.testing_dataset, batch_size=self.batch_size, shuffle=False, num_workers=32)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.val_dataloader()\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        abstract = batch['abstract']\n",
    "        claims = batch['claims']\n",
    "        embedded_cpc = batch['embedded_cpc']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        embedding = self.embed_patent(abstract, claims, embedded_cpc)\n",
    "        logits = self.classify_patent(embedding)\n",
    "        loss = self.loss_function(logits, labels)\n",
    "        self.log(\"traing_loss\", loss, prog_bar=True)\n",
    " \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        abstract = batch['abstract']\n",
    "        claims = batch['claims']\n",
    "        embedded_cpc = batch['embedded_cpc']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "    \n",
    "        embedding = self.embed_patent(abstract, claims, embedded_cpc)\n",
    "        logits = self.classify_patent(embedding)\n",
    "        loss = self.loss_function(logits, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        \n",
    "        predictions = (F.sigmoid(logits).cpu().detach().numpy() >= 0.5).tolist()\n",
    "        labels = labels.cpu().detach().numpy().astype(np.bool).tolist()\n",
    "        \n",
    "        return predictions, labels\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        predictions = list(f.cat(x[0] for x in outputs))\n",
    "        labels = list(f.cat(x[1] for x in outputs))\n",
    "        self.log(\"val_f1\", f1_score(labels, predictions, average=\"samples\"), prog_bar=True)\n",
    "        print(classification_report(labels, predictions, target_names=self.label_encoder.classes_))\n",
    "        \n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "2020-12-07 07:01:20 - GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "2020-12-07 07:01:20 - TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2020-12-07 07:01:20 - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "model = BasicSystem()\n",
    "early_stopping = EarlyStopping('val_f1', mode=\"max\", verbose=True, patience=3)\n",
    "#early_stopping = EarlyStopping('loss', mode=\"min\", verbose=True, patience=3)\n",
    "trainer = pl.Trainer(gpus=1,\n",
    "                     #overfit_batches=5,\n",
    "                     callbacks=[early_stopping],\n",
    "                     #precision=16,\n",
    "                     #auto_scale_batch_size=True,\n",
    "                     #auto_lr_find=True,\n",
    "                     log_every_n_steps=5,\n",
    "                     #val_check_interval=100000,\n",
    "                     #limit_val_batches=0.0,\n",
    "                     flush_logs_every_n_steps=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.tune(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                        | Type              | Params\n",
      "------------------------------------------------------------------\n",
      "0 | text_embedder               | BertModel         | 344 M \n",
      "1 | embedding_dropout_layer     | Dropout           | 0     \n",
      "2 | embedding_layer             | Linear            | 1.1 M \n",
      "3 | preclassifier_dropout_layer | Dropout           | 0     \n",
      "4 | preclassifier_layer         | Linear            | 131 K \n",
      "5 | classifier_layer            | Linear            | 5.7 K \n",
      "6 | loss_function               | BCEWithLogitsLoss | 0     \n",
      "2020-12-07 07:01:30 - \n",
      "  | Name                        | Type              | Params\n",
      "------------------------------------------------------------------\n",
      "0 | text_embedder               | BertModel         | 344 M \n",
      "1 | embedding_dropout_layer     | Dropout           | 0     \n",
      "2 | embedding_layer             | Linear            | 1.1 M \n",
      "3 | preclassifier_dropout_layer | Dropout           | 0     \n",
      "4 | preclassifier_layer         | Linear            | 131 K \n",
      "5 | classifier_layer            | Linear            | 5.7 K \n",
      "6 | loss_function               | BCEWithLogitsLoss | 0     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               precision    recall  f1-score   support\n",
      "\n",
      "                        Analysis and Modeling       0.00      0.00      0.00        14\n",
      "           Analysis and Modeling: 3D Modeling       0.27      0.36      0.31        11\n",
      "                            Anatomical Target       0.00      0.00      0.00        22\n",
      "           Anatomical Target: Lower Extremity       0.48      1.00      0.65        15\n",
      "     Anatomical Target: Lower Extremity - Hip       0.16      1.00      0.27         5\n",
      "    Anatomical Target: Lower Extremity - Knee       0.00      0.00      0.00        10\n",
      "                     Anatomical Target: Torso       0.00      0.00      0.00         2\n",
      "             Anatomical Target: Torso - Spine       0.00      0.00      0.00         1\n",
      "           Anatomical Target: Upper Extremity       0.16      1.00      0.27         5\n",
      "Anatomical Target: Upper Extremity - Shoulder       0.16      1.00      0.28         5\n",
      "                                      Imaging       0.58      1.00      0.73        18\n",
      "                                  Imaging: CT       0.24      0.62      0.34         8\n",
      "                                 Imaging: MRI       0.22      1.00      0.36         7\n",
      "                          Imaging: Ultrasound       0.16      1.00      0.27         5\n",
      "                                Manufacturing       0.00      0.00      0.00        12\n",
      "        Manufacturing: Additive Manufacturing       0.06      1.00      0.12         2\n",
      "           Personalized Product: Guide or Jig       0.00      0.00      0.00        16\n",
      "                Personalized Product: Implant       0.53      1.00      0.69        17\n",
      "                         Specification of Use       0.25      0.20      0.22        10\n",
      "                Specification of Use: Disease       0.00      0.00      0.00         5\n",
      "      Specification of Use: Joint Replacement       0.00      0.00      0.00         4\n",
      "                              Surgical Method       0.00      0.00      0.00         8\n",
      "\n",
      "                                    micro avg       0.27      0.45      0.34       202\n",
      "                                    macro avg       0.15      0.46      0.21       202\n",
      "                                 weighted avg       0.19      0.45      0.25       202\n",
      "                                  samples avg       0.27      0.45      0.32       202\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ca390a1ea04f3b9c5175c200771e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               precision    recall  f1-score   support\n",
      "\n",
      "                        Analysis and Modeling       0.35      1.00      0.51        84\n",
      "           Analysis and Modeling: 3D Modeling       0.29      1.00      0.45        71\n",
      "                            Anatomical Target       0.67      1.00      0.81       164\n",
      "           Anatomical Target: Lower Extremity       0.47      1.00      0.63       113\n",
      "     Anatomical Target: Lower Extremity - Hip       0.16      1.00      0.28        40\n",
      "    Anatomical Target: Lower Extremity - Knee       0.34      1.00      0.50        82\n",
      "                     Anatomical Target: Torso       0.14      0.97      0.25        35\n",
      "             Anatomical Target: Torso - Spine       0.08      0.86      0.15        21\n",
      "           Anatomical Target: Upper Extremity       0.13      0.90      0.23        31\n",
      "Anatomical Target: Upper Extremity - Shoulder       0.00      0.00      0.00        23\n",
      "                                      Imaging       0.55      1.00      0.71       133\n",
      "                                  Imaging: CT       0.24      1.00      0.39        59\n",
      "                                 Imaging: MRI       0.24      1.00      0.39        59\n",
      "                          Imaging: Ultrasound       0.14      1.00      0.24        32\n",
      "                                Manufacturing       0.34      1.00      0.51        83\n",
      "        Manufacturing: Additive Manufacturing       0.14      0.82      0.24        38\n",
      "           Personalized Product: Guide or Jig       0.49      1.00      0.66       120\n",
      "                Personalized Product: Implant       0.51      1.00      0.68       124\n",
      "                         Specification of Use       0.33      1.00      0.49        79\n",
      "                Specification of Use: Disease       0.13      1.00      0.22        30\n",
      "      Specification of Use: Joint Replacement       0.18      1.00      0.31        44\n",
      "                              Surgical Method       0.16      1.00      0.28        40\n",
      "\n",
      "                                    micro avg       0.29      0.98      0.45      1505\n",
      "                                    macro avg       0.28      0.93      0.41      1505\n",
      "                                 weighted avg       0.37      0.98      0.52      1505\n",
      "                                  samples avg       0.29      0.98      0.43      1505\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               precision    recall  f1-score   support\n",
      "\n",
      "                        Analysis and Modeling       0.35      1.00      0.51        84\n",
      "           Analysis and Modeling: 3D Modeling       0.29      1.00      0.45        71\n",
      "                            Anatomical Target       0.67      1.00      0.81       164\n",
      "           Anatomical Target: Lower Extremity       0.47      1.00      0.63       113\n",
      "     Anatomical Target: Lower Extremity - Hip       0.17      0.75      0.28        40\n",
      "    Anatomical Target: Lower Extremity - Knee       0.34      1.00      0.50        82\n",
      "                     Anatomical Target: Torso       0.14      0.97      0.25        35\n",
      "             Anatomical Target: Torso - Spine       0.08      0.81      0.15        21\n",
      "           Anatomical Target: Upper Extremity       0.13      1.00      0.23        31\n",
      "Anatomical Target: Upper Extremity - Shoulder       0.09      1.00      0.17        23\n",
      "                                      Imaging       0.55      1.00      0.71       133\n",
      "                                  Imaging: CT       0.24      1.00      0.39        59\n",
      "                                 Imaging: MRI       0.24      1.00      0.39        59\n",
      "                          Imaging: Ultrasound       0.13      0.97      0.23        32\n",
      "                                Manufacturing       0.34      1.00      0.51        83\n",
      "        Manufacturing: Additive Manufacturing       0.16      1.00      0.27        38\n",
      "           Personalized Product: Guide or Jig       0.49      1.00      0.66       120\n",
      "                Personalized Product: Implant       0.51      1.00      0.68       124\n",
      "                         Specification of Use       0.33      1.00      0.49        79\n",
      "                Specification of Use: Disease       0.17      0.13      0.15        30\n",
      "      Specification of Use: Joint Replacement       0.18      1.00      0.31        44\n",
      "                              Surgical Method       0.16      1.00      0.28        40\n",
      "\n",
      "                                    micro avg       0.29      0.97      0.45      1505\n",
      "                                    macro avg       0.28      0.94      0.41      1505\n",
      "                                 weighted avg       0.38      0.97      0.52      1505\n",
      "                                  samples avg       0.29      0.97      0.43      1505\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               precision    recall  f1-score   support\n",
      "\n",
      "                        Analysis and Modeling       0.35      1.00      0.51        84\n",
      "           Analysis and Modeling: 3D Modeling       0.29      0.97      0.45        71\n",
      "                            Anatomical Target       0.67      1.00      0.81       164\n",
      "           Anatomical Target: Lower Extremity       0.47      1.00      0.63       113\n",
      "     Anatomical Target: Lower Extremity - Hip       0.16      1.00      0.28        40\n",
      "    Anatomical Target: Lower Extremity - Knee       0.34      1.00      0.50        82\n",
      "                     Anatomical Target: Torso       0.00      0.00      0.00        35\n",
      "             Anatomical Target: Torso - Spine       0.00      0.00      0.00        21\n",
      "           Anatomical Target: Upper Extremity       0.13      0.84      0.23        31\n",
      "Anatomical Target: Upper Extremity - Shoulder       0.10      1.00      0.17        23\n",
      "                                      Imaging       0.55      1.00      0.71       133\n",
      "                                  Imaging: CT       0.24      1.00      0.39        59\n",
      "                                 Imaging: MRI       0.24      0.98      0.39        59\n",
      "                          Imaging: Ultrasound       0.13      1.00      0.23        32\n",
      "                                Manufacturing       0.34      1.00      0.51        83\n",
      "        Manufacturing: Additive Manufacturing       0.16      1.00      0.27        38\n",
      "           Personalized Product: Guide or Jig       0.49      1.00      0.66       120\n",
      "                Personalized Product: Implant       0.51      1.00      0.68       124\n",
      "                         Specification of Use       0.33      1.00      0.49        79\n",
      "                Specification of Use: Disease       0.12      1.00      0.22        30\n",
      "      Specification of Use: Joint Replacement       0.18      1.00      0.31        44\n",
      "                              Surgical Method       0.16      1.00      0.28        40\n",
      "\n",
      "                                    micro avg       0.30      0.96      0.46      1505\n",
      "                                    macro avg       0.27      0.90      0.40      1505\n",
      "                                 weighted avg       0.37      0.96      0.52      1505\n",
      "                                  samples avg       0.30      0.96      0.44      1505\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               precision    recall  f1-score   support\n",
      "\n",
      "                        Analysis and Modeling       0.35      1.00      0.51        84\n",
      "           Analysis and Modeling: 3D Modeling       0.29      1.00      0.45        71\n",
      "                            Anatomical Target       0.67      1.00      0.81       164\n",
      "           Anatomical Target: Lower Extremity       0.47      1.00      0.63       113\n",
      "     Anatomical Target: Lower Extremity - Hip       0.16      1.00      0.28        40\n",
      "    Anatomical Target: Lower Extremity - Knee       0.34      1.00      0.50        82\n",
      "                     Anatomical Target: Torso       0.14      0.97      0.25        35\n",
      "             Anatomical Target: Torso - Spine       0.08      0.90      0.15        21\n",
      "           Anatomical Target: Upper Extremity       0.13      1.00      0.23        31\n",
      "Anatomical Target: Upper Extremity - Shoulder       0.10      1.00      0.18        23\n",
      "                                      Imaging       0.55      1.00      0.71       133\n",
      "                                  Imaging: CT       0.24      1.00      0.39        59\n",
      "                                 Imaging: MRI       0.24      1.00      0.39        59\n",
      "                          Imaging: Ultrasound       0.13      0.91      0.23        32\n",
      "                                Manufacturing       0.34      1.00      0.51        83\n",
      "        Manufacturing: Additive Manufacturing       0.15      0.97      0.27        38\n",
      "           Personalized Product: Guide or Jig       0.49      1.00      0.66       120\n",
      "                Personalized Product: Implant       0.51      1.00      0.68       124\n",
      "                         Specification of Use       0.33      1.00      0.49        79\n",
      "                Specification of Use: Disease       0.12      1.00      0.22        30\n",
      "      Specification of Use: Joint Replacement       0.18      1.00      0.31        44\n",
      "                              Surgical Method       0.16      1.00      0.28        40\n",
      "\n",
      "                                    micro avg       0.28      1.00      0.44      1505\n",
      "                                    macro avg       0.28      0.99      0.41      1505\n",
      "                                 weighted avg       0.38      1.00      0.52      1505\n",
      "                                  samples avg       0.28      0.99      0.42      1505\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stack(listOfDicts):\n",
    "#     initDict = listOfDicts[0]\n",
    "#     finalDict = {}\n",
    "#     for key in initDict.keys():\n",
    "#         tensors = tuple(d[key] for d in listOfDicts)\n",
    "#         finalDict[key] = torch.stack(tensors)\n",
    "#     return finalDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstracts = stack(list(tokenize(tokenizer, x, max_len=MAX_LEN_ABSTRACT) for x in testing_set.abstract))\n",
    "\n",
    "# claims = stack(list(tokenize(tokenizer, x, max_len=MAX_LEN_CLAIMS) for x in testing_set.claims))\n",
    "\n",
    "# predictions = model.forward(abstract=abstracts, claims=claims)\n",
    "\n",
    "# binarized = predictions.detach().numpy() > 0.5\n",
    "\n",
    "# from sklearn.metrics import classification_report\n",
    "# testing_labels = testing_set[subset]\n",
    "# print(classification_report(testing_labels, binarized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
