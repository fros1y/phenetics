{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModel, AutoModelForSequenceClassification, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import funcy as f\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from datasets import load_dataset, load_metric\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import util\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_json(\"training_set.json.gz\", lines=True, orient=\"records\")\n",
    "testing_set = pd.read_json(\"testing_set.json.gz\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(972, 110)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.11008230452675"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "17603/972"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = list(sorted(set(all_tiers_100)-set([\"PersonalizedProduct\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import funcy as f\n",
    "from random import shuffle\n",
    "\n",
    "@f.collecting\n",
    "def create_examples(row, num_neg=2):\n",
    "    abstract = row.abstract\n",
    "    claims = row.claims\n",
    "    yield (abstract, claims, True)\n",
    "    for text in [abstract,claims]:\n",
    "        neg_count = 0\n",
    "        shuffle(subset)\n",
    "        for tag in subset:\n",
    "            if row[tag]:\n",
    "                yield (text, f\"Tagged as {tier_translations[tag]}.\", True)\n",
    "            elif num_neg is None or neg_count < num_neg:\n",
    "                neg_count = neg_count + 1\n",
    "                yield (text, f\"Tagged as {tier_translations[tag]}.\", False)\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17603, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_triplets = pd.DataFrame(training_set.apply(f.partial(create_examples, num_neg=3), axis=1).explode().tolist()).drop_duplicates()\n",
    "training_triplets.columns = [\"sentence1\", \"sentence2\", \"label\"]\n",
    "training_triplets = training_triplets.reset_index()\n",
    "training_triplets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4105, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_triplets = pd.DataFrame(testing_set.apply(f.partial(create_examples, num_neg=2), axis=1).explode().tolist()).drop_duplicates()\n",
    "testing_triplets.columns = [\"sentence1\", \"sentence2\", \"label\"]\n",
    "testing_triplets = testing_triplets.reset_index()\n",
    "testing_triplets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bertForPatents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triplets = pd.read_parquet(\"triples.parquet\")\n",
    "# triplets.reset_index(inplace=True)\n",
    "\n",
    "# test_triplets = pd.read_parquet(\"testing_triples.parquet\")\n",
    "# test_triplets.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, maxlen=512, with_labels=True, bert_model=model_name):\n",
    "\n",
    "        self.data = data  # pandas dataframe\n",
    "        #Initialize the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)  \n",
    "\n",
    "        self.maxlen = maxlen\n",
    "        self.with_labels = with_labels \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Selecting sentence1 and sentence2 at the specified index in the data frame\n",
    "        sent1 = self.data.sentence1[index]\n",
    "        sent2 = self.data.sentence2[index]\n",
    "\n",
    "        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n",
    "        encoded_pair = self.tokenizer(sent1, sent2, \n",
    "                                      padding='max_length',  # Pad to max_length\n",
    "                                      truncation=True,  # Truncate to max_length\n",
    "                                      max_length=self.maxlen,  \n",
    "                                      return_tensors='pt')  # Return torch.Tensor objects\n",
    "        \n",
    "        token_ids = encoded_pair['input_ids'].squeeze(0)  # tensor of token ids\n",
    "        attn_masks = encoded_pair['attention_mask'].squeeze(0)  # binary tensor with \"0\" for padded values and \"1\" for the other values\n",
    "        token_type_ids = encoded_pair['token_type_ids'].squeeze(0)  # binary tensor with \"0\" for the 1st sentence tokens & \"1\" for the 2nd sentence tokens\n",
    "\n",
    "        if self.with_labels:  # True if the dataset has labels\n",
    "            label = self.data.label[index].astype(np.long)\n",
    "            return token_ids, attn_masks, token_type_ids, label  \n",
    "        else:\n",
    "            return token_ids, attn_masks, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\" Set all seeds to make results reproducible \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "\n",
    "class SentencePairClassifier(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                     training_dataset,\n",
    "                     testing_dataset,\n",
    "                     lr_warmup_steps=1000,\n",
    "                     bert_model=\"bertForPatents\", \n",
    "                     learning_rate=3e-5, \n",
    "                     freeze_bert=False,\n",
    "                     batch_size=16,\n",
    "                     seed=42):\n",
    "        super(SentencePairClassifier, self).__init__()\n",
    "        \n",
    "        if seed:\n",
    "            set_seed(seed)\n",
    "        \n",
    "        self.lr_warmup_steps = lr_warmup_steps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.training_dataset = training_dataset\n",
    "        self.testing_dataset = testing_dataset\n",
    "        \n",
    "        self.bert_layer = AutoModel.from_pretrained(bert_model, gradient_checkpointing=True)\n",
    "        hidden_size = 1024\n",
    "\n",
    "        # Freeze bert layers and only train the classification layer weights\n",
    "        if freeze_bert:\n",
    "            for p in self.bert_layer.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # Classification layer\n",
    "        self.cls_layer = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "        self.train_accuracy = pl.metrics.Accuracy()\n",
    "        self.val_accuracy = pl.metrics.Accuracy()\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(CustomDataset(self.training_dataset), batch_size=self.batch_size, shuffle=True, num_workers=32)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(CustomDataset(self.testing_dataset), batch_size=self.batch_size, shuffle=False, num_workers=32)\n",
    "    \n",
    "    def optimizer_step(self, optimizer, *args, **kwargs):\n",
    "        if self.trainer.global_step < self.lr_warmup_steps:\n",
    "            lr_scale = min(1., float(self.trainer.global_step + 1) / float(self.lr_warmup_steps))\n",
    "            lr = lr_scale * self.learning_rate\n",
    "            self.log('learning_rate', lr, on_step=True, on_epoch=False)\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg['lr'] = lr\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(params = self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attn_masks, token_type_ids):\n",
    "        cont_reps, pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)\n",
    "        logits = self.cls_layer(self.dropout(pooler_output))\n",
    "        return logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        seq, attn_masks, token_type_ids, labels = batch\n",
    "        logits = self.forward(seq, attn_masks, token_type_ids)\n",
    "        loss = self.criterion(logits.squeeze(-1), labels.float())\n",
    "        self.log('train_loss', loss, on_epoch=False, on_step=True, prog_bar=True)\n",
    "        self.log('train_acc_step', self.train_accuracy(F.sigmoid(logits).squeeze(-1), labels), on_step=True, on_epoch=False, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        seq, attn_masks, token_type_ids, labels = batch\n",
    "        logits = self.forward(seq, attn_masks, token_type_ids)\n",
    "        loss = self.criterion(logits.squeeze(-1), labels.float())\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_accuracy', self.val_accuracy(F.sigmoid(logits).squeeze(-1),labels), on_step=False, on_epoch=True, prog_bar=True )\n",
    "        \n",
    "    def criterion(self, y_pred, y_true):\n",
    "        return nn.BCEWithLogitsLoss()(y_pred, y_true)\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type      | Params\n",
      "---------------------------------------------\n",
      "0 | bert_layer     | BertModel | 344 M \n",
      "1 | cls_layer      | Linear    | 1.0 K \n",
      "2 | dropout        | Dropout   | 0     \n",
      "3 | train_accuracy | Accuracy  | 0     \n",
      "4 | val_accuracy   | Accuracy  | 0     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/anaconda3/envs/phenetics2/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/home/martin/anaconda3/envs/phenetics2/lib/python3.8/site-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426271f15076479ba88c22231c42ab8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/anaconda3/envs/phenetics2/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "    pl.callbacks.ModelCheckpoint(monitor='val_loss', save_top_k=1),\n",
    "    #pl.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=3, )\n",
    "]\n",
    "\n",
    "model = SentencePairClassifier(training_dataset=training_triplets, testing_dataset=testing_triplets)\n",
    "trainer = pl.Trainer(gpus=1, \n",
    "                     accumulate_grad_batches=4,\n",
    "                     max_epochs=5,\n",
    "                     #auto_scale_batch_size=True,\n",
    "                     precision=32,\n",
    "                     log_every_n_steps=1,\n",
    "                     flush_logs_every_n_steps=10,\n",
    "                     callbacks=callbacks,\n",
    "                     val_check_interval=0.10)\n",
    "#trainer.tune(model)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
