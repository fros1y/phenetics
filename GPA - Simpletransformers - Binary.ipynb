{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import *\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import logging\n",
    "from util import *\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "#from transformers imp ort *\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_json(\"training_set.json.gz\", lines=True, orient=\"records\")\n",
    "testing_set = pd.read_json(\"testing_set.json.gz\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame()\n",
    "train_df[\"text\"] = training_set.abstract\n",
    "train_df[\"labels\"] = training_set.AnalysisAndModeling.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame()\n",
    "test_df[\"text\"] = testing_set.description\n",
    "test_df['labels'] = testing_set.AnalysisAndModeling.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TECHNICAL FIELD \\n     The present disclosure ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CROSS-REFERENCE TO RELATED APPLICATIONS \\n    ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CROSS-REFERENCE TO RELATED APPLICATIONS \\n    ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CROSS-REFERENCE TO RELATED APPLICATIONS \\n    ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CROSS-REFERENCE TO RELATED APPLICATIONS \\n    ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>CROSS-REFERENCE TO RELATED APPLICATIONS \\n   T...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>FIELD OF THE INVENTION \\n     The present inve...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>BACKGROUND OF THE INVENTION \\n   The present i...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>FIELD \\n     The present teachings relate to a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>CROSS REFERENCE TO RELATED APPLICATIONS \\n    ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text labels\n",
       "0    TECHNICAL FIELD \\n     The present disclosure ...   True\n",
       "1    CROSS-REFERENCE TO RELATED APPLICATIONS \\n    ...   True\n",
       "2    CROSS-REFERENCE TO RELATED APPLICATIONS \\n    ...  False\n",
       "3    CROSS-REFERENCE TO RELATED APPLICATIONS \\n    ...  False\n",
       "4    CROSS-REFERENCE TO RELATED APPLICATIONS \\n    ...   True\n",
       "..                                                 ...    ...\n",
       "238  CROSS-REFERENCE TO RELATED APPLICATIONS \\n   T...  False\n",
       "239  FIELD OF THE INVENTION \\n     The present inve...   True\n",
       "240  BACKGROUND OF THE INVENTION \\n   The present i...  False\n",
       "241  FIELD \\n     The present teachings relate to a...  False\n",
       "242  CROSS REFERENCE TO RELATED APPLICATIONS \\n    ...   True\n",
       "\n",
       "[243 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ClassificationArgs()\n",
    "model_args.fp16 = True\n",
    "model_args.use_early_stopping = True\n",
    "model_args.early_stopping_delta = 0.01\n",
    "model_args.early_stopping_metric = \"f1\"\n",
    "model_args.early_stopping_metric_minimize = False\n",
    "model_args.early_stopping_patience = 5\n",
    "model_args.evaluate_during_training_steps = 100\n",
    "model_args.num_train_epochs = 50\n",
    "model_args.do_lower_case = False\n",
    "model_args.dataloader_num_workers=0\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.evaluate_during_training=True\n",
    "model_args.evaluate_during_training_silent=False\n",
    "model_args.evaluate_during_training_verbose=True\n",
    "model_args.use_cached_eval_features=True\n",
    "model_args.evaluate_each_epoch=False\n",
    "model_args.evaluate_during_training_steps = 32\n",
    "model_args.max_seq_length=256\n",
    "model_args.train_batch_size=32\n",
    "model_args.save_best_model=True\n",
    "model_args.save_model_every_epoch=False\n",
    "model_args.save_eval_checkpoints=False\n",
    "model_args.labels_list = [\"True\", \"False\"]\n",
    "#model_args.sliding_windows=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.learning_rate = 4e-04\n",
    "model_args.warmup_ratio = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = BertForPreTraining.from_pretrained(\"/var/patentmark/patentBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_pretrained(\"/var/patentmark/conv-patentBERT2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ClassificationModel(\n",
    "    \"albert\", \"albert-base-v2\", args=model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fefde5f770b49e28c1f4688a9b93999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=972.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245d0e5876d744788b39038a4905bb7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=50.0, style=ProgressStyle(description_width='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8099a56ae28045b7bfb126bf3a18a71c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 0 of 50', max=31.0, style=ProgressStyle(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be759722d1ce4115adccb84427f60cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 1 of 50', max=31.0, style=ProgressStyle(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model: Features loaded from cache at cache_dir/cached_dev_albert_256_2_243\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed144729c89747e481353ce95236931a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Evaluation', max=31.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/anaconda3/envs/phenetics/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.0, 'tp': 84, 'tn': 0, 'fp': 159, 'fn': 0, 'f1': 0.5137614678899083, 'eval_loss': 0.8376387376939097}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/anaconda3/envs/phenetics/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in f1\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 1\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08774885ebe4d2e837dbca848b35db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 2 of 50', max=31.0, style=ProgressStyle(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model: Features loaded from cache at cache_dir/cached_dev_albert_256_2_243\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790d9479f4114c01b5dbb287e9f4c042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Evaluation', max=31.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.0, 'tp': 84, 'tn': 0, 'fp': 159, 'fn': 0, 'f1': 0.5137614678899083, 'eval_loss': 0.7171270482001766}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in f1\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 2\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85e61c99ea7474691e688ece23a24bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 3 of 50', max=31.0, style=ProgressStyle(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model: Features loaded from cache at cache_dir/cached_dev_albert_256_2_243\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cca5f18a1b34f3b83f4a70b48b5add4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Evaluation', max=31.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.0, 'tp': 84, 'tn': 0, 'fp': 159, 'fn': 0, 'f1': 0.5137614678899083, 'eval_loss': 0.8562185985426749}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in f1\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 3\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044890003829494c8f866d0440876e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 4 of 50', max=31.0, style=ProgressStyle(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model: Features loaded from cache at cache_dir/cached_dev_albert_256_2_243\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eac1c8f63104a39ad3aad406469b03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Evaluation', max=31.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.0, 'tp': 84, 'tn': 0, 'fp': 159, 'fn': 0, 'f1': 0.5137614678899083, 'eval_loss': 0.8974122212779138}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in f1\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 4\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1d7e9503fb4e1ba8fc9584aab39030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 5 of 50', max=31.0, style=ProgressStyle(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model: Features loaded from cache at cache_dir/cached_dev_albert_256_2_243\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8edecb8aaa4e458b8873530b3a92cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Evaluation', max=31.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.0, 'tp': 84, 'tn': 0, 'fp': 159, 'fn': 0, 'f1': 0.5137614678899083, 'eval_loss': 0.7288853083887408}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in f1\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 5\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598a2fd5697f4c9b9eb2ea84b9c9cdbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 6 of 50', max=31.0, style=ProgressStyle(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model: Features loaded from cache at cache_dir/cached_dev_albert_256_2_243\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9242e169b2fd4f528257d68f29d9a2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Evaluation', max=31.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.0, 'tp': 84, 'tn': 0, 'fp': 159, 'fn': 0, 'f1': 0.5137614678899083, 'eval_loss': 0.7278599527574354}\n",
      "INFO:simpletransformers.classification.classification_model: Patience of 5 steps reached\n",
      "INFO:simpletransformers.classification.classification_model: Training terminated.\n",
      "INFO:simpletransformers.classification.classification_model: Training of albert model complete. Saved to outputs/.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(192,\n",
       " {'global_step': [32, 64, 96, 128, 160, 192],\n",
       "  'tp': [84, 84, 84, 84, 84, 84],\n",
       "  'tn': [0, 0, 0, 0, 0, 0],\n",
       "  'fp': [159, 159, 159, 159, 159, 159],\n",
       "  'fn': [0, 0, 0, 0, 0, 0],\n",
       "  'mcc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  'train_loss': [0.6682323813438416,\n",
       "   0.6601671576499939,\n",
       "   0.8827059268951416,\n",
       "   0.6258347034454346,\n",
       "   0.6931665539741516,\n",
       "   0.6723867654800415],\n",
       "  'eval_loss': [0.8376387376939097,\n",
       "   0.7171270482001766,\n",
       "   0.8562185985426749,\n",
       "   0.8974122212779138,\n",
       "   0.7288853083887408,\n",
       "   0.7278599527574354],\n",
       "  'f1': [0.5137614678899083,\n",
       "   0.5137614678899083,\n",
       "   0.5137614678899083,\n",
       "   0.5137614678899083,\n",
       "   0.5137614678899083,\n",
       "   0.5137614678899083]})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_model(train_df, eval_df=test_df, f1=sklearn.metrics.f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, model_outputs, wrong_predictions = model.eval_model(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.mean(model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(testing_labels, model_outputs, target_names=mlb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import os\n",
    "# import re\n",
    "# import tensorflow as tf\n",
    "# import torch\n",
    "# def convert_tf_checkpoint_to_pytorch(tf_checkpoint_path, bert_config_file, pytorch_dump_path):\n",
    "#     config_path = os.path.abspath(bert_config_file)\n",
    "#     tf_path = os.path.abspath(tf_checkpoint_path)\n",
    "#     print(\"Converting TensorFlow checkpoint from {} with config at {}\".format(tf_path, config_path))\n",
    "#     # Load weights from TF model\n",
    "#     init_vars = tf.train.list_variables(tf_path)\n",
    "#     excluded = [\"BERTAdam\", \"_power\", \"global_step\", \"_CHECKPOINTABLE_OBJECT_GRAPH\"]\n",
    "#     init_vars = list(filter(lambda x: all([True if e not in x[0] else False for e in excluded]), init_vars))\n",
    "#     names = []\n",
    "#     arrays = []\n",
    "#     for name, shape in init_vars:\n",
    "#         print(\"Loading TF weight {} with shape {}\".format(name, shape))\n",
    "#         array = tf.train.load_variable(tf_path, name)\n",
    "#         names.append(name)\n",
    "#         arrays.append(array)\n",
    "\n",
    "#     config = BertConfig.from_json_file(bert_config_file)\n",
    "#     print(\"Building PyTorch model from configuration: {}\".format(str(config)))\n",
    "#     # Initialise PyTorch model\n",
    "#     model = BertForSequenceClassification(config)\n",
    "\n",
    "#     for name, array in zip(names, arrays):\n",
    "#         name = name.split(\"/\")\n",
    "#         # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n",
    "#         # which are not required for using pretrained model\n",
    "#         if any(n in [\"adam_v\", \"adam_m\", \"global_step\", \"bad_steps\", \"global_step\", \"good_steps\", \"loss_scale\",\n",
    "#                      \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"save_counter\", \".OPTIMIZER_SLOT\"] for n in name) or \\\n",
    "#                 name[0] == \"optimizer\":\n",
    "#             print(\"Skipping {}\".format(\"/\".join(name)))\n",
    "#             continue\n",
    "#         if \".OPTIMIZER_SLOT\" in name:\n",
    "#             idx = name.index(\".OPTIMIZER_SLOT\")\n",
    "#             name = name[:idx]\n",
    "#         elif \".ATTRIBUTES\" in name:\n",
    "#             idx = name.index(\".ATTRIBUTES\")\n",
    "#             name = name[:idx]\n",
    "#         print(name)\n",
    "#         pointer = model\n",
    "#         for m_name in name:\n",
    "#             try:\n",
    "#                 if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n",
    "#                     scope_names = re.split(r\"_(\\d+)\", m_name)\n",
    "#                 else:\n",
    "#                     scope_names = [m_name]\n",
    "#                 if scope_names[0] == \"kernel\" or scope_names[0] == \"gamma\":\n",
    "#                     pointer = getattr(pointer, \"weight\")\n",
    "#                 elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n",
    "#                     pointer = getattr(pointer, \"bias\")\n",
    "#                 elif scope_names[0] == \"output_weights\":\n",
    "#                     pointer = getattr(pointer, \"weight\")\n",
    "#                 elif scope_names[0] == \"squad\":\n",
    "#                     pointer = getattr(pointer, \"classifier\")\n",
    "#                 elif scope_names[0] == \"dense_output\" or scope_names[0] == \"bert_output\":\n",
    "#                     pointer = getattr(pointer, \"output\")\n",
    "#                 elif scope_names[0] == \"self_attention\":\n",
    "#                     pointer = getattr(pointer, \"self\")\n",
    "#                 else:\n",
    "#                     pointer = getattr(pointer, scope_names[0])\n",
    "#             except AttributeError:\n",
    "#                     logger.info(\"Skipping {}\".format(\"/\".join(name)))\n",
    "#                     continue\n",
    "#             if len(scope_names) >= 2:\n",
    "#                 num = int(scope_names[1])\n",
    "#                 pointer = pointer[num]\n",
    "#         if m_name[-11:] == \"_embeddings\":\n",
    "#             pointer = getattr(pointer, \"weight\")\n",
    "#         elif m_name == \"kernel\" or m_name == \"gamma\" or m_name == \"output_weights\":\n",
    "#             array = np.transpose(array)\n",
    "#         # print(\"Initialize PyTorch weight {}\".format(name))\n",
    "#         pointer.data = torch.from_numpy(array)\n",
    "\n",
    "#     # Save pytorch-model\n",
    "#     print(\"Save PyTorch model to {}\".format(pytorch_dump_path))\n",
    "#     torch.save(model.state_dict(), pytorch_dump_path)\n",
    "\n",
    "\n",
    "# convert_tf_checkpoint_to_pytorch(\"/var/patentmark/patentBERT/model.ckpt\", \"/var/patentmark/patentBERT/config.json\", pytorch_dump_path=\"/var/patentmark/patentBERT/pytorch.bin\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
