{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample, LoggingHandler, losses, models, util\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from util import *\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = list(sorted(set(all_tiers_100)-set([\"PersonalizedProduct\"])))\n",
    "training_set = pd.read_json(\"training_set.json.gz\", lines=True, orient=\"records\")\n",
    "testing_set = pd.read_json(\"testing_set.json.gz\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "training_labels = training_set[subset].apply(util.array_labels_textual, axis=1).values.tolist()\n",
    "testing_labels = testing_set[subset].apply(util.array_labels_textual, axis=1).values.tolist()\n",
    "all_labels = training_labels + testing_labels\n",
    "\n",
    "lbe = LabelEncoder()\n",
    "lbe.fit(all_labels)\n",
    "training_set['label'] = lbe.transform(training_labels)\n",
    "testing_set['label'] = lbe.transform(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import funcy as f\n",
    "@f.collecting\n",
    "def create_examples(row):\n",
    "    abstract = row.abstract\n",
    "    claims = row.claims\n",
    "    yield (abstract, claims, 1)\n",
    "    for text in [abstract,claims]:\n",
    "        for tag in subset:\n",
    "            yield (text, f\"Tag: {tier_translations[tag]}\", row[tag])\n",
    "raw_triplets = training_set.apply(create_examples, axis=1).explode()\n",
    "\n",
    "def build_example(entry):\n",
    "    return InputExample(texts=[entry[0], entry[1]], label=entry[2])\n",
    "\n",
    "all_examples = raw_triplets.apply(build_example).sample(frac=1.0)\n",
    "dev_examples = all_examples[:1000].values\n",
    "train_examples = all_examples[1000:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bertForPatents/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 8\n",
    "output_path = \"output/training-triplets-\"+model_name+\"-\"+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_model = models.Transformer(model_name, max_seq_length=192, model_args={\"gradient_checkpointing\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import OnlineContrastiveLoss\n",
    "\n",
    "train_dataset = SentencesDataset(train_examples, model=model)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = OnlineContrastiveLoss(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator.from_input_examples(dev_examples, name='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_steps = int(len(train_dataset) * num_epochs / train_batch_size * 0.1) #10% of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          use_amp=True,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=output_path)\n",
    "\n",
    "# ##############################################################################\n",
    "# #\n",
    "# # Load the stored model and evaluate its performance on STS benchmark dataset\n",
    "# #\n",
    "# ##############################################################################\n",
    "\n",
    "# logging.info(\"Read test examples\")\n",
    "# test_examples = []\n",
    "# with open(os.path.join(dataset_path, 'test.csv'), encoding=\"utf-8\") as fIn:\n",
    "#     reader = csv.DictReader(fIn, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "#     for row in reader:\n",
    "#         test_examples.append(InputExample(texts=[row['Sentence1'], row['Sentence2'], row['Sentence3']]))\n",
    "\n",
    "\n",
    "# model = SentenceTransformer(output_path)\n",
    "# test_evaluator = TripletEvaluator.from_input_examples(test_examples, name='test')\n",
    "# test_evaluator(model, output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
