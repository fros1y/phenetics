{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.init import xavier_normal\n",
    "\n",
    "class GloVe(nn.Module):\n",
    "    def __init__(self, co_oc, embed_size, x_max=100, alpha=0.75):\n",
    "        \"\"\"\n",
    "        :param co_oc: Co-occurrence ndarray with shape of [num_classes, num_classes]\n",
    "        :param embed_size: embedding size\n",
    "        :param x_max: An int representing cutoff of the weighting function\n",
    "        :param alpha: Ant float parameter of the weighting function\n",
    "        \"\"\"\n",
    "\n",
    "        super(GloVe, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.x_max = x_max\n",
    "        self.alpha = alpha\n",
    "\n",
    "        ''' co_oc Matrix is shifted in order to prevent having log(0) '''\n",
    "        self.co_oc = co_oc + 1.0\n",
    "\n",
    "        [self.num_classes, _] = self.co_oc.shape\n",
    "\n",
    "        self.in_embed = nn.Embedding(self.num_classes, self.embed_size)\n",
    "        self.in_embed.weight = xavier_normal(self.in_embed.weight)\n",
    "\n",
    "        self.in_bias = nn.Embedding(self.num_classes, 1)\n",
    "        self.in_bias.weight = xavier_normal(self.in_bias.weight)\n",
    "\n",
    "        self.out_embed = nn.Embedding(self.num_classes, self.embed_size)\n",
    "        self.out_embed.weight = xavier_normal(self.out_embed.weight)\n",
    "\n",
    "        self.out_bias = nn.Embedding(self.num_classes, 1)\n",
    "        self.out_bias.weight = xavier_normal(self.out_bias.weight)\n",
    "\n",
    "    def forward(self, input, output):\n",
    "        \"\"\"\n",
    "        :param input: An array with shape of [batch_size] of int type\n",
    "        :param output: An array with shape of [batch_size] of int type\n",
    "        :return: loss estimation for Global Vectors word representations\n",
    "                 defined in nlp.stanford.edu/pubs/glove.pdf\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = len(input)\n",
    "\n",
    "        co_occurences = np.array([self.co_oc[input[i], output[i]] for i in range(batch_size)])\n",
    "        weights = np.array([self._weight(var) for var in co_occurences])\n",
    "\n",
    "        co_occurences = Variable(t.from_numpy(co_occurences)).float()\n",
    "        weights = Variable(t.from_numpy(weights)).float()\n",
    "\n",
    "        input = Variable(t.from_numpy(input))\n",
    "        output = Variable(t.from_numpy(output))\n",
    "\n",
    "        input_embed = self.in_embed(input)\n",
    "        input_bias = self.in_bias(input)\n",
    "        output_embed = self.out_embed(output)\n",
    "        output_bias = self.out_bias(output)\n",
    "\n",
    "        return (t.pow(\n",
    "            ((input_embed * output_embed).sum(1) + input_bias + output_bias).squeeze(1) - t.log(co_occurences), 2\n",
    "        ) * weights).sum()\n",
    "\n",
    "    def _weight(self, x):\n",
    "        return 1 if x > self.x_max else (x / self.x_max) ** self.alpha\n",
    "\n",
    "    def embeddings(self):\n",
    "        return self.in_embed.weight.data.cpu().numpy() + self.out_embed.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_frame = pd.read_csv(\"/var/patentmark/subset_cooc.csv\", sep=\",\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_frame.columns = [\"code1\", \"code2\", \"weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21513823, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cooc_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_frame['code1'] = cooc_frame.code1.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_frame['code2'] = cooc_frame.code2.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_codes = list(set(cooc_frame.code1.cat.categories.values.tolist() + cooc_frame.code2.cat.categories.values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node2id = LabelEncoder()\n",
    "node2id.fit(all_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['node2id.joblib']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(node2id, \"node2id.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_mapping = pd.DataFrame( [(node_id, node) for node_id, node in enumerate(node2id.classes_)])\n",
    "node_mapping.columns = [\"node_id\", \"node\"]\n",
    "node_mapping.to_csv(\"cpc_mapping.csv\", header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Embedding(object):\n",
    "    def __init__(self, embedding_path: str, dimensions: int, index_path: str = None):\n",
    "        self.dimensions = dimensions\n",
    "        self.embeddings = self.load_embeddings(embedding_path)\n",
    "        self.index: Dict[str, int] = {}\n",
    "        if index_path:\n",
    "            self.load_index(index_path)\n",
    "\n",
    "    def load_embeddings(self, file_name: str) -> np.ndarray:\n",
    "        print(\"Loading embeddings...\")\n",
    "        embeddings = np.fromfile(file_name, dtype=np.float32)\n",
    "        length = embeddings.shape[0]\n",
    "        assert length % self.dimensions == 0, f\"The number of floats ({length}) in the embeddings is not divisible by\" \\\n",
    "                                              f\"the number of dimensions ({self.dimensions})!\"\n",
    "        embedding_shape = [int(length / self.dimensions), self.dimensions]\n",
    "        embeddings = embeddings.reshape(embedding_shape)\n",
    "        print(f\"Done loading embeddings (shape: {embeddings.shape}).\")\n",
    "        return embeddings\n",
    "\n",
    "    def load_index(self, index_path: str) -> None:\n",
    "        print(\"Loading uri index...\")\n",
    "        with open(index_path, \"r\") as file:\n",
    "            for line in [line.strip() for line in file.readlines()]:\n",
    "                index, uri = line.split(\",\", 1)\n",
    "                self.index[uri] = int(index)\n",
    "        print(f\"Done loading {len(self.index)} items.\")\n",
    "\n",
    "    def __getitem__(self, item) -> np.ndarray:\n",
    "        if self.index and isinstance(item, str):\n",
    "            return self.embeddings[self.index[item]]\n",
    "        return self.embeddings[item]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings...\n",
      "Done loading embeddings (shape: (164296, 32)).\n",
      "Loading uri index...\n",
      "Done loading 164296 items.\n"
     ]
    }
   ],
   "source": [
    "embedding_file = \"/home/martin/cpc.emb.verse.32d.bin\"\n",
    "index_file = \"cpc_mapping.csv\"\n",
    "embeddings = Embedding(embedding_file, 32, index_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Embedding at 0x7fe61b887070>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164296"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(node2id.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_frame[\"code1_id\"] = node2id.transform(cooc_frame.code1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_frame[\"code2_id\"] = node2id.transform(cooc_frame.code2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_frame[['code1_id', 'code2_id', 'weight']].to_csv(\"code_cooc_weighs_encoded.csv\", index=False, header=None, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = GloVe(co_oc_matrix, embed_size)\n",
    "    \n",
    "optimizer = Adagrad(glove.parameters(), 0.05)\n",
    "    \n",
    "for i in range(num_iterations):\n",
    "    ''' \n",
    "    input and target are [batch_size] shaped arrays of int type\n",
    "    '''\n",
    "    input, target = next_batch(batch_size)\n",
    "        \n",
    "    loss = glove(input, target)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "word_embeddings = glove.embeddings()      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
