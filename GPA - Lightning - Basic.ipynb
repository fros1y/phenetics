{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import logging\n",
    "from util import *\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "import pprint\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_json(\"training_set.json.gz\", lines=True, orient=\"records\")\n",
    "testing_set = pd.read_json(\"testing_set.json.gz\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = list(sorted(set(all_tiers_100)-set([\"PersonalizedProduct\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"/home/martin/IdeaProjects/phenetics/bertForPatents/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModel.from_pretrained(model_name, gradient_checkpointing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['labels']=training_set[subset].astype(int).values.tolist()\n",
    "testing_set['labels']=testing_set[subset].astype(int).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokenizer, text, max_len):\n",
    "        text = str(text)\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            f\"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            f\"attention_mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            f\"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN_CLAIMS = 512\n",
    "MAX_LEN_ABSTRACT = 160\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-5\n",
    "SEED = 17\n",
    "PRED_THRES = 0.4\n",
    "ACCUM_STEPS = 8\n",
    "NUM_LABELS = len(subset)\n",
    "\n",
    "logdir=\"/var/patentmark/logdir/fit2/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        \n",
    "        self.claims = dataframe.claims\n",
    "        self.abstracts = dataframe.abstract\n",
    "        \n",
    "        self.labels = dataframe.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        abstract = tokenize(tokenizer, self.abstracts[index], max_len=MAX_LEN_ABSTRACT)\n",
    "        claims = tokenize(tokenizer, self.claims[index], MAX_LEN_CLAIMS)\n",
    "        labels = torch.tensor(np.array(self.labels[index]), dtype=torch.\n",
    "                              float)                \n",
    "        return {\"abstract\": abstract, \n",
    "                \"claims\": claims,\n",
    "                'labels': labels}\n",
    "    \n",
    "training_dataset = MultiLabelDataset(training_set, tokenizer)\n",
    "testing_dataset = MultiLabelDataset(testing_set, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import label_ranking_average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "logger = TensorBoardLogger('/var/patentmark/tb_logs', name='basicModel')\n",
    "\n",
    "class BasicModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.text_embedder = AutoModel.from_pretrained(model_name, gradient_checkpointing=True)\n",
    "        \n",
    "        total_embedding_size = self.text_embedder.pooler.dense.out_features*2\n",
    "        output_size = NUM_LABELS\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(total_embedding_size, output_size)\n",
    "\n",
    "            \n",
    "    def forward(self, abstract, claims):\n",
    "        abstract_emb = self.text_embedder(input_ids=abstract[\"input_ids\"], attention_mask=abstract[\"attention_mask\"])\n",
    "        abstract_emb = abstract_emb[0][:, 0]\n",
    "        \n",
    "        claim_emb = self.text_embedder(input_ids=claims[\"input_ids\"], attention_mask=claims[\"attention_mask\"])\n",
    "        claim_emb = claim_emb[0][:, 0]\n",
    "           \n",
    "        x = torch.cat((abstract_emb, claim_emb), 1)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class BasicSystem(pl.LightningModule):\n",
    "    def __init__(self, batch_size=4, learning_rate = 1e-05):\n",
    "        super().__init__()\n",
    "        self.model = BasicModel()\n",
    "        self.loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(training_dataset, batch_size=self.batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(testing_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "    \n",
    "    def forward(self, abstract, claims):\n",
    "        predictions = F.sigmoid(self.model(abstract=abstract, claims=claims))\n",
    "        return predictions\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        abstract = batch['abstract']\n",
    "        claims = batch['claims']\n",
    "        labels = batch['labels']\n",
    "        logits = self.model(abstract=abstract, claims=claims)\n",
    "        loss = self.loss_function(logits, labels)\n",
    "\n",
    "        labels = labels.cpu().detach().numpy()\n",
    "        logits = F.sigmoid(logits).cpu().detach().numpy()\n",
    "        lrap = label_ranking_average_precision_score(labels, logits)\n",
    "        \n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_lrap', lrap, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        abstract = batch['abstract']\n",
    "        claims = batch['claims']\n",
    "        labels = batch['labels']\n",
    "        logits = self.model(abstract=abstract, claims=claims)\n",
    "        loss = self.loss_function(logits, labels)\n",
    "        \n",
    "        labels = labels.cpu().detach().numpy()\n",
    "        logits = F.sigmoid(logits).cpu().detach().numpy()\n",
    "        lrap = label_ranking_average_precision_score(labels, logits)\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_lrap', lrap, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(params = self.model.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "2020-12-02 20:13:59 - GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "2020-12-02 20:13:59 - TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2020-12-02 20:13:59 - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type              | Params\n",
      "----------------------------------------------------\n",
      "0 | model         | BasicModel        | 344 M \n",
      "1 | loss_function | BCEWithLogitsLoss | 0     \n",
      "2020-12-02 20:14:01 - \n",
      "  | Name          | Type              | Params\n",
      "----------------------------------------------------\n",
      "0 | model         | BasicModel        | 344 M \n",
      "1 | loss_function | BCEWithLogitsLoss | 0     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9158592b0e4d239075ca022b218022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BasicSystem()\n",
    "trainer = pl.Trainer(gpus=1, accumulate_grad_batches=4, logger=logger)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
