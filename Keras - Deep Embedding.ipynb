{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import *\n",
    "from sklearn.feature_extraction.text import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import scipy.sparse as sps\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_json(\"training_set.json.gz\", lines=True, orient=\"records\")\n",
    "testing_set = pd.read_json(\"testing_set.json.gz\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_vectorizer = CountVectorizer(min_df=5, analyzer=lambda x: x)\n",
    "code_vectors = code_vectorizer.fit_transform(training_set.cpc_codes)\n",
    "test_code_vectors = code_vectorizer.transform(testing_set.cpc_codes)\n",
    "\n",
    "# text_vectorizer = TfidfVectorizer(min_df=5)\n",
    "\n",
    "# abstract_vectors = text_vectorizer.fit_transform(training_set.abstract)\n",
    "# test_abstract_vectors = text_vectorizer.transform(testing_set.abstract)\n",
    "\n",
    "# text_vectorizer = TfidfVectorizer(min_df=5)\n",
    "\n",
    "# claim_vectors = text_vectorizer.fit_transform(training_set.claims)\n",
    "# test_claim_vectors = text_vectorizer.transform(testing_set.claims)\n",
    "\n",
    "# text_vectorizer = TfidfVectorizer(min_df=2, max_df=0.5)\n",
    "\n",
    "# desc_vectors = text_vectorizer.fit_transform(training_set.description)\n",
    "# test_desc_vectors = text_vectorizer.transform(testing_set.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = list(set(all_tiers_100)-set([\"PersonalizedProduct\"]))\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = training_set[subset].values.astype(int)\n",
    "test_labels = testing_set[subset].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# # def tokenize(sentences, tokenizer):\n",
    "# #     input_ids, input_masks, input_segments = [],[],[]\n",
    "# #     for sentence in tqdm(sentences):\n",
    "# #         inputs = tokenizer.encode_plus(sentence,\n",
    "# #                                        truncation=True,\n",
    "# #                                        add_special_tokens=True, \n",
    "# #                                        max_length=256,\n",
    "# #                                        padding=True,\n",
    "# #                                        return_tensors='tf',\n",
    "# #                                        #pad_to_max_length=True,\n",
    "# #                                        return_attention_mask=False, \n",
    "# #                                        return_token_type_ids=False)\n",
    "# #         input_ids.append(inputs['input_ids'])\n",
    "# #         input_masks.append(inputs['attention_mask'])\n",
    "# #         input_segments.append(inputs['token_type_ids'])        \n",
    "        \n",
    "#     return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"/var/patentmark/patentBERT/\")\n",
    "#transformer = TFAutoModel.from_pretrained(\"/var/patentmark/patentBERT/\", from_pt=True)\n",
    "#config = AutoConfig.from_pretrained(\"/var/patentmark/patentBERT/\")\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"./bertForPatents/\")\n",
    "#transformer = TFAutoModel.from_pretrained(\"./bertForPatents/\")\n",
    "#config = AutoConfig.from_pretrained(\"./bertForPatents/\")\n",
    "model_name = \"/home/martin/IdeaProjects/phenetics/bertForPatents\" #\"johngiorgi/declutr-sci-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "transformer = TFAutoModel.from_pretrained(model_name, from_pt=True)\n",
    "config = AutoConfig.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_input_ids = tokenizer(text=training_set.claims.to_list(),\n",
    "                            truncation=True,\n",
    "                           add_special_tokens=True, \n",
    "                           max_length=max_length,\n",
    "                           padding=True,\n",
    "                           return_tensors='tf',\n",
    "                           #pad_to_max_length=True,\n",
    "                           return_attention_mask=False, \n",
    "                           return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_claim_input_ids = tokenizer(text=testing_set.claims.to_list(),\n",
    "                            truncation=True,\n",
    "                           add_special_tokens=True, \n",
    "                           max_length=max_length,\n",
    "                           padding=True,\n",
    "                           return_tensors='tf',\n",
    "                           #pad_to_max_length=True,\n",
    "                           return_attention_mask=False, \n",
    "                           return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_input_ids = tokenizer(text=training_set.abstract.to_list(),\n",
    "                            truncation=True,\n",
    "                           add_special_tokens=True, \n",
    "                           max_length=max_length,\n",
    "                           padding=True,\n",
    "                           return_tensors='tf',\n",
    "                           #pad_to_max_length=True,\n",
    "                           return_attention_mask=False, \n",
    "                           return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_abstract_input_ids = tokenizer(text=testing_set.abstract.to_list(),\n",
    "                            truncation=True,\n",
    "                           add_special_tokens=True, \n",
    "                           max_length=max_length,\n",
    "                           padding=True,\n",
    "                           return_tensors='tf',\n",
    "                           #pad_to_max_length=True,\n",
    "                           return_attention_mask=False, \n",
    "                           return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claim_input_ids, claim_masks, _ = tokenize(training_set.claims, tokenizer)\n",
    "# test_claim_input_ids, test_claim_masks, _ = tokenize(testing_set.claims, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    #claim_input = layers.Input(shape=(claim_vectors.shape[1]))\n",
    "    #description_input = layers.Input(shape=(desc_vectors.shape[1]))\n",
    "    \n",
    "    #codes_in = layers.Input(shape=(code_vectors.shape[1]), dtype='int32', name='code_vector')\n",
    "    #code_embedding = layers.Dense(16)(codes_in)\n",
    "    \n",
    "    claim_ids_in = layers.Input(shape=(max_length,), dtype='int32', name=\"claim_tokens\")\n",
    "    abstract_ids_in = layers.Input(shape=(max_length,), dtype='int32', name=\"abstract_tokens\")\n",
    "    #claim_masks_in = layers.Input(shape=(claim_masks.shape[1]), dtype='int32')\n",
    "    claim_embedding_layer = transformer(claim_ids_in)[1]\n",
    "    abstract_embedding_layer = transformer(abstract_ids_in)[1]\n",
    "    \n",
    "    embedding_layers = layers.Concatenate()([abstract_embedding_layer\n",
    "                                             , claim_embedding_layer\n",
    "                                             #, code_embedding\n",
    "                                            ])\n",
    "    #, attention_mask=claim_masks_in)[0]\n",
    "    #claim_embedding_layer = layers.GlobalAveragePooling1D()(claim_embedding_layer)\n",
    "    \n",
    "    #concat_layer = layers.Concatenate()([claim_input])\n",
    "    #dropout = layers.Dropout(config.hidden_dropout_prob)(claim_embedding_layer, training=False)\n",
    "    \n",
    "    #abstract_input = layers.Input(shape=(abstract_vectors.shape[1]), name=\"abstract_vectors\")\n",
    "    #claim_input = layers.Input(shape=(claim_vectors.shape[1]), name=\"claim_vectors\")\n",
    "    \n",
    "#     concat = layers.Concatenate()((abstract_input, claim_input))\n",
    "#     dropout0 = layers.Dropout(0.7)(concat)\n",
    "#     dense0 = layers.Dense(64, activation='elu')(dropout0)\n",
    "#     dropout1 = layers.Dropout(0.7)(dense0)\n",
    "    dropout = layers.Dropout(0.5)(embedding_layers)\n",
    "    dense = layers.Dense(64, activation='elu')(dropout)\n",
    "    output = layers.Dense(units=len(subset), activation='sigmoid')(dense)\n",
    "    \n",
    "                          #kernel_initializer=keras.initializers.TruncatedNormal(stddev=config.initializer_range))(dense0)\n",
    "    \n",
    "    #, activation='sigmoid')(dropout)\n",
    "    \n",
    "    optimizer = tfa.optimizers.AdamW(weight_decay=5e-5, learning_rate=1e-03, epsilon=1e-06, beta_1 = 0.9, beta_2=0.999, amsgrad=True)\n",
    "    loss = keras.losses.BinaryCrossentropy()\n",
    "    #loss = tfa.metrics.HammingLoss(mode='multilabel', threshold=0.5, name=\"hamming\")\n",
    "    #loss = tfa.losses.SigmoidFocalCrossEntropy()\n",
    "    #loss = tfa.metrics.hamming\n",
    "    \n",
    "    #metric = keras.metrics.CategoricalAccuracy('accuracy')\n",
    "    metric = tfa.metrics.HammingLoss(mode='multilabel', threshold=0.5)\n",
    "    #metric = 'acc'\n",
    "    \n",
    "    net = models.Model([\n",
    "                          abstract_ids_in\n",
    "                          #, codes_in\n",
    "                        , claim_ids_in\n",
    "                       ], output)\n",
    "    \n",
    "    net.compile(loss=loss, metrics=[metric], optimizer=optimizer)\n",
    "#     for layer in net.layers[:3]:\n",
    "#         layer.trainable = False\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import * \n",
    "\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "callbacks = [\n",
    "        ReduceLROnPlateau(monitor='loss'),\n",
    "        EarlyStopping(patience=5, monitor='val_loss'),\n",
    "        ModelCheckpoint(filepath=\"keras-model\", save_best_only=True),\n",
    "        keras.callbacks.TensorBoard(log_dir=logdir),\n",
    "        tfa.callbacks.TQDMProgressBar()\n",
    "    ]\n",
    "\n",
    "model.fit(x={   \"code_vector\": code_vectors.todense(),\n",
    "                \"abstract_tokens\": abstract_input_ids['input_ids'],\n",
    "                \"claim_tokens\": claim_input_ids['input_ids']}, \n",
    "          y=labels,\n",
    "          verbose=2, \n",
    "          epochs=100, \n",
    "          validation_split=0.2, \n",
    "          batch_size=4,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict({\"abstract_tokens\": abstract_input_ids[\"input_ids\"],\n",
    "                       \"code_vector\": code_vectors.todense(),\n",
    "                       \"claim_tokens\": claim_input_ids[\"input_ids\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.mean(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[preds>=thresholds] = 1\n",
    "preds[preds<=thresholds] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels, preds, target_names=subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds =model.predict({\n",
    "                            \"code_vector\": test_code_vectors,\n",
    "                           \"abstract_tokens\": test_abstract_input_ids[\"input_ids\"], \n",
    "                           \"claim_tokens\": test_claim_input_ids[\"input_ids\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds[test_preds>=thresholds] = 1\n",
    "test_preds[test_preds<thresholds] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, test_preds, target_names=subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
