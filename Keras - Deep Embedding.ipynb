{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import *\n",
    "from sklearn.feature_extraction.text import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import scipy.sparse as sps\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_json(\"training_set.json.gz\", lines=True, orient=\"records\")\n",
    "testing_set = pd.read_json(\"testing_set.json.gz\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_vectorizer = CountVectorizer(min_df=5, analyzer=lambda x: x)\n",
    "code_vectors = code_vectorizer.fit_transform(training_set.cpc_codes)\n",
    "test_code_vectors = code_vectorizer.transform(testing_set.cpc_codes)\n",
    "\n",
    "# text_vectorizer = TfidfVectorizer(min_df=5)\n",
    "\n",
    "# abstract_vectors = text_vectorizer.fit_transform(training_set.abstract)\n",
    "# test_abstract_vectors = text_vectorizer.transform(testing_set.abstract)\n",
    "\n",
    "# text_vectorizer = TfidfVectorizer(min_df=5)\n",
    "\n",
    "# claim_vectors = text_vectorizer.fit_transform(training_set.claims)\n",
    "# test_claim_vectors = text_vectorizer.transform(testing_set.claims)\n",
    "\n",
    "# text_vectorizer = TfidfVectorizer(min_df=2, max_df=0.5)\n",
    "\n",
    "# desc_vectors = text_vectorizer.fit_transform(training_set.description)\n",
    "# test_desc_vectors = text_vectorizer.transform(testing_set.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AnatomicalTarget_LowerExtremity',\n",
       " 'SurgicalMethod',\n",
       " 'Imaging_CT',\n",
       " 'AnatomicalTarget_Torso_Spine',\n",
       " 'Imaging_Ultrasound',\n",
       " 'AnatomicalTarget_LowerExtremity_Hip',\n",
       " 'AnatomicalTarget_LowerExtremity_Knee',\n",
       " 'PersonalizedProduct_Implant',\n",
       " 'AnatomicalTarget_UpperExtremity_Shoulder',\n",
       " 'SpecificationofUse_JointReplacement',\n",
       " 'AnatomicalTarget',\n",
       " 'Manufacturing_AdditiveManufacturing',\n",
       " 'Manufacturing',\n",
       " 'AnalysisAndModeling',\n",
       " 'SpecificationofUse',\n",
       " 'SpecificationofUse_Disease',\n",
       " 'AnatomicalTarget_Torso',\n",
       " 'AnatomicalTarget_UpperExtremity',\n",
       " 'PersonalizedProduct_Guide/Jig',\n",
       " 'Imaging_MRI',\n",
       " 'AnalysisAndModeling_3DModeling',\n",
       " 'Imaging']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = list(set(all_tiers_100)-set([\"PersonalizedProduct\"]))\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = training_set[subset].values.astype(int)\n",
    "test_labels = testing_set[subset].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(972, 22)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# # def tokenize(sentences, tokenizer):\n",
    "# #     input_ids, input_masks, input_segments = [],[],[]\n",
    "# #     for sentence in tqdm(sentences):\n",
    "# #         inputs = tokenizer.encode_plus(sentence,\n",
    "# #                                        truncation=True,\n",
    "# #                                        add_special_tokens=True, \n",
    "# #                                        max_length=256,\n",
    "# #                                        padding=True,\n",
    "# #                                        return_tensors='tf',\n",
    "# #                                        #pad_to_max_length=True,\n",
    "# #                                        return_attention_mask=False, \n",
    "# #                                        return_token_type_ids=False)\n",
    "# #         input_ids.append(inputs['input_ids'])\n",
    "# #         input_masks.append(inputs['attention_mask'])\n",
    "# #         input_segments.append(inputs['token_type_ids'])        \n",
    "        \n",
    "#     return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'bert.embeddings.position_ids', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"/var/patentmark/patentBERT/\")\n",
    "#transformer = TFAutoModel.from_pretrained(\"/var/patentmark/patentBERT/\", from_pt=True)\n",
    "#config = AutoConfig.from_pretrained(\"/var/patentmark/patentBERT/\")\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"./bertForPatents/\")\n",
    "#transformer = TFAutoModel.from_pretrained(\"./bertForPatents/\")\n",
    "#config = AutoConfig.from_pretrained(\"./bertForPatents/\")\n",
    "model_name = \"/home/martin/IdeaProjects/phenetics/bertForPatents\" #\"johngiorgi/declutr-sci-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "transformer = TFAutoModel.from_pretrained(model_name, from_pt=True)\n",
    "config = AutoConfig.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c33ecea5f64a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m claim_input_ids = tokenizer(text=training_set.claims.to_list(),\n\u001b[0m\u001b[1;32m      2\u001b[0m                             \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                            \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                            \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                            \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phenetics2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_batched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2196\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2197\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2198\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2199\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phenetics2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2380\u001b[0m         )\n\u001b[1;32m   2381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   2383\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phenetics2/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         batch_outputs = self._batch_prepare_for_model(\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phenetics2/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_prepare_for_model\u001b[0;34m(self, batch_ids_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_length, verbose)\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfirst_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_ids_pairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m             outputs = self.prepare_for_model(\n\u001b[0m\u001b[1;32m    602\u001b[0m                 \u001b[0mfirst_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0msecond_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phenetics2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mprepare_for_model\u001b[0;34m(self, ids, pair_ids, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[1;32m   2698\u001b[0m         \u001b[0moverflowing_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtruncation_strategy\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mTruncationStrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDO_NOT_TRUNCATE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtotal_len\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2700\u001b[0;31m             ids, pair_ids, overflowing_tokens = self.truncate_sequences(\n\u001b[0m\u001b[1;32m   2701\u001b[0m                 \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m                 \u001b[0mpair_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpair_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phenetics2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mtruncate_sequences\u001b[0;34m(self, ids, pair_ids, num_tokens_to_remove, truncation_strategy, stride)\u001b[0m\n\u001b[1;32m   2815\u001b[0m                         \u001b[0mwindow_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2816\u001b[0m                     \u001b[0moverflowing_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mwindow_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2817\u001b[0;31m                     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2818\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2819\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverflowing_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "claim_input_ids = tokenizer(text=training_set.claims.to_list(),\n",
    "                            truncation=True,\n",
    "                           add_special_tokens=True, \n",
    "                           max_length=max_length,\n",
    "                           padding=True,\n",
    "                           return_tensors='tf',\n",
    "                           #pad_to_max_length=True,\n",
    "                           return_attention_mask=False, \n",
    "                           return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_claim_input_ids = tokenizer(text=testing_set.claims.to_list(),\n",
    "                            truncation=True,\n",
    "                           add_special_tokens=True, \n",
    "                           max_length=max_length,\n",
    "                           padding=True,\n",
    "                           return_tensors='tf',\n",
    "                           #pad_to_max_length=True,\n",
    "                           return_attention_mask=False, \n",
    "                           return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_input_ids = tokenizer(text=training_set.abstract.to_list(),\n",
    "                            truncation=True,\n",
    "                           add_special_tokens=True, \n",
    "                           max_length=max_length,\n",
    "                           padding=True,\n",
    "                           return_tensors='tf',\n",
    "                           #pad_to_max_length=True,\n",
    "                           return_attention_mask=False, \n",
    "                           return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_abstract_input_ids = tokenizer(text=testing_set.abstract.to_list(),\n",
    "                            truncation=True,\n",
    "                           add_special_tokens=True, \n",
    "                           max_length=max_length,\n",
    "                           padding=True,\n",
    "                           return_tensors='tf',\n",
    "                           #pad_to_max_length=True,\n",
    "                           return_attention_mask=False, \n",
    "                           return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claim_input_ids, claim_masks, _ = tokenize(training_set.claims, tokenizer)\n",
    "# test_claim_input_ids, test_claim_masks, _ = tokenize(testing_set.claims, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    #claim_input = layers.Input(shape=(claim_vectors.shape[1]))\n",
    "    #description_input = layers.Input(shape=(desc_vectors.shape[1]))\n",
    "    \n",
    "    #codes_in = layers.Input(shape=(code_vectors.shape[1]), dtype='int32', name='code_vector')\n",
    "    #code_embedding = layers.Dense(16)(codes_in)\n",
    "    \n",
    "    claim_ids_in = layers.Input(shape=(max_length,), dtype='int32', name=\"claim_tokens\")\n",
    "    abstract_ids_in = layers.Input(shape=(max_length,), dtype='int32', name=\"abstract_tokens\")\n",
    "    #claim_masks_in = layers.Input(shape=(claim_masks.shape[1]), dtype='int32')\n",
    "    claim_embedding_layer = transformer(claim_ids_in)[1]\n",
    "    abstract_embedding_layer = transformer(abstract_ids_in)[1]\n",
    "    \n",
    "    embedding_layers = layers.Concatenate()([abstract_embedding_layer\n",
    "                                             , claim_embedding_layer\n",
    "                                             #, code_embedding\n",
    "                                            ])\n",
    "    #, attention_mask=claim_masks_in)[0]\n",
    "    #claim_embedding_layer = layers.GlobalAveragePooling1D()(claim_embedding_layer)\n",
    "    \n",
    "    #concat_layer = layers.Concatenate()([claim_input])\n",
    "    #dropout = layers.Dropout(config.hidden_dropout_prob)(claim_embedding_layer, training=False)\n",
    "    \n",
    "    #abstract_input = layers.Input(shape=(abstract_vectors.shape[1]), name=\"abstract_vectors\")\n",
    "    #claim_input = layers.Input(shape=(claim_vectors.shape[1]), name=\"claim_vectors\")\n",
    "    \n",
    "#     concat = layers.Concatenate()((abstract_input, claim_input))\n",
    "#     dropout0 = layers.Dropout(0.7)(concat)\n",
    "#     dense0 = layers.Dense(64, activation='elu')(dropout0)\n",
    "#     dropout1 = layers.Dropout(0.7)(dense0)\n",
    "    dropout = layers.Dropout(0.5)(embedding_layers)\n",
    "    dense = layers.Dense(64, activation='elu')(dropout)\n",
    "    output = layers.Dense(units=len(subset), activation='sigmoid')(dense)\n",
    "    \n",
    "                          #kernel_initializer=keras.initializers.TruncatedNormal(stddev=config.initializer_range))(dense0)\n",
    "    \n",
    "    #, activation='sigmoid')(dropout)\n",
    "    \n",
    "    optimizer = tfa.optimizers.AdamW(weight_decay=5e-5, learning_rate=1e-03, epsilon=1e-06, beta_1 = 0.9, beta_2=0.999, amsgrad=True)\n",
    "    loss = keras.losses.BinaryCrossentropy()\n",
    "    #loss = tfa.metrics.HammingLoss(mode='multilabel', threshold=0.5, name=\"hamming\")\n",
    "    #loss = tfa.losses.SigmoidFocalCrossEntropy()\n",
    "    #loss = tfa.metrics.hamming\n",
    "    \n",
    "    #metric = keras.metrics.CategoricalAccuracy('accuracy')\n",
    "    metric = tfa.metrics.HammingLoss(mode='multilabel', threshold=0.5)\n",
    "    #metric = 'acc'\n",
    "    \n",
    "    net = models.Model([\n",
    "                          abstract_ids_in\n",
    "                          #, codes_in\n",
    "                        , claim_ids_in\n",
    "                       ], output)\n",
    "    \n",
    "    net.compile(loss=loss, metrics=[metric], optimizer=optimizer)\n",
    "#     for layer in net.layers[:3]:\n",
    "#         layer.trainable = False\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import * \n",
    "\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "callbacks = [\n",
    "        ReduceLROnPlateau(monitor='loss'),\n",
    "        EarlyStopping(patience=5, monitor='val_loss'),\n",
    "        ModelCheckpoint(filepath=\"keras-model\", save_best_only=True),\n",
    "        keras.callbacks.TensorBoard(log_dir=logdir),\n",
    "        tfa.callbacks.TQDMProgressBar()\n",
    "    ]\n",
    "\n",
    "model.fit(x={   \"code_vector\": code_vectors.todense(),\n",
    "                \"abstract_tokens\": abstract_input_ids['input_ids'],\n",
    "                \"claim_tokens\": claim_input_ids['input_ids']}, \n",
    "          y=labels,\n",
    "          verbose=2, \n",
    "          epochs=100, \n",
    "          validation_split=0.2, \n",
    "          batch_size=4,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict({\"abstract_tokens\": abstract_input_ids[\"input_ids\"],\n",
    "                       \"code_vector\": code_vectors.todense(),\n",
    "                       \"claim_tokens\": claim_input_ids[\"input_ids\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.mean(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[preds>=thresholds] = 1\n",
    "preds[preds<=thresholds] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels, preds, target_names=subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds =model.predict({\n",
    "                            \"code_vector\": test_code_vectors,\n",
    "                           \"abstract_tokens\": test_abstract_input_ids[\"input_ids\"], \n",
    "                           \"claim_tokens\": test_claim_input_ids[\"input_ids\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds[test_preds>=thresholds] = 1\n",
    "test_preds[test_preds<thresholds] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, test_preds, target_names=subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
