{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from catalyst import dl\n",
    "from catalyst import dl, utils\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_json(\"training_set.json.gz\", lines=True, orient=\"records\")\n",
    "testing_set = pd.read_json(\"testing_set.json.gz\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = list(sorted(set(all_tiers_100)-set([\"PersonalizedProduct\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"/home/martin/IdeaProjects/phenetics/bertForPatents/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModel.from_pretrained(model_name, gradient_checkpointing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['labels']=training_set[subset].astype(int).values.tolist()\n",
    "testing_set['labels']=testing_set[subset].astype(int).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Analysis and Modeling',\n",
       " 'Analysis and Modeling: 3D Modeling',\n",
       " 'Anatomical Target',\n",
       " 'Anatomical Target: Lower Extremity',\n",
       " 'Anatomical Target: Lower Extremity - Hip',\n",
       " 'Anatomical Target: Lower Extremity - Knee',\n",
       " 'Anatomical Target: Torso',\n",
       " 'Anatomical Target: Torso - Spine',\n",
       " 'Anatomical Target: Upper Extremity',\n",
       " 'Anatomical Target: Upper Extremity - Shoulder',\n",
       " 'Imaging',\n",
       " 'Imaging: CT',\n",
       " 'Imaging: MRI',\n",
       " 'Imaging: Ultrasound',\n",
       " 'Manufacturing',\n",
       " 'Manufacturing: Additive Manufacturing',\n",
       " 'Personalized Product: Guide or Jig',\n",
       " 'Personalized Product: Implant',\n",
       " 'Specification of Use',\n",
       " 'Specification of Use: Disease',\n",
       " 'Specification of Use: Joint Replacement',\n",
       " 'Surgical Method']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nice_subset = [tier_translations[x] for x in subset]\n",
    "nice_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0.09129103335241477, -0.8074875394503276, -0....\n",
       "1      [-0.0626441298850945, -0.8264780470303127, -0....\n",
       "2      [-0.2087969978650411, -0.8326806823412577, -0....\n",
       "3      [0.020394775830209256, -0.8215901732444764, -0...\n",
       "4      [-0.26043402403593063, -0.6891247034072876, -0...\n",
       "                             ...                        \n",
       "238    [-0.23802674313386282, -0.628900408744812, -0....\n",
       "239    [-0.3754243354002635, -0.6894144614537557, -0....\n",
       "240    [-0.12913421913981438, -0.6960149183869362, -0...\n",
       "241    [-0.3880331997688, -0.702021429171929, -0.2717...\n",
       "242    [-0.2977850042283535, -0.7015813589096069, -0....\n",
       "Name: embedded_cpc, Length: 243, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpc_embeddings = np.fromfile(\"/home/martin/patentmark/cpc.node2vec.emb.32d.bin\", dtype=np.float32).reshape((-1,32))\n",
    "\n",
    "import joblib\n",
    "cpc_labelizer = joblib.load('./node2id.joblib')\n",
    "cpc_lookup = {c: n for n, c in enumerate(cpc_labelizer.classes_)}\n",
    "\n",
    "@f.collecting\n",
    "def convert_cpc_codes(codes):\n",
    "    for code in codes:\n",
    "        if code in cpc_lookup:\n",
    "            yield cpc_lookup[code]\n",
    "    \n",
    "def embed_cpc_codes(codes):\n",
    "    embedding = np.zeros(32)\n",
    "    converted = convert_cpc_codes(codes)\n",
    "    \n",
    "    if not converted:\n",
    "        return embedding\n",
    "    \n",
    "    for code_id in converted:\n",
    "        embedding = embedding + cpc_embeddings[code_id]\n",
    "        \n",
    "    return embedding / len(converted)\n",
    "\n",
    "training_set['embedded_cpc'] = training_set.cpc_codes.apply(embed_cpc_codes)\n",
    "training_set.embedded_cpc\n",
    "\n",
    "testing_set['embedded_cpc'] = testing_set.cpc_codes.apply(embed_cpc_codes)\n",
    "testing_set.embedded_cpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN_CLAIMS = 512\n",
    "MAX_LEN_ABSTRACT = 160\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-5\n",
    "SEED = 17\n",
    "PRED_THRES = 0.4\n",
    "ACCUM_STEPS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#cpc_coder = CountVectorizer(analyzer=cpc_split, min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['citing'] = training_set[['citations', 'cited_by']].apply(\n",
    "        lambda row: list(set(row['citations']+row['cited_by'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_set['citing'] = testing_set[['citations', 'cited_by']].apply(\n",
    "        lambda row: list(set(row['citations']+row['cited_by'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['people'] = training_set[['assignees', 'inventors']].apply(lambda row: list(set(row['assignees']+row['inventors'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_set['people'] = testing_set[['assignees', 'inventors']].apply(lambda row: list(set(row['assignees']+row['inventors'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format(t):\n",
    "    CORP_TYPES = set(\n",
    "        [\n",
    "            \"INC\",\n",
    "            \"LLC\" \"CORP\",\n",
    "            \"KK\",\n",
    "            \"SA\",\n",
    "            \"SRL\",\n",
    "            \"LTD\",\n",
    "            \"NL\",\n",
    "            \"PTY\",\n",
    "            \"AG\",\n",
    "            \"GMBH\",\n",
    "            \"KG\",\n",
    "            \"OG\",\n",
    "            \"LIMITED\",\n",
    "            \"SARL\",\n",
    "            \"BM\",\n",
    "            \"PLC\",\n",
    "            \"LP\",\n",
    "            \"IP\",\n",
    "            \"DBA\",\n",
    "            \"CORP\",\n",
    "            \"CO\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    tokenized = strip_non_alphanum(strip_punctuation(t)).upper().split(\" \")\n",
    "    cleaned = [t for t in tokenized if t not in CORP_TYPES]\n",
    "    return \"\".join(cleaned)\n",
    "\n",
    "\n",
    "people_coder = CountVectorizer(analyzer=lambda x: map(format, x), min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "citing_coder = CountVectorizer(analyzer=lambda x: x, min_df=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2076"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citing_coder.fit(training_set.citing)\n",
    "len(citing_coder.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=<function <lambda> at 0x7f8000ea6310>, min_df=2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_coder.fit(training_set.people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(people_coder.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_set['cpc_vec'] = list(cpc_coder.transform(training_set.cpc_codes).todense())\n",
    "#testing_set['cpc_vec'] = list(cpc_coder.transform(testing_set.cpc_codes).todense())\n",
    "training_set['people_vec'] = list(np.array(people_coder.transform(training_set.people).todense()))\n",
    "testing_set['people_vec'] = list(np.array(people_coder.transform(testing_set.people).todense()))\n",
    "training_set['citing_vec'] = list(np.array(citing_coder.transform(training_set.citing).todense()))\n",
    "testing_set['citing_vec'] = list(np.array(citing_coder.transform(testing_set.citing).todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, ...\n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2      [1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...\n",
       "3      [1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, ...\n",
       "4      [0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "                             ...                        \n",
       "967    [1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, ...\n",
       "968    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...\n",
       "969    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...\n",
       "970    [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, ...\n",
       "971    [0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, ...\n",
       "Name: labels, Length: 972, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set = training_set.explode('labels').reset_index()\n",
    "# testing_set = testing_set.explode('labels').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_set.labels.str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_labels = set([tier_translations[x] for x in subset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Analysis and Modeling',\n",
       " 'Analysis and Modeling: 3D Modeling',\n",
       " 'Anatomical Target',\n",
       " 'Anatomical Target: Lower Extremity',\n",
       " 'Anatomical Target: Lower Extremity - Hip',\n",
       " 'Anatomical Target: Lower Extremity - Knee',\n",
       " 'Anatomical Target: Torso',\n",
       " 'Anatomical Target: Torso - Spine',\n",
       " 'Anatomical Target: Upper Extremity',\n",
       " 'Anatomical Target: Upper Extremity - Shoulder',\n",
       " 'Imaging',\n",
       " 'Imaging: CT',\n",
       " 'Imaging: MRI',\n",
       " 'Imaging: Ultrasound',\n",
       " 'Manufacturing',\n",
       " 'Manufacturing: Additive Manufacturing',\n",
       " 'Personalized Product: Guide or Jig',\n",
       " 'Personalized Product: Implant',\n",
       " 'Specification of Use',\n",
       " 'Specification of Use: Disease',\n",
       " 'Specification of Use: Joint Replacement',\n",
       " 'Surgical Method'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokenizer, text, max_len):\n",
    "        text = str(text)\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            f\"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            f\"attention_mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            f\"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "        }\n",
    "    \n",
    "def tokenize_list(tokenizer, text_list, max_len):\n",
    "        \n",
    "        inputs = tokenizer.batch_encode_plus(\n",
    "            text_list,\n",
    "            #None,\n",
    "            #add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            f\"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            f\"attention_mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            f\"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "subset_tokenized = tokenize_list(tokenizer, nice_subset, 56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,  3771,  1663,  ...,     0,     0,     0],\n",
       "        [    2,  3771,  1663,  ...,     0,     0,     0],\n",
       "        [    2, 27806,  4204,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    2, 12492,  1662,  ...,     0,     0,     0],\n",
       "        [    2, 12492,  1662,  ...,     0,     0,     0],\n",
       "        [    2, 11372,  3783,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_tokenized['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fse\n",
    "# import gensim.downloader as api\n",
    "# glove = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fse import IndexedList, SplitIndexedList\n",
    "# from fse.models import uSIF\n",
    "# s = SplitIndexedList(nice_subset)\n",
    "# label_model = uSIF(glove, workers=32, lang_freq=\"en\")\n",
    "# label_model.train(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset_embeddings = label_model.infer(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        \n",
    "        self.claims = dataframe.claims\n",
    "        self.abstracts = dataframe.abstract\n",
    "        \n",
    "        self.labels = dataframe.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        abstract = tokenize(tokenizer, self.abstracts[index], max_len=MAX_LEN_ABSTRACT)\n",
    "        claims = tokenize(tokenizer, self.claims[index], MAX_LEN_CLAIMS)\n",
    "        \n",
    "        labels = torch.tensor(np.array(self.labels[index]), dtype=torch.float)\n",
    "        \n",
    "        people = torch.tensor(np.array(self.data.people_vec[index]), dtype=torch.float)\n",
    "        citing = torch.tensor(np.array(self.data.citing_vec[index]), dtype=torch.float)\n",
    "        embedded_cpc = torch.tensor(np.array(self.data.embedded_cpc[index]), dtype=torch.float)        \n",
    "                \n",
    "        return {\"abstract\": abstract, \n",
    "                \"claims\": claims, \n",
    "                \n",
    "                #'cpcs': cpcs,\n",
    "                 'people': people,\n",
    "                 'citing': citing,\n",
    "                 'embedded_cpc': embedded_cpc,\n",
    "                 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = MultiLabelDataset(training_set, tokenizer)\n",
    "testing_dataset = MultiLabelDataset(testing_set, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_dataset, **train_params)\n",
    "testing_loader = DataLoader(testing_dataset, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from pytorch_metric_learning import miners, losses\n",
    "\n",
    "NUM_LABELS = len(nice_subset)\n",
    "import catalyst.contrib as contrib\n",
    "device = utils.get_device()\n",
    "from datetime import datetime\n",
    "logdir=\"/var/patentmark/logdir/fit2/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir /var/patentmark/logdir/fit/ --bind_all\n",
    "\n",
    "class PatentModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.text_embedder = AutoModel.from_pretrained(model_name, gradient_checkpointing=True)\n",
    "        \n",
    "        self.people_embedder = torch.nn.Linear(len(people_coder.vocabulary_), 64)\n",
    "        self.citing_embedder = torch.nn.Linear(len(citing_coder.vocabulary_), 64)\n",
    "        \n",
    "        total_embedding_size = self.text_embedder.pooler.dense.out_features*2+32+64*2\n",
    "        output_size = 256 #self.text_embedder.pooler.dense.out_features\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        \n",
    "        self.dense1 = nn.Linear(total_embedding_size, output_size)\n",
    "        self.dense1label = nn.Linear(self.text_embedder.pooler.dense.out_features, output_size)\n",
    "        \n",
    "        self.categorizer = nn.Linear(output_size, NUM_LABELS)\n",
    "\n",
    "    \n",
    "    def encode_label(self, label):\n",
    "        label_emb = self.text_embedder(input_ids=label[\"input_ids\"].to(device), attention_mask=label[\"attention_mask\"].to(device))\n",
    "        label_emb = label_emb[0][:,0]\n",
    "        \n",
    "        x = self.dropout1(label_emb)\n",
    "        x = F.elu(self.dense1label(x))\n",
    "        return x\n",
    "    \n",
    "    def predict_classes(self, embeddings):\n",
    "        x = self.dropout1(embeddings)\n",
    "        x = self.categorizer(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "    def encode_patent(self, abstract, claims, embedded_cpc, people, citing):\n",
    "        \n",
    "        abstract_emb = self.text_embedder(input_ids=abstract[\"input_ids\"].to(device), \n",
    "                                          attention_mask=abstract[\"attention_mask\"].to(device))\n",
    "        abstract_emb = abstract_emb[0][:, 0]\n",
    "        \n",
    "        claim_emb = self.text_embedder(input_ids=claims[\"input_ids\"].to(device), \n",
    "                                       attention_mask=claims[\"attention_mask\"].to(device))\n",
    "        claim_emb = claim_emb[0][:, 0]\n",
    "        \n",
    "        people_emb = F.elu(self.people_embedder(people.to(device)))\n",
    "        citing_emb = F.elu(self.citing_embedder(citing.to(device)))\n",
    "    \n",
    "        x = torch.cat((abstract_emb, claim_emb, embedded_cpc.to(device), people_emb, citing_emb), 1)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.elu(self.dense1(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = PatentModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_no: 0, epoch: 0, batch_no: 0, loss: 1.1900696754455566, avg_loss: 1.1900696754455566\n",
      "step_no: 1, epoch: 0, batch_no: 1, loss: 0.7996355295181274, avg_loss: 0.994852602481842\n",
      "step_no: 2, epoch: 0, batch_no: 2, loss: 1.1026958227157593, avg_loss: 1.0308003425598145\n",
      "step_no: 3, epoch: 0, batch_no: 3, loss: 1.6710759401321411, avg_loss: 1.1908692121505737\n",
      "step_no: 4, epoch: 0, batch_no: 4, loss: 1.3944175243377686, avg_loss: 1.2315788269042969\n",
      "step_no: 5, epoch: 0, batch_no: 5, loss: 3.112457513809204, avg_loss: 1.5450586080551147\n",
      "step_no: 6, epoch: 0, batch_no: 6, loss: 0.647013783454895, avg_loss: 1.4167665243148804\n",
      "step_no: 7, epoch: 0, batch_no: 7, loss: 1.9238325357437134, avg_loss: 1.480149745941162\n",
      "step_no: 8, epoch: 0, batch_no: 8, loss: 1.6977035999298096, avg_loss: 1.5043224096298218\n",
      "step_no: 9, epoch: 0, batch_no: 9, loss: 0.6345551013946533, avg_loss: 1.417345643043518\n",
      "step_no: 10, epoch: 0, batch_no: 10, loss: 1.1138719320297241, avg_loss: 1.3897571563720703\n",
      "step_no: 11, epoch: 0, batch_no: 11, loss: 1.961198091506958, avg_loss: 1.4373772144317627\n",
      "step_no: 12, epoch: 0, batch_no: 12, loss: 0.7572514414787292, avg_loss: 1.3850598335266113\n",
      "step_no: 13, epoch: 0, batch_no: 13, loss: 0.9206905364990234, avg_loss: 1.3518906831741333\n",
      "step_no: 14, epoch: 0, batch_no: 14, loss: 0.7860274910926819, avg_loss: 1.3141664266586304\n",
      "step_no: 15, epoch: 0, batch_no: 15, loss: 1.1139394044876099, avg_loss: 1.301652193069458\n",
      "step_no: 16, epoch: 0, batch_no: 16, loss: 0.7054687738418579, avg_loss: 1.2665826082229614\n",
      "step_no: 17, epoch: 0, batch_no: 17, loss: 0.8256227374076843, avg_loss: 1.2420848608016968\n",
      "step_no: 18, epoch: 0, batch_no: 18, loss: 0.9116119146347046, avg_loss: 1.2246915102005005\n",
      "step_no: 19, epoch: 0, batch_no: 19, loss: 1.359380841255188, avg_loss: 1.2314260005950928\n",
      "step_no: 20, epoch: 0, batch_no: 20, loss: 1.3029495477676392, avg_loss: 1.2348319292068481\n",
      "step_no: 21, epoch: 0, batch_no: 21, loss: 0.7586331963539124, avg_loss: 1.213186502456665\n",
      "step_no: 22, epoch: 0, batch_no: 22, loss: 1.022121787071228, avg_loss: 1.2048792839050293\n",
      "step_no: 23, epoch: 0, batch_no: 23, loss: 0.8994572758674622, avg_loss: 1.1921534538269043\n",
      "step_no: 24, epoch: 0, batch_no: 24, loss: 1.1245321035385132, avg_loss: 1.1894484758377075\n",
      "step_no: 25, epoch: 0, batch_no: 25, loss: 0.8006187081336975, avg_loss: 1.174493670463562\n",
      "step_no: 26, epoch: 0, batch_no: 26, loss: 0.4815719425678253, avg_loss: 1.1488298177719116\n",
      "step_no: 27, epoch: 0, batch_no: 27, loss: 1.4131262302398682, avg_loss: 1.158268928527832\n",
      "step_no: 28, epoch: 0, batch_no: 28, loss: 1.1117870807647705, avg_loss: 1.1566660404205322\n",
      "step_no: 29, epoch: 0, batch_no: 29, loss: 0.47177526354789734, avg_loss: 1.1338363885879517\n",
      "step_no: 30, epoch: 0, batch_no: 30, loss: 0.7346382141113281, avg_loss: 1.120958924293518\n",
      "step_no: 31, epoch: 0, batch_no: 31, loss: 0.7351827025413513, avg_loss: 1.1089035272598267\n",
      "step_no: 32, epoch: 0, batch_no: 32, loss: 1.0822536945343018, avg_loss: 1.1080960035324097\n",
      "step_no: 33, epoch: 0, batch_no: 33, loss: 1.5563771724700928, avg_loss: 1.1212806701660156\n",
      "step_no: 34, epoch: 0, batch_no: 34, loss: 1.0367956161499023, avg_loss: 1.1188668012619019\n",
      "step_no: 35, epoch: 0, batch_no: 35, loss: 0.7951646447181702, avg_loss: 1.109875202178955\n",
      "step_no: 36, epoch: 0, batch_no: 36, loss: 0.9218708872795105, avg_loss: 1.1047940254211426\n",
      "step_no: 37, epoch: 0, batch_no: 37, loss: 0.7445108890533447, avg_loss: 1.0953128337860107\n",
      "step_no: 38, epoch: 0, batch_no: 38, loss: 0.8097091317176819, avg_loss: 1.0879895687103271\n",
      "step_no: 39, epoch: 0, batch_no: 39, loss: 0.7997502684593201, avg_loss: 1.080783724784851\n",
      "step_no: 40, epoch: 0, batch_no: 40, loss: 0.7596850991249084, avg_loss: 1.0729519128799438\n",
      "step_no: 41, epoch: 0, batch_no: 41, loss: 0.7190603017807007, avg_loss: 1.0645259618759155\n",
      "step_no: 42, epoch: 0, batch_no: 42, loss: 0.9670516848564148, avg_loss: 1.0622591972351074\n",
      "step_no: 43, epoch: 0, batch_no: 43, loss: 0.49066081643104553, avg_loss: 1.049268364906311\n",
      "step_no: 44, epoch: 0, batch_no: 44, loss: 0.8567526340484619, avg_loss: 1.0449901819229126\n",
      "step_no: 45, epoch: 0, batch_no: 45, loss: 1.0336356163024902, avg_loss: 1.044743299484253\n",
      "step_no: 46, epoch: 0, batch_no: 46, loss: 0.5360240936279297, avg_loss: 1.0339194536209106\n",
      "step_no: 47, epoch: 0, batch_no: 47, loss: 1.1561973094940186, avg_loss: 1.0364669561386108\n",
      "step_no: 48, epoch: 0, batch_no: 48, loss: 1.238675594329834, avg_loss: 1.0405936241149902\n",
      "step_no: 49, epoch: 0, batch_no: 49, loss: 1.236002802848816, avg_loss: 1.044501781463623\n",
      "step_no: 50, epoch: 0, batch_no: 50, loss: 0.7636265158653259, avg_loss: 1.0389944314956665\n",
      "step_no: 51, epoch: 0, batch_no: 51, loss: 1.3099781274795532, avg_loss: 1.044205665588379\n",
      "step_no: 52, epoch: 0, batch_no: 52, loss: 0.33632394671440125, avg_loss: 1.0308494567871094\n",
      "step_no: 53, epoch: 0, batch_no: 53, loss: 0.8616684079170227, avg_loss: 1.0277163982391357\n",
      "step_no: 54, epoch: 0, batch_no: 54, loss: 0.34829166531562805, avg_loss: 1.0153632164001465\n",
      "step_no: 55, epoch: 0, batch_no: 55, loss: 1.5988537073135376, avg_loss: 1.025782823562622\n",
      "step_no: 56, epoch: 0, batch_no: 56, loss: 0.5591365694999695, avg_loss: 1.017595887184143\n",
      "step_no: 57, epoch: 0, batch_no: 57, loss: 0.9778253436088562, avg_loss: 1.016910195350647\n",
      "step_no: 58, epoch: 0, batch_no: 58, loss: 0.7929204702377319, avg_loss: 1.0131137371063232\n",
      "step_no: 59, epoch: 0, batch_no: 59, loss: 0.5715336799621582, avg_loss: 1.0057541131973267\n",
      "step_no: 60, epoch: 0, batch_no: 60, loss: 0.975348174571991, avg_loss: 1.0052555799484253\n",
      "step_no: 61, epoch: 0, batch_no: 61, loss: 0.6941100358963013, avg_loss: 1.0002371072769165\n",
      "step_no: 62, epoch: 0, batch_no: 62, loss: 0.992344081401825, avg_loss: 1.0001119375228882\n",
      "step_no: 63, epoch: 0, batch_no: 63, loss: 0.7455068230628967, avg_loss: 0.9961336851119995\n",
      "step_no: 64, epoch: 0, batch_no: 64, loss: 0.6187201738357544, avg_loss: 0.9903273582458496\n",
      "step_no: 65, epoch: 0, batch_no: 65, loss: 0.909462571144104, avg_loss: 0.9891021251678467\n",
      "step_no: 66, epoch: 0, batch_no: 66, loss: 0.6888309717178345, avg_loss: 0.9846203923225403\n",
      "step_no: 67, epoch: 0, batch_no: 67, loss: 0.5264061689376831, avg_loss: 0.9778819680213928\n",
      "step_no: 68, epoch: 0, batch_no: 68, loss: 0.7430020570755005, avg_loss: 0.9744779467582703\n",
      "step_no: 69, epoch: 0, batch_no: 69, loss: 0.41913437843322754, avg_loss: 0.9665444493293762\n",
      "step_no: 70, epoch: 0, batch_no: 70, loss: 0.8305923342704773, avg_loss: 0.964629590511322\n",
      "step_no: 71, epoch: 0, batch_no: 71, loss: 0.9668277502059937, avg_loss: 0.964660108089447\n",
      "step_no: 72, epoch: 0, batch_no: 72, loss: 0.5026116967201233, avg_loss: 0.9583306312561035\n",
      "step_no: 73, epoch: 0, batch_no: 73, loss: 0.5452165603637695, avg_loss: 0.9527480602264404\n",
      "step_no: 74, epoch: 0, batch_no: 74, loss: 0.7321264147758484, avg_loss: 0.9498064517974854\n",
      "step_no: 75, epoch: 0, batch_no: 75, loss: 0.8783102631568909, avg_loss: 0.9488657116889954\n",
      "step_no: 76, epoch: 0, batch_no: 76, loss: 0.5533390641212463, avg_loss: 0.9437289834022522\n",
      "step_no: 77, epoch: 0, batch_no: 77, loss: 0.9716095924377441, avg_loss: 0.9440864324569702\n",
      "step_no: 78, epoch: 0, batch_no: 78, loss: 0.6127498745918274, avg_loss: 0.9398922920227051\n",
      "step_no: 79, epoch: 0, batch_no: 79, loss: 0.792391836643219, avg_loss: 0.9380484819412231\n",
      "step_no: 80, epoch: 0, batch_no: 80, loss: 0.8832561373710632, avg_loss: 0.9373720288276672\n",
      "step_no: 81, epoch: 0, batch_no: 81, loss: 0.49050506949424744, avg_loss: 0.9319223165512085\n",
      "step_no: 82, epoch: 0, batch_no: 82, loss: 0.848300039768219, avg_loss: 0.9309147596359253\n",
      "step_no: 83, epoch: 0, batch_no: 83, loss: 0.6451364159584045, avg_loss: 0.9275127053260803\n",
      "step_no: 84, epoch: 0, batch_no: 84, loss: 0.8169722557067871, avg_loss: 0.9262121915817261\n",
      "step_no: 85, epoch: 0, batch_no: 85, loss: 0.546481192111969, avg_loss: 0.9217966794967651\n",
      "step_no: 86, epoch: 0, batch_no: 86, loss: 0.9478795528411865, avg_loss: 0.9220964312553406\n",
      "step_no: 87, epoch: 0, batch_no: 87, loss: 0.5415822267532349, avg_loss: 0.9177724123001099\n",
      "step_no: 88, epoch: 0, batch_no: 88, loss: 0.4892277121543884, avg_loss: 0.9129572510719299\n",
      "step_no: 89, epoch: 0, batch_no: 89, loss: 1.1528220176696777, avg_loss: 0.9156224727630615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_no: 90, epoch: 0, batch_no: 90, loss: 1.1137853860855103, avg_loss: 0.9178000688552856\n",
      "step_no: 91, epoch: 0, batch_no: 91, loss: 0.9466953873634338, avg_loss: 0.9181141257286072\n",
      "step_no: 92, epoch: 0, batch_no: 92, loss: 0.31674885749816895, avg_loss: 0.9116478562355042\n",
      "step_no: 93, epoch: 0, batch_no: 93, loss: 0.6594262719154358, avg_loss: 0.9089645743370056\n",
      "step_no: 94, epoch: 0, batch_no: 94, loss: 0.7873609066009521, avg_loss: 0.9076846241950989\n",
      "step_no: 95, epoch: 0, batch_no: 95, loss: 1.043910264968872, avg_loss: 0.9091035723686218\n",
      "step_no: 96, epoch: 0, batch_no: 96, loss: 1.1328743696212769, avg_loss: 0.9114104509353638\n",
      "step_no: 97, epoch: 0, batch_no: 97, loss: 0.7867861986160278, avg_loss: 0.9101387858390808\n",
      "step_no: 98, epoch: 0, batch_no: 98, loss: 0.5760913491249084, avg_loss: 0.906764566898346\n",
      "step_no: 99, epoch: 0, batch_no: 99, loss: 1.7689217329025269, avg_loss: 0.9153860807418823\n",
      "0.38159776180678046\n",
      "step_no: 100, epoch: 0, batch_no: 100, loss: 0.852558434009552, avg_loss: 0.9147641062736511\n",
      "step_no: 101, epoch: 0, batch_no: 101, loss: 0.9310020208358765, avg_loss: 0.9149233102798462\n",
      "step_no: 102, epoch: 0, batch_no: 102, loss: 0.5981241464614868, avg_loss: 0.9118475317955017\n",
      "step_no: 103, epoch: 0, batch_no: 103, loss: 0.8507816195487976, avg_loss: 0.9112604260444641\n",
      "step_no: 104, epoch: 0, batch_no: 104, loss: 0.7864252328872681, avg_loss: 0.9100714921951294\n",
      "step_no: 105, epoch: 0, batch_no: 105, loss: 0.9829673767089844, avg_loss: 0.9107591509819031\n",
      "step_no: 106, epoch: 0, batch_no: 106, loss: 1.0629007816314697, avg_loss: 0.91218101978302\n",
      "step_no: 107, epoch: 0, batch_no: 107, loss: 0.729412853717804, avg_loss: 0.9104887843132019\n",
      "step_no: 108, epoch: 0, batch_no: 108, loss: 1.0520055294036865, avg_loss: 0.9117870330810547\n",
      "step_no: 109, epoch: 0, batch_no: 109, loss: 0.7408940196037292, avg_loss: 0.9102334380149841\n",
      "step_no: 110, epoch: 0, batch_no: 110, loss: 1.2148736715316772, avg_loss: 0.9129779934883118\n",
      "step_no: 111, epoch: 0, batch_no: 111, loss: 0.6810685992240906, avg_loss: 0.9109073877334595\n",
      "step_no: 112, epoch: 0, batch_no: 112, loss: 0.5984355807304382, avg_loss: 0.90814208984375\n",
      "step_no: 113, epoch: 0, batch_no: 113, loss: 0.6272080540657043, avg_loss: 0.9056777358055115\n",
      "step_no: 114, epoch: 0, batch_no: 114, loss: 0.6170887351036072, avg_loss: 0.9031682014465332\n",
      "step_no: 115, epoch: 0, batch_no: 115, loss: 0.8506138324737549, avg_loss: 0.9027152061462402\n",
      "step_no: 116, epoch: 0, batch_no: 116, loss: 1.193719506263733, avg_loss: 0.9052024483680725\n",
      "step_no: 117, epoch: 0, batch_no: 117, loss: 0.7599923014640808, avg_loss: 0.9039718508720398\n",
      "step_no: 118, epoch: 0, batch_no: 118, loss: 1.0401946306228638, avg_loss: 0.905116617679596\n",
      "step_no: 119, epoch: 0, batch_no: 119, loss: 0.7656463384628296, avg_loss: 0.9039543867111206\n",
      "step_no: 120, epoch: 0, batch_no: 120, loss: 0.5144157409667969, avg_loss: 0.9007349610328674\n",
      "step_no: 121, epoch: 0, batch_no: 121, loss: 0.580836296081543, avg_loss: 0.8981128334999084\n",
      "step_no: 122, epoch: 0, batch_no: 122, loss: 0.8076469898223877, avg_loss: 0.8973773717880249\n",
      "step_no: 123, epoch: 0, batch_no: 123, loss: 0.8384088277816772, avg_loss: 0.8969017863273621\n",
      "step_no: 124, epoch: 0, batch_no: 124, loss: 1.014242172241211, avg_loss: 0.8978406190872192\n",
      "step_no: 125, epoch: 0, batch_no: 125, loss: 0.9762406945228577, avg_loss: 0.8984628915786743\n",
      "step_no: 126, epoch: 0, batch_no: 126, loss: 0.6581064462661743, avg_loss: 0.8965702056884766\n",
      "step_no: 127, epoch: 0, batch_no: 127, loss: 0.8257401585578918, avg_loss: 0.8960168361663818\n",
      "step_no: 128, epoch: 0, batch_no: 128, loss: 1.0491262674331665, avg_loss: 0.8972037434577942\n",
      "step_no: 129, epoch: 0, batch_no: 129, loss: 0.8188568353652954, avg_loss: 0.8966010212898254\n",
      "step_no: 130, epoch: 0, batch_no: 130, loss: 0.8461178541183472, avg_loss: 0.8962156772613525\n",
      "step_no: 131, epoch: 0, batch_no: 131, loss: 0.6422856450080872, avg_loss: 0.8942919969558716\n",
      "step_no: 132, epoch: 0, batch_no: 132, loss: 0.6312608122825623, avg_loss: 0.8923143148422241\n",
      "step_no: 133, epoch: 0, batch_no: 133, loss: 0.7804110646247864, avg_loss: 0.8914791941642761\n",
      "step_no: 134, epoch: 0, batch_no: 134, loss: 0.5330037474632263, avg_loss: 0.8888238072395325\n",
      "step_no: 135, epoch: 0, batch_no: 135, loss: 0.7105998396873474, avg_loss: 0.8875133991241455\n",
      "step_no: 136, epoch: 0, batch_no: 136, loss: 0.7512056231498718, avg_loss: 0.8865184187889099\n",
      "step_no: 137, epoch: 0, batch_no: 137, loss: 0.8027539253234863, avg_loss: 0.8859114646911621\n",
      "step_no: 138, epoch: 0, batch_no: 138, loss: 0.8247299790382385, avg_loss: 0.8854713439941406\n",
      "step_no: 139, epoch: 0, batch_no: 139, loss: 0.6815453171730042, avg_loss: 0.8840147256851196\n",
      "step_no: 140, epoch: 0, batch_no: 140, loss: 0.7376177310943604, avg_loss: 0.8829764127731323\n",
      "step_no: 141, epoch: 0, batch_no: 141, loss: 0.8882065415382385, avg_loss: 0.8830132484436035\n",
      "step_no: 142, epoch: 0, batch_no: 142, loss: 0.5986759066581726, avg_loss: 0.8810248970985413\n",
      "step_no: 143, epoch: 0, batch_no: 143, loss: 1.0934311151504517, avg_loss: 0.8824999928474426\n",
      "step_no: 144, epoch: 0, batch_no: 144, loss: 0.8289148807525635, avg_loss: 0.8821304440498352\n",
      "step_no: 145, epoch: 0, batch_no: 145, loss: 0.8708674907684326, avg_loss: 0.8820533156394958\n",
      "step_no: 146, epoch: 0, batch_no: 146, loss: 0.6501432061195374, avg_loss: 0.8804756999015808\n",
      "step_no: 147, epoch: 0, batch_no: 147, loss: 0.6805484890937805, avg_loss: 0.8791248202323914\n",
      "step_no: 148, epoch: 0, batch_no: 148, loss: 0.7853259444236755, avg_loss: 0.8784952759742737\n",
      "step_no: 149, epoch: 0, batch_no: 149, loss: 0.7462876439094543, avg_loss: 0.8776139616966248\n",
      "step_no: 150, epoch: 0, batch_no: 150, loss: 0.6496167778968811, avg_loss: 0.8761039972305298\n",
      "step_no: 151, epoch: 0, batch_no: 151, loss: 0.8860056400299072, avg_loss: 0.8761690855026245\n",
      "step_no: 152, epoch: 0, batch_no: 152, loss: 0.6865179538726807, avg_loss: 0.8749296069145203\n",
      "step_no: 153, epoch: 0, batch_no: 153, loss: 0.600765585899353, avg_loss: 0.8731493353843689\n",
      "step_no: 154, epoch: 0, batch_no: 154, loss: 0.7983849048614502, avg_loss: 0.8726669549942017\n",
      "step_no: 155, epoch: 0, batch_no: 155, loss: 0.8341615796089172, avg_loss: 0.8724201917648315\n",
      "step_no: 156, epoch: 0, batch_no: 156, loss: 0.6126339435577393, avg_loss: 0.8707655668258667\n",
      "step_no: 157, epoch: 0, batch_no: 157, loss: 0.28186893463134766, avg_loss: 0.8670384287834167\n",
      "step_no: 158, epoch: 0, batch_no: 158, loss: 0.36920976638793945, avg_loss: 0.8639073967933655\n",
      "step_no: 159, epoch: 0, batch_no: 159, loss: 0.9073351621627808, avg_loss: 0.8641788363456726\n",
      "step_no: 160, epoch: 0, batch_no: 160, loss: 0.8055071830749512, avg_loss: 0.8638144731521606\n",
      "step_no: 161, epoch: 0, batch_no: 161, loss: 0.6718720197677612, avg_loss: 0.8626296520233154\n",
      "step_no: 162, epoch: 0, batch_no: 162, loss: 0.8203635811805725, avg_loss: 0.8623703122138977\n",
      "step_no: 163, epoch: 0, batch_no: 163, loss: 0.8728035092353821, avg_loss: 0.862433910369873\n",
      "step_no: 164, epoch: 0, batch_no: 164, loss: 0.3570549190044403, avg_loss: 0.8593710064888\n",
      "step_no: 165, epoch: 0, batch_no: 165, loss: 1.011172890663147, avg_loss: 0.8602854609489441\n",
      "step_no: 166, epoch: 0, batch_no: 166, loss: 0.7049921751022339, avg_loss: 0.8593555688858032\n",
      "step_no: 167, epoch: 0, batch_no: 167, loss: 0.8820822238922119, avg_loss: 0.8594908118247986\n",
      "step_no: 168, epoch: 0, batch_no: 168, loss: 0.6423483490943909, avg_loss: 0.8582059741020203\n",
      "step_no: 169, epoch: 0, batch_no: 169, loss: 1.582067608833313, avg_loss: 0.8624639511108398\n",
      "step_no: 170, epoch: 0, batch_no: 170, loss: 2.280243158340454, avg_loss: 0.8707550168037415\n",
      "step_no: 171, epoch: 0, batch_no: 171, loss: 3.525458812713623, avg_loss: 0.8861892819404602\n",
      "step_no: 172, epoch: 0, batch_no: 172, loss: 3.2225451469421387, avg_loss: 0.8996942639350891\n",
      "step_no: 173, epoch: 0, batch_no: 173, loss: 0.5601874589920044, avg_loss: 0.8977430462837219\n",
      "step_no: 174, epoch: 0, batch_no: 174, loss: 0.61518794298172, avg_loss: 0.8961284160614014\n",
      "step_no: 175, epoch: 0, batch_no: 175, loss: 0.927300751209259, avg_loss: 0.8963056206703186\n",
      "step_no: 176, epoch: 0, batch_no: 176, loss: 1.0979101657867432, avg_loss: 0.8974446654319763\n",
      "step_no: 177, epoch: 0, batch_no: 177, loss: 1.0577061176300049, avg_loss: 0.8983449935913086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_no: 178, epoch: 0, batch_no: 178, loss: 0.7836066484451294, avg_loss: 0.8977039456367493\n",
      "step_no: 179, epoch: 0, batch_no: 179, loss: 0.7782843708992004, avg_loss: 0.897040605545044\n",
      "step_no: 180, epoch: 0, batch_no: 180, loss: 0.8342980146408081, avg_loss: 0.896694004535675\n",
      "step_no: 181, epoch: 0, batch_no: 181, loss: 0.8028872609138489, avg_loss: 0.8961785435676575\n",
      "step_no: 182, epoch: 0, batch_no: 182, loss: 0.6592616438865662, avg_loss: 0.8948838710784912\n",
      "step_no: 183, epoch: 0, batch_no: 183, loss: 0.8032845854759216, avg_loss: 0.8943860530853271\n",
      "step_no: 184, epoch: 0, batch_no: 184, loss: 0.8286000490188599, avg_loss: 0.8940304517745972\n",
      "step_no: 185, epoch: 0, batch_no: 185, loss: 0.7493975758552551, avg_loss: 0.8932528495788574\n",
      "step_no: 186, epoch: 0, batch_no: 186, loss: 0.8990655541419983, avg_loss: 0.8932839632034302\n",
      "step_no: 187, epoch: 0, batch_no: 187, loss: 0.7727659940719604, avg_loss: 0.8926428556442261\n",
      "step_no: 188, epoch: 0, batch_no: 188, loss: 0.5963786244392395, avg_loss: 0.8910753130912781\n",
      "step_no: 189, epoch: 0, batch_no: 189, loss: 0.8177274465560913, avg_loss: 0.8906893730163574\n",
      "step_no: 190, epoch: 0, batch_no: 190, loss: 0.8523053526878357, avg_loss: 0.8904883861541748\n",
      "step_no: 191, epoch: 0, batch_no: 191, loss: 0.7569702863693237, avg_loss: 0.8897930383682251\n",
      "step_no: 192, epoch: 0, batch_no: 192, loss: 0.5286079049110413, avg_loss: 0.8879215717315674\n",
      "step_no: 193, epoch: 0, batch_no: 193, loss: 0.7547293305397034, avg_loss: 0.8872349858283997\n",
      "step_no: 194, epoch: 0, batch_no: 194, loss: 0.7311843037605286, avg_loss: 0.88643479347229\n",
      "step_no: 195, epoch: 0, batch_no: 195, loss: 0.5444137454032898, avg_loss: 0.8846898078918457\n",
      "step_no: 196, epoch: 0, batch_no: 196, loss: 1.2751578092575073, avg_loss: 0.8866719007492065\n",
      "step_no: 197, epoch: 0, batch_no: 197, loss: 0.9172847867012024, avg_loss: 0.8868265151977539\n",
      "step_no: 198, epoch: 0, batch_no: 198, loss: 0.9151031374931335, avg_loss: 0.8869685530662537\n",
      "step_no: 199, epoch: 0, batch_no: 199, loss: 1.0014201402664185, avg_loss: 0.8875408172607422\n",
      "0.3951995098257817\n",
      "step_no: 200, epoch: 0, batch_no: 200, loss: 0.6254152655601501, avg_loss: 0.8862366676330566\n",
      "step_no: 201, epoch: 0, batch_no: 201, loss: 0.6581611037254333, avg_loss: 0.8851075768470764\n",
      "step_no: 202, epoch: 0, batch_no: 202, loss: 0.6983055472373962, avg_loss: 0.8841873407363892\n",
      "step_no: 203, epoch: 0, batch_no: 203, loss: 0.9391717314720154, avg_loss: 0.884456992149353\n",
      "step_no: 204, epoch: 0, batch_no: 204, loss: 1.033401370048523, avg_loss: 0.8851835131645203\n",
      "step_no: 205, epoch: 0, batch_no: 205, loss: 0.5982799530029297, avg_loss: 0.8837907910346985\n",
      "step_no: 206, epoch: 0, batch_no: 206, loss: 0.4394965171813965, avg_loss: 0.8816444277763367\n",
      "step_no: 207, epoch: 0, batch_no: 207, loss: 0.47296178340911865, avg_loss: 0.8796796202659607\n",
      "step_no: 208, epoch: 0, batch_no: 208, loss: 1.1943578720092773, avg_loss: 0.8811851739883423\n",
      "step_no: 209, epoch: 0, batch_no: 209, loss: 0.8599685430526733, avg_loss: 0.8810842037200928\n",
      "step_no: 210, epoch: 0, batch_no: 210, loss: 0.7715849876403809, avg_loss: 0.8805652856826782\n",
      "step_no: 211, epoch: 0, batch_no: 211, loss: 0.918519139289856, avg_loss: 0.8807442784309387\n",
      "step_no: 212, epoch: 0, batch_no: 212, loss: 0.8920713663101196, avg_loss: 0.8807975053787231\n",
      "step_no: 213, epoch: 0, batch_no: 213, loss: 0.7037056088447571, avg_loss: 0.8799698948860168\n",
      "step_no: 214, epoch: 0, batch_no: 214, loss: 0.5371822714805603, avg_loss: 0.8783755898475647\n",
      "step_no: 215, epoch: 0, batch_no: 215, loss: 0.8147385120391846, avg_loss: 0.8780810236930847\n",
      "step_no: 216, epoch: 0, batch_no: 216, loss: 0.9870575666427612, avg_loss: 0.8785831928253174\n",
      "step_no: 217, epoch: 0, batch_no: 217, loss: 0.663040280342102, avg_loss: 0.8775944709777832\n",
      "step_no: 218, epoch: 0, batch_no: 218, loss: 0.6828551292419434, avg_loss: 0.8767052292823792\n",
      "step_no: 219, epoch: 0, batch_no: 219, loss: 0.3336960971355438, avg_loss: 0.874237060546875\n",
      "step_no: 220, epoch: 0, batch_no: 220, loss: 0.8148133754730225, avg_loss: 0.873968243598938\n",
      "step_no: 221, epoch: 0, batch_no: 221, loss: 0.7292587757110596, avg_loss: 0.8733164072036743\n",
      "step_no: 222, epoch: 0, batch_no: 222, loss: 0.5725342631340027, avg_loss: 0.8719676733016968\n",
      "step_no: 223, epoch: 0, batch_no: 223, loss: 0.992078423500061, avg_loss: 0.8725038766860962\n",
      "step_no: 224, epoch: 0, batch_no: 224, loss: 0.845380425453186, avg_loss: 0.8723832964897156\n",
      "step_no: 225, epoch: 0, batch_no: 225, loss: 0.6114727854728699, avg_loss: 0.8712287545204163\n",
      "step_no: 226, epoch: 0, batch_no: 226, loss: 0.9368430972099304, avg_loss: 0.8715177774429321\n",
      "step_no: 227, epoch: 0, batch_no: 227, loss: 0.8272261619567871, avg_loss: 0.8713235855102539\n",
      "step_no: 228, epoch: 0, batch_no: 228, loss: 0.9400879740715027, avg_loss: 0.8716238737106323\n",
      "step_no: 229, epoch: 0, batch_no: 229, loss: 0.8559032678604126, avg_loss: 0.8715554475784302\n",
      "step_no: 230, epoch: 0, batch_no: 230, loss: 0.7965609431266785, avg_loss: 0.8712308406829834\n",
      "step_no: 231, epoch: 0, batch_no: 231, loss: 0.61893630027771, avg_loss: 0.8701433539390564\n",
      "step_no: 232, epoch: 0, batch_no: 232, loss: 1.0870219469070435, avg_loss: 0.8710741400718689\n",
      "step_no: 233, epoch: 0, batch_no: 233, loss: 0.7982691526412964, avg_loss: 0.8707630634307861\n",
      "step_no: 234, epoch: 0, batch_no: 234, loss: 0.5455285906791687, avg_loss: 0.8693790435791016\n",
      "step_no: 235, epoch: 0, batch_no: 235, loss: 0.577203094959259, avg_loss: 0.8681410551071167\n",
      "step_no: 236, epoch: 0, batch_no: 236, loss: 1.1587446928024292, avg_loss: 0.8693671226501465\n",
      "step_no: 237, epoch: 0, batch_no: 237, loss: 0.4386046826839447, avg_loss: 0.8675572872161865\n",
      "step_no: 238, epoch: 0, batch_no: 238, loss: 0.8257057666778564, avg_loss: 0.8673820495605469\n",
      "step_no: 239, epoch: 0, batch_no: 239, loss: 0.7048690319061279, avg_loss: 0.8667050004005432\n",
      "step_no: 240, epoch: 0, batch_no: 240, loss: 1.016808032989502, avg_loss: 0.867327868938446\n",
      "step_no: 241, epoch: 0, batch_no: 241, loss: 0.6848099231719971, avg_loss: 0.8665735721588135\n",
      "step_no: 242, epoch: 0, batch_no: 242, loss: 0.6435556411743164, avg_loss: 0.8656558394432068\n",
      "step_no: 243, epoch: 1, batch_no: 0, loss: 0.46949395537376404, avg_loss: 0.46949395537376404\n",
      "step_no: 244, epoch: 1, batch_no: 1, loss: 0.7331262826919556, avg_loss: 0.601310133934021\n",
      "step_no: 245, epoch: 1, batch_no: 2, loss: 0.0, avg_loss: 0.40087342262268066\n",
      "step_no: 246, epoch: 1, batch_no: 3, loss: 0.8565018177032471, avg_loss: 0.5147805213928223\n",
      "step_no: 247, epoch: 1, batch_no: 4, loss: 0.6288743615150452, avg_loss: 0.5375992655754089\n",
      "step_no: 248, epoch: 1, batch_no: 5, loss: 0.9030536413192749, avg_loss: 0.598508358001709\n",
      "step_no: 249, epoch: 1, batch_no: 6, loss: 0.8475364446640015, avg_loss: 0.6340838670730591\n",
      "step_no: 250, epoch: 1, batch_no: 7, loss: 0.30816566944122314, avg_loss: 0.5933440327644348\n",
      "step_no: 251, epoch: 1, batch_no: 8, loss: 0.6475119590759277, avg_loss: 0.5993626713752747\n",
      "step_no: 252, epoch: 1, batch_no: 9, loss: 1.4913053512573242, avg_loss: 0.688556969165802\n",
      "step_no: 253, epoch: 1, batch_no: 10, loss: 0.762799859046936, avg_loss: 0.6953063011169434\n",
      "step_no: 254, epoch: 1, batch_no: 11, loss: 0.5493586659431458, avg_loss: 0.6831440329551697\n",
      "step_no: 255, epoch: 1, batch_no: 12, loss: 0.8217887878417969, avg_loss: 0.6938090324401855\n",
      "step_no: 256, epoch: 1, batch_no: 13, loss: 0.6834911108016968, avg_loss: 0.6930720210075378\n",
      "step_no: 257, epoch: 1, batch_no: 14, loss: 0.6305410861968994, avg_loss: 0.688903272151947\n",
      "step_no: 258, epoch: 1, batch_no: 15, loss: 0.7005377411842346, avg_loss: 0.689630389213562\n",
      "step_no: 259, epoch: 1, batch_no: 16, loss: 0.7258240580558777, avg_loss: 0.691759467124939\n",
      "step_no: 260, epoch: 1, batch_no: 17, loss: 0.5678014159202576, avg_loss: 0.6848729252815247\n",
      "step_no: 261, epoch: 1, batch_no: 18, loss: 0.7118176817893982, avg_loss: 0.6862910389900208\n",
      "step_no: 262, epoch: 1, batch_no: 19, loss: 0.6439938545227051, avg_loss: 0.6841761469841003\n",
      "step_no: 263, epoch: 1, batch_no: 20, loss: 0.6327202916145325, avg_loss: 0.6817258596420288\n",
      "step_no: 264, epoch: 1, batch_no: 21, loss: 1.1396300792694092, avg_loss: 0.7025397419929504\n",
      "step_no: 265, epoch: 1, batch_no: 22, loss: 0.7379192113876343, avg_loss: 0.704077959060669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_no: 266, epoch: 1, batch_no: 23, loss: 0.8488627672195435, avg_loss: 0.7101106643676758\n",
      "step_no: 267, epoch: 1, batch_no: 24, loss: 0.7835418581962585, avg_loss: 0.7130479216575623\n",
      "step_no: 268, epoch: 1, batch_no: 25, loss: 1.218063235282898, avg_loss: 0.7324716448783875\n",
      "step_no: 269, epoch: 1, batch_no: 26, loss: 0.830669641494751, avg_loss: 0.7361086010932922\n",
      "step_no: 270, epoch: 1, batch_no: 27, loss: 0.8893259763717651, avg_loss: 0.7415806651115417\n",
      "step_no: 271, epoch: 1, batch_no: 28, loss: 0.4052910804748535, avg_loss: 0.7299844026565552\n",
      "step_no: 272, epoch: 1, batch_no: 29, loss: 0.5410018563270569, avg_loss: 0.7236850261688232\n",
      "step_no: 273, epoch: 1, batch_no: 30, loss: 0.7951177954673767, avg_loss: 0.7259892821311951\n",
      "step_no: 274, epoch: 1, batch_no: 31, loss: 0.659998893737793, avg_loss: 0.7239271402359009\n",
      "step_no: 275, epoch: 1, batch_no: 32, loss: 0.7608404755592346, avg_loss: 0.7250458002090454\n",
      "step_no: 276, epoch: 1, batch_no: 33, loss: 0.41050446033477783, avg_loss: 0.715794563293457\n",
      "step_no: 277, epoch: 1, batch_no: 34, loss: 0.9124502539634705, avg_loss: 0.7214133143424988\n",
      "step_no: 278, epoch: 1, batch_no: 35, loss: 0.5414045453071594, avg_loss: 0.7164130806922913\n",
      "step_no: 279, epoch: 1, batch_no: 36, loss: 0.6676738262176514, avg_loss: 0.7150958180427551\n",
      "step_no: 280, epoch: 1, batch_no: 37, loss: 0.6436324119567871, avg_loss: 0.713215172290802\n",
      "step_no: 281, epoch: 1, batch_no: 38, loss: 0.6409512758255005, avg_loss: 0.7113622426986694\n",
      "step_no: 282, epoch: 1, batch_no: 39, loss: 0.5308888554573059, avg_loss: 0.7068504691123962\n",
      "step_no: 283, epoch: 1, batch_no: 40, loss: 0.49932000041007996, avg_loss: 0.7017886638641357\n",
      "step_no: 284, epoch: 1, batch_no: 41, loss: 0.5603343844413757, avg_loss: 0.6984207630157471\n",
      "step_no: 285, epoch: 1, batch_no: 42, loss: 0.6493328213691711, avg_loss: 0.697279155254364\n",
      "step_no: 286, epoch: 1, batch_no: 43, loss: 0.38367903232574463, avg_loss: 0.6901518702507019\n",
      "step_no: 287, epoch: 1, batch_no: 44, loss: 0.526583731174469, avg_loss: 0.686517059803009\n",
      "step_no: 288, epoch: 1, batch_no: 45, loss: 0.6879897713661194, avg_loss: 0.6865490674972534\n",
      "step_no: 289, epoch: 1, batch_no: 46, loss: 0.7802478075027466, avg_loss: 0.6885426044464111\n",
      "step_no: 290, epoch: 1, batch_no: 47, loss: 0.8913246989250183, avg_loss: 0.6927672624588013\n",
      "step_no: 291, epoch: 1, batch_no: 48, loss: 0.7140014171600342, avg_loss: 0.6932005286216736\n",
      "step_no: 292, epoch: 1, batch_no: 49, loss: 1.2620923519134521, avg_loss: 0.7045783996582031\n",
      "step_no: 293, epoch: 1, batch_no: 50, loss: 0.600465714931488, avg_loss: 0.702536940574646\n",
      "step_no: 294, epoch: 1, batch_no: 51, loss: 0.7495944499969482, avg_loss: 0.7034419178962708\n",
      "step_no: 295, epoch: 1, batch_no: 52, loss: 0.7938750982284546, avg_loss: 0.7051482200622559\n",
      "step_no: 296, epoch: 1, batch_no: 53, loss: 0.764823853969574, avg_loss: 0.7062533497810364\n",
      "step_no: 297, epoch: 1, batch_no: 54, loss: 0.9887250065803528, avg_loss: 0.7113891243934631\n",
      "step_no: 298, epoch: 1, batch_no: 55, loss: 0.5338107347488403, avg_loss: 0.7082181572914124\n",
      "step_no: 299, epoch: 1, batch_no: 56, loss: 0.83219975233078, avg_loss: 0.7103931903839111\n",
      "0.3935721313997427\n",
      "step_no: 300, epoch: 1, batch_no: 57, loss: 0.6506288051605225, avg_loss: 0.7093627452850342\n",
      "step_no: 301, epoch: 1, batch_no: 58, loss: 0.42037051916122437, avg_loss: 0.7044646143913269\n",
      "step_no: 302, epoch: 1, batch_no: 59, loss: 0.6844669580459595, avg_loss: 0.7041313648223877\n",
      "step_no: 303, epoch: 1, batch_no: 60, loss: 0.7190354466438293, avg_loss: 0.7043756246566772\n",
      "step_no: 304, epoch: 1, batch_no: 61, loss: 0.5295401215553284, avg_loss: 0.7015557289123535\n",
      "step_no: 305, epoch: 1, batch_no: 62, loss: 0.9119529128074646, avg_loss: 0.7048954367637634\n",
      "step_no: 306, epoch: 1, batch_no: 63, loss: 0.6966822147369385, avg_loss: 0.7047670483589172\n",
      "step_no: 307, epoch: 1, batch_no: 64, loss: 0.5027110576629639, avg_loss: 0.7016584873199463\n",
      "step_no: 308, epoch: 1, batch_no: 65, loss: 0.9990860819816589, avg_loss: 0.7061650156974792\n",
      "step_no: 309, epoch: 1, batch_no: 66, loss: 0.8897346258163452, avg_loss: 0.7089048027992249\n",
      "step_no: 310, epoch: 1, batch_no: 67, loss: 1.110060453414917, avg_loss: 0.7148042321205139\n",
      "step_no: 311, epoch: 1, batch_no: 68, loss: 0.7858390212059021, avg_loss: 0.7158337235450745\n",
      "step_no: 312, epoch: 1, batch_no: 69, loss: 0.9583122730255127, avg_loss: 0.7192977070808411\n",
      "step_no: 313, epoch: 1, batch_no: 70, loss: 0.7116677761077881, avg_loss: 0.7191901803016663\n",
      "step_no: 314, epoch: 1, batch_no: 71, loss: 0.6743162870407104, avg_loss: 0.7185669541358948\n",
      "step_no: 315, epoch: 1, batch_no: 72, loss: 0.9955382347106934, avg_loss: 0.7223610281944275\n",
      "step_no: 316, epoch: 1, batch_no: 73, loss: 0.6366815567016602, avg_loss: 0.7212032079696655\n",
      "step_no: 317, epoch: 1, batch_no: 74, loss: 0.8851451277732849, avg_loss: 0.7233890891075134\n",
      "step_no: 318, epoch: 1, batch_no: 75, loss: 0.7433674335479736, avg_loss: 0.7236519455909729\n",
      "step_no: 319, epoch: 1, batch_no: 76, loss: 1.136438012123108, avg_loss: 0.7290127873420715\n",
      "step_no: 320, epoch: 1, batch_no: 77, loss: 0.899459958076477, avg_loss: 0.7311980128288269\n",
      "step_no: 321, epoch: 1, batch_no: 78, loss: 0.8044856190681458, avg_loss: 0.7321256995201111\n",
      "step_no: 322, epoch: 1, batch_no: 79, loss: 1.0312343835830688, avg_loss: 0.7358645796775818\n",
      "step_no: 323, epoch: 1, batch_no: 80, loss: 0.8635640144348145, avg_loss: 0.7374410629272461\n",
      "step_no: 324, epoch: 1, batch_no: 81, loss: 0.8015047311782837, avg_loss: 0.7382223010063171\n",
      "step_no: 325, epoch: 1, batch_no: 82, loss: 0.854117214679718, avg_loss: 0.7396186590194702\n",
      "step_no: 326, epoch: 1, batch_no: 83, loss: 0.7520246505737305, avg_loss: 0.7397664189338684\n",
      "step_no: 327, epoch: 1, batch_no: 84, loss: 0.998479962348938, avg_loss: 0.7428101301193237\n",
      "step_no: 328, epoch: 1, batch_no: 85, loss: 0.7425411939620972, avg_loss: 0.7428069710731506\n",
      "step_no: 329, epoch: 1, batch_no: 86, loss: 0.71434086561203, avg_loss: 0.7424798011779785\n",
      "step_no: 330, epoch: 1, batch_no: 87, loss: 0.5556730628013611, avg_loss: 0.740356981754303\n",
      "step_no: 331, epoch: 1, batch_no: 88, loss: 0.482943594455719, avg_loss: 0.7374646663665771\n",
      "step_no: 332, epoch: 1, batch_no: 89, loss: 1.0275189876556396, avg_loss: 0.7406874895095825\n",
      "step_no: 333, epoch: 1, batch_no: 90, loss: 0.9342612028121948, avg_loss: 0.7428146600723267\n",
      "step_no: 334, epoch: 1, batch_no: 91, loss: 0.6120699048042297, avg_loss: 0.7413934469223022\n",
      "step_no: 335, epoch: 1, batch_no: 92, loss: 0.6731503009796143, avg_loss: 0.7406596541404724\n",
      "step_no: 336, epoch: 1, batch_no: 93, loss: 0.5452306270599365, avg_loss: 0.738580584526062\n",
      "step_no: 337, epoch: 1, batch_no: 94, loss: 0.7159740328788757, avg_loss: 0.7383426427841187\n",
      "step_no: 338, epoch: 1, batch_no: 95, loss: 0.6315015554428101, avg_loss: 0.7372297048568726\n",
      "step_no: 339, epoch: 1, batch_no: 96, loss: 1.0420541763305664, avg_loss: 0.7403721809387207\n",
      "step_no: 340, epoch: 1, batch_no: 97, loss: 0.6436624526977539, avg_loss: 0.7393853068351746\n",
      "step_no: 341, epoch: 1, batch_no: 98, loss: 0.5444020628929138, avg_loss: 0.7374157905578613\n",
      "step_no: 342, epoch: 1, batch_no: 99, loss: 0.5825802087783813, avg_loss: 0.735867440700531\n",
      "step_no: 343, epoch: 1, batch_no: 100, loss: 0.5088506937026978, avg_loss: 0.733619749546051\n",
      "step_no: 344, epoch: 1, batch_no: 101, loss: 0.33985838294029236, avg_loss: 0.7297593951225281\n",
      "step_no: 345, epoch: 1, batch_no: 102, loss: 0.5938645601272583, avg_loss: 0.72843998670578\n",
      "step_no: 346, epoch: 1, batch_no: 103, loss: 0.46260324120521545, avg_loss: 0.7258839011192322\n",
      "step_no: 347, epoch: 1, batch_no: 104, loss: 0.8044368624687195, avg_loss: 0.7266319990158081\n",
      "step_no: 348, epoch: 1, batch_no: 105, loss: 0.5818231105804443, avg_loss: 0.7252658605575562\n",
      "step_no: 349, epoch: 1, batch_no: 106, loss: 0.6547998189926147, avg_loss: 0.7246072888374329\n",
      "step_no: 350, epoch: 1, batch_no: 107, loss: 0.36833152174949646, avg_loss: 0.7213084697723389\n",
      "step_no: 351, epoch: 1, batch_no: 108, loss: 0.7301047444343567, avg_loss: 0.72138911485672\n",
      "step_no: 352, epoch: 1, batch_no: 109, loss: 0.7060928344726562, avg_loss: 0.721250057220459\n",
      "step_no: 353, epoch: 1, batch_no: 110, loss: 0.5866585969924927, avg_loss: 0.720037579536438\n",
      "step_no: 354, epoch: 1, batch_no: 111, loss: 0.8156187534332275, avg_loss: 0.7208910584449768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_no: 355, epoch: 1, batch_no: 112, loss: 0.675605833530426, avg_loss: 0.7204902172088623\n",
      "step_no: 356, epoch: 1, batch_no: 113, loss: 0.8341785073280334, avg_loss: 0.7214874625205994\n",
      "step_no: 357, epoch: 1, batch_no: 114, loss: 0.891735315322876, avg_loss: 0.7229679226875305\n",
      "step_no: 358, epoch: 1, batch_no: 115, loss: 0.9173758625984192, avg_loss: 0.7246438264846802\n",
      "step_no: 359, epoch: 1, batch_no: 116, loss: 0.510428786277771, avg_loss: 0.7228130102157593\n",
      "step_no: 360, epoch: 1, batch_no: 117, loss: 0.8179741501808167, avg_loss: 0.7236194014549255\n",
      "step_no: 361, epoch: 1, batch_no: 118, loss: 0.9140125513076782, avg_loss: 0.7252193689346313\n",
      "step_no: 362, epoch: 1, batch_no: 119, loss: 0.6838849186897278, avg_loss: 0.7248749136924744\n",
      "step_no: 363, epoch: 1, batch_no: 120, loss: 0.6136227250099182, avg_loss: 0.7239554524421692\n",
      "step_no: 364, epoch: 1, batch_no: 121, loss: 0.918286919593811, avg_loss: 0.7255483269691467\n",
      "step_no: 365, epoch: 1, batch_no: 122, loss: 0.8455997109413147, avg_loss: 0.7265243530273438\n",
      "step_no: 366, epoch: 1, batch_no: 123, loss: 0.7210835218429565, avg_loss: 0.7264804840087891\n",
      "step_no: 367, epoch: 1, batch_no: 124, loss: 1.1868369579315186, avg_loss: 0.7301633358001709\n",
      "step_no: 368, epoch: 1, batch_no: 125, loss: 0.7519816160202026, avg_loss: 0.7303365468978882\n",
      "step_no: 369, epoch: 1, batch_no: 126, loss: 0.5178666710853577, avg_loss: 0.7286635041236877\n",
      "step_no: 370, epoch: 1, batch_no: 127, loss: 0.6809729337692261, avg_loss: 0.7282909154891968\n",
      "step_no: 371, epoch: 1, batch_no: 128, loss: 0.7430354952812195, avg_loss: 0.7284051775932312\n",
      "step_no: 372, epoch: 1, batch_no: 129, loss: 1.02137291431427, avg_loss: 0.730658769607544\n",
      "step_no: 373, epoch: 1, batch_no: 130, loss: 0.556357204914093, avg_loss: 0.7293282747268677\n",
      "step_no: 374, epoch: 1, batch_no: 131, loss: 0.7613055109977722, avg_loss: 0.7295705080032349\n",
      "step_no: 375, epoch: 1, batch_no: 132, loss: 0.6934221982955933, avg_loss: 0.7292987108230591\n",
      "step_no: 376, epoch: 1, batch_no: 133, loss: 0.5425560474395752, avg_loss: 0.7279050946235657\n",
      "step_no: 377, epoch: 1, batch_no: 134, loss: 1.04613196849823, avg_loss: 0.7302623391151428\n",
      "step_no: 378, epoch: 1, batch_no: 135, loss: 0.7854429483413696, avg_loss: 0.7306681275367737\n",
      "step_no: 379, epoch: 1, batch_no: 136, loss: 1.023476243019104, avg_loss: 0.7328053712844849\n",
      "step_no: 380, epoch: 1, batch_no: 137, loss: 0.6869525909423828, avg_loss: 0.7324731349945068\n",
      "step_no: 381, epoch: 1, batch_no: 138, loss: 0.8009631633758545, avg_loss: 0.732965886592865\n",
      "step_no: 382, epoch: 1, batch_no: 139, loss: 0.9793351292610168, avg_loss: 0.7347256541252136\n",
      "step_no: 383, epoch: 1, batch_no: 140, loss: 0.7662558555603027, avg_loss: 0.7349492311477661\n",
      "step_no: 384, epoch: 1, batch_no: 141, loss: 1.0825425386428833, avg_loss: 0.7373970746994019\n",
      "step_no: 385, epoch: 1, batch_no: 142, loss: 0.7767564654350281, avg_loss: 0.7376723289489746\n",
      "step_no: 386, epoch: 1, batch_no: 143, loss: 0.26700881123542786, avg_loss: 0.7344038486480713\n",
      "step_no: 387, epoch: 1, batch_no: 144, loss: 0.719283401966095, avg_loss: 0.7342995405197144\n",
      "step_no: 388, epoch: 1, batch_no: 145, loss: 0.3212830722332001, avg_loss: 0.7314706444740295\n",
      "step_no: 389, epoch: 1, batch_no: 146, loss: 0.7317095398902893, avg_loss: 0.7314723134040833\n",
      "step_no: 390, epoch: 1, batch_no: 147, loss: 0.4985727369785309, avg_loss: 0.7298986911773682\n",
      "step_no: 391, epoch: 1, batch_no: 148, loss: 0.980550229549408, avg_loss: 0.731580913066864\n",
      "step_no: 392, epoch: 1, batch_no: 149, loss: 0.9344863891601562, avg_loss: 0.7329336404800415\n",
      "step_no: 393, epoch: 1, batch_no: 150, loss: 1.0421028137207031, avg_loss: 0.734981119632721\n",
      "step_no: 394, epoch: 1, batch_no: 151, loss: 0.6982209086418152, avg_loss: 0.7347392439842224\n",
      "step_no: 395, epoch: 1, batch_no: 152, loss: 0.47774654626846313, avg_loss: 0.7330595850944519\n",
      "step_no: 396, epoch: 1, batch_no: 153, loss: 0.7376676797866821, avg_loss: 0.7330895066261292\n",
      "step_no: 397, epoch: 1, batch_no: 154, loss: 0.9583746194839478, avg_loss: 0.7345429062843323\n",
      "step_no: 398, epoch: 1, batch_no: 155, loss: 0.5034133195877075, avg_loss: 0.7330613136291504\n",
      "step_no: 399, epoch: 1, batch_no: 156, loss: 0.6925209164619446, avg_loss: 0.7328031063079834\n",
      "0.3946685109578103\n",
      "step_no: 400, epoch: 1, batch_no: 157, loss: 0.675241231918335, avg_loss: 0.7324388027191162\n",
      "step_no: 401, epoch: 1, batch_no: 158, loss: 0.7312302589416504, avg_loss: 0.732431173324585\n",
      "step_no: 402, epoch: 1, batch_no: 159, loss: 0.9352421760559082, avg_loss: 0.7336987853050232\n",
      "step_no: 403, epoch: 1, batch_no: 160, loss: 0.7873227596282959, avg_loss: 0.7340318560600281\n",
      "step_no: 404, epoch: 1, batch_no: 161, loss: 0.8879283666610718, avg_loss: 0.7349818348884583\n",
      "step_no: 405, epoch: 1, batch_no: 162, loss: 0.7278735637664795, avg_loss: 0.7349382042884827\n",
      "step_no: 406, epoch: 1, batch_no: 163, loss: 1.1913936138153076, avg_loss: 0.7377214431762695\n",
      "step_no: 407, epoch: 1, batch_no: 164, loss: 0.5975579023361206, avg_loss: 0.7368719577789307\n",
      "step_no: 408, epoch: 1, batch_no: 165, loss: 0.5113564133644104, avg_loss: 0.73551344871521\n",
      "step_no: 409, epoch: 1, batch_no: 166, loss: 0.7389714121818542, avg_loss: 0.7355341911315918\n",
      "step_no: 410, epoch: 1, batch_no: 167, loss: 0.575609564781189, avg_loss: 0.734582245349884\n",
      "step_no: 411, epoch: 1, batch_no: 168, loss: 0.5251446962356567, avg_loss: 0.7333429455757141\n",
      "step_no: 412, epoch: 1, batch_no: 169, loss: 1.0310521125793457, avg_loss: 0.7350941896438599\n",
      "step_no: 413, epoch: 1, batch_no: 170, loss: 1.060562252998352, avg_loss: 0.7369974851608276\n",
      "step_no: 414, epoch: 1, batch_no: 171, loss: 0.5032480955123901, avg_loss: 0.7356384992599487\n",
      "step_no: 415, epoch: 1, batch_no: 172, loss: 0.5112932920455933, avg_loss: 0.7343416810035706\n",
      "step_no: 416, epoch: 1, batch_no: 173, loss: 0.6784920692443848, avg_loss: 0.7340207099914551\n",
      "step_no: 417, epoch: 1, batch_no: 174, loss: 0.6324573159217834, avg_loss: 0.7334403991699219\n",
      "step_no: 418, epoch: 1, batch_no: 175, loss: 0.8865137100219727, avg_loss: 0.7343101501464844\n",
      "step_no: 419, epoch: 1, batch_no: 176, loss: 0.9396929740905762, avg_loss: 0.7354705333709717\n",
      "step_no: 420, epoch: 1, batch_no: 177, loss: 0.6441421508789062, avg_loss: 0.7349573969841003\n",
      "step_no: 421, epoch: 1, batch_no: 178, loss: 0.9290825128555298, avg_loss: 0.7360418438911438\n",
      "step_no: 422, epoch: 1, batch_no: 179, loss: 0.5614330768585205, avg_loss: 0.7350718379020691\n",
      "step_no: 423, epoch: 1, batch_no: 180, loss: 0.8191555142402649, avg_loss: 0.7355363965034485\n",
      "step_no: 424, epoch: 1, batch_no: 181, loss: 0.8942848443984985, avg_loss: 0.7364086508750916\n",
      "step_no: 425, epoch: 1, batch_no: 182, loss: 0.4520793557167053, avg_loss: 0.7348548173904419\n",
      "step_no: 426, epoch: 1, batch_no: 183, loss: 0.25288644433021545, avg_loss: 0.7322354912757874\n",
      "step_no: 427, epoch: 1, batch_no: 184, loss: 0.6691910624504089, avg_loss: 0.7318946719169617\n",
      "step_no: 428, epoch: 1, batch_no: 185, loss: 0.9014580249786377, avg_loss: 0.7328063249588013\n",
      "step_no: 429, epoch: 1, batch_no: 186, loss: 0.881683349609375, avg_loss: 0.7336024641990662\n",
      "step_no: 430, epoch: 1, batch_no: 187, loss: 0.9390767812728882, avg_loss: 0.734695315361023\n",
      "step_no: 431, epoch: 1, batch_no: 188, loss: 0.8404003977775574, avg_loss: 0.7352545857429504\n",
      "step_no: 432, epoch: 1, batch_no: 189, loss: 0.9941257238388062, avg_loss: 0.7366170883178711\n",
      "step_no: 433, epoch: 1, batch_no: 190, loss: 0.5030962824821472, avg_loss: 0.7353944778442383\n",
      "step_no: 434, epoch: 1, batch_no: 191, loss: 1.0573605298995972, avg_loss: 0.7370713949203491\n",
      "step_no: 435, epoch: 1, batch_no: 192, loss: 0.4950254261493683, avg_loss: 0.7358171939849854\n",
      "step_no: 436, epoch: 1, batch_no: 193, loss: 0.5558755397796631, avg_loss: 0.7348896861076355\n",
      "step_no: 437, epoch: 1, batch_no: 194, loss: 0.8366273641586304, avg_loss: 0.7354114651679993\n",
      "step_no: 438, epoch: 1, batch_no: 195, loss: 0.966992199420929, avg_loss: 0.7365929484367371\n",
      "step_no: 439, epoch: 1, batch_no: 196, loss: 0.8017370700836182, avg_loss: 0.7369236946105957\n",
      "step_no: 440, epoch: 1, batch_no: 197, loss: 0.6997362375259399, avg_loss: 0.7367358803749084\n",
      "step_no: 441, epoch: 1, batch_no: 198, loss: 0.8554831147193909, avg_loss: 0.7373325824737549\n",
      "step_no: 442, epoch: 1, batch_no: 199, loss: 1.106048345565796, avg_loss: 0.7391761541366577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_no: 443, epoch: 1, batch_no: 200, loss: 0.9538229703903198, avg_loss: 0.7402440905570984\n",
      "step_no: 444, epoch: 1, batch_no: 201, loss: 0.7576296329498291, avg_loss: 0.740330159664154\n",
      "step_no: 445, epoch: 1, batch_no: 202, loss: 0.6623415350914001, avg_loss: 0.7399459481239319\n",
      "step_no: 446, epoch: 1, batch_no: 203, loss: 0.5099999308586121, avg_loss: 0.7388187646865845\n",
      "step_no: 447, epoch: 1, batch_no: 204, loss: 0.6903181672096252, avg_loss: 0.738582193851471\n",
      "step_no: 448, epoch: 1, batch_no: 205, loss: 0.5856930017471313, avg_loss: 0.7378399968147278\n",
      "step_no: 449, epoch: 1, batch_no: 206, loss: 0.7413152456283569, avg_loss: 0.7378568053245544\n",
      "step_no: 450, epoch: 1, batch_no: 207, loss: 0.6501511931419373, avg_loss: 0.7374351620674133\n",
      "step_no: 451, epoch: 1, batch_no: 208, loss: 0.5072299242019653, avg_loss: 0.7363336682319641\n",
      "step_no: 452, epoch: 1, batch_no: 209, loss: 0.6761978268623352, avg_loss: 0.7360473275184631\n",
      "step_no: 453, epoch: 1, batch_no: 210, loss: 0.7950416803359985, avg_loss: 0.7363269329071045\n",
      "step_no: 454, epoch: 1, batch_no: 211, loss: 0.9713197946548462, avg_loss: 0.7374353408813477\n",
      "step_no: 455, epoch: 1, batch_no: 212, loss: 0.7787400484085083, avg_loss: 0.7376292943954468\n",
      "step_no: 456, epoch: 1, batch_no: 213, loss: 0.8685726523399353, avg_loss: 0.7382411360740662\n",
      "step_no: 457, epoch: 1, batch_no: 214, loss: 0.6537714600563049, avg_loss: 0.7378483414649963\n",
      "step_no: 458, epoch: 1, batch_no: 215, loss: 0.5095999836921692, avg_loss: 0.7367916107177734\n",
      "step_no: 459, epoch: 1, batch_no: 216, loss: 0.7855749726295471, avg_loss: 0.7370163798332214\n",
      "step_no: 460, epoch: 1, batch_no: 217, loss: 0.9990047216415405, avg_loss: 0.7382181286811829\n",
      "step_no: 461, epoch: 1, batch_no: 218, loss: 0.6701356172561646, avg_loss: 0.7379072904586792\n",
      "step_no: 462, epoch: 1, batch_no: 219, loss: 0.5622900724411011, avg_loss: 0.7371090054512024\n",
      "step_no: 463, epoch: 1, batch_no: 220, loss: 0.8837921023368835, avg_loss: 0.7377727627754211\n",
      "step_no: 464, epoch: 1, batch_no: 221, loss: 0.8986978530883789, avg_loss: 0.7384976148605347\n",
      "step_no: 465, epoch: 1, batch_no: 222, loss: 0.6937456130981445, avg_loss: 0.7382969856262207\n",
      "step_no: 466, epoch: 1, batch_no: 223, loss: 0.6141861081123352, avg_loss: 0.7377428412437439\n",
      "step_no: 467, epoch: 1, batch_no: 224, loss: 0.7593095302581787, avg_loss: 0.7378386855125427\n",
      "step_no: 468, epoch: 1, batch_no: 225, loss: 0.9726389646530151, avg_loss: 0.7388775944709778\n",
      "step_no: 469, epoch: 1, batch_no: 226, loss: 0.6347271800041199, avg_loss: 0.7384187579154968\n",
      "step_no: 470, epoch: 1, batch_no: 227, loss: 0.4467385411262512, avg_loss: 0.7371394634246826\n",
      "step_no: 471, epoch: 1, batch_no: 228, loss: 1.1150522232055664, avg_loss: 0.7387897372245789\n",
      "step_no: 472, epoch: 1, batch_no: 229, loss: 0.5824580192565918, avg_loss: 0.7381100058555603\n",
      "step_no: 473, epoch: 1, batch_no: 230, loss: 0.6278689503669739, avg_loss: 0.7376328110694885\n",
      "step_no: 474, epoch: 1, batch_no: 231, loss: 0.3681798279285431, avg_loss: 0.7360402941703796\n",
      "step_no: 475, epoch: 1, batch_no: 232, loss: 0.6344331502914429, avg_loss: 0.7356042265892029\n",
      "step_no: 476, epoch: 1, batch_no: 233, loss: 0.6909242868423462, avg_loss: 0.7354133129119873\n",
      "step_no: 477, epoch: 1, batch_no: 234, loss: 0.7541508078575134, avg_loss: 0.7354929447174072\n",
      "step_no: 478, epoch: 1, batch_no: 235, loss: 0.7215559482574463, avg_loss: 0.7354339361190796\n",
      "step_no: 479, epoch: 1, batch_no: 236, loss: 0.5615296363830566, avg_loss: 0.734700083732605\n",
      "step_no: 480, epoch: 1, batch_no: 237, loss: 0.6730524301528931, avg_loss: 0.7344411611557007\n",
      "step_no: 481, epoch: 1, batch_no: 238, loss: 1.0948340892791748, avg_loss: 0.7359489798545837\n",
      "step_no: 482, epoch: 1, batch_no: 239, loss: 0.7129937410354614, avg_loss: 0.7358534336090088\n",
      "step_no: 483, epoch: 1, batch_no: 240, loss: 0.6198152899742126, avg_loss: 0.7353719472885132\n",
      "step_no: 484, epoch: 1, batch_no: 241, loss: 0.8584288358688354, avg_loss: 0.7358803749084473\n",
      "step_no: 485, epoch: 1, batch_no: 242, loss: 0.6160382032394409, avg_loss: 0.7353872060775757\n",
      "step_no: 486, epoch: 2, batch_no: 0, loss: 0.5753128528594971, avg_loss: 0.5753128528594971\n",
      "step_no: 487, epoch: 2, batch_no: 1, loss: 0.5918818116188049, avg_loss: 0.5835973024368286\n",
      "step_no: 488, epoch: 2, batch_no: 2, loss: 0.7596175670623779, avg_loss: 0.6422707438468933\n",
      "step_no: 489, epoch: 2, batch_no: 3, loss: 0.5877168774604797, avg_loss: 0.6286322474479675\n",
      "step_no: 490, epoch: 2, batch_no: 4, loss: 0.8278247117996216, avg_loss: 0.6684707999229431\n",
      "step_no: 491, epoch: 2, batch_no: 5, loss: 0.6960670351982117, avg_loss: 0.6730701327323914\n",
      "step_no: 492, epoch: 2, batch_no: 6, loss: 1.2663968801498413, avg_loss: 0.7578311562538147\n",
      "step_no: 493, epoch: 2, batch_no: 7, loss: 0.3710790276527405, avg_loss: 0.7094870805740356\n",
      "step_no: 494, epoch: 2, batch_no: 8, loss: 0.7631890177726746, avg_loss: 0.7154539227485657\n",
      "step_no: 495, epoch: 2, batch_no: 9, loss: 0.7245718836784363, avg_loss: 0.7163657546043396\n",
      "step_no: 496, epoch: 2, batch_no: 10, loss: 0.730162501335144, avg_loss: 0.7176200151443481\n",
      "step_no: 497, epoch: 2, batch_no: 11, loss: 0.5260246396064758, avg_loss: 0.7016537189483643\n",
      "step_no: 498, epoch: 2, batch_no: 12, loss: 0.5946944355964661, avg_loss: 0.6934260725975037\n",
      "step_no: 499, epoch: 2, batch_no: 13, loss: 0.6537323594093323, avg_loss: 0.6905907988548279\n",
      "0.39732841761443943\n",
      "step_no: 500, epoch: 2, batch_no: 14, loss: 0.37241029739379883, avg_loss: 0.6693788170814514\n",
      "step_no: 501, epoch: 2, batch_no: 15, loss: 0.7314653396606445, avg_loss: 0.6732591986656189\n",
      "step_no: 502, epoch: 2, batch_no: 16, loss: 0.5868529081344604, avg_loss: 0.6681764721870422\n",
      "step_no: 503, epoch: 2, batch_no: 17, loss: 0.5535628795623779, avg_loss: 0.6618090867996216\n",
      "step_no: 504, epoch: 2, batch_no: 18, loss: 1.216369867324829, avg_loss: 0.6909964680671692\n",
      "step_no: 505, epoch: 2, batch_no: 19, loss: 0.7231401205062866, avg_loss: 0.6926036477088928\n",
      "step_no: 506, epoch: 2, batch_no: 20, loss: 0.9807993769645691, avg_loss: 0.7063272595405579\n",
      "step_no: 507, epoch: 2, batch_no: 21, loss: 0.7636643648147583, avg_loss: 0.7089335322380066\n",
      "step_no: 508, epoch: 2, batch_no: 22, loss: 0.632631778717041, avg_loss: 0.7056159973144531\n",
      "step_no: 509, epoch: 2, batch_no: 23, loss: 0.5266024470329285, avg_loss: 0.6981570720672607\n",
      "step_no: 510, epoch: 2, batch_no: 24, loss: 0.9541240334510803, avg_loss: 0.708395779132843\n",
      "step_no: 511, epoch: 2, batch_no: 25, loss: 0.730802595615387, avg_loss: 0.7092576026916504\n",
      "step_no: 512, epoch: 2, batch_no: 26, loss: 0.9431174397468567, avg_loss: 0.7179190516471863\n",
      "step_no: 513, epoch: 2, batch_no: 27, loss: 0.3471614122390747, avg_loss: 0.7046777606010437\n",
      "step_no: 514, epoch: 2, batch_no: 28, loss: 0.5785344243049622, avg_loss: 0.70032799243927\n",
      "step_no: 515, epoch: 2, batch_no: 29, loss: 0.46779322624206543, avg_loss: 0.6925768256187439\n",
      "step_no: 516, epoch: 2, batch_no: 30, loss: 0.4984259605407715, avg_loss: 0.6863138675689697\n",
      "step_no: 517, epoch: 2, batch_no: 31, loss: 0.8809098601341248, avg_loss: 0.6923949718475342\n",
      "step_no: 518, epoch: 2, batch_no: 32, loss: 0.7649884819984436, avg_loss: 0.6945948004722595\n",
      "step_no: 519, epoch: 2, batch_no: 33, loss: 0.7134914398193359, avg_loss: 0.6951505541801453\n",
      "step_no: 520, epoch: 2, batch_no: 34, loss: 0.5864685773849487, avg_loss: 0.6920453310012817\n",
      "step_no: 521, epoch: 2, batch_no: 35, loss: 0.5324453711509705, avg_loss: 0.687611997127533\n",
      "step_no: 522, epoch: 2, batch_no: 36, loss: 0.5211434960365295, avg_loss: 0.6831128597259521\n",
      "step_no: 523, epoch: 2, batch_no: 37, loss: 0.6149097681045532, avg_loss: 0.6813180446624756\n",
      "step_no: 524, epoch: 2, batch_no: 38, loss: 0.7867464423179626, avg_loss: 0.6840213537216187\n",
      "step_no: 525, epoch: 2, batch_no: 39, loss: 0.47363194823265076, avg_loss: 0.6787616610527039\n",
      "step_no: 526, epoch: 2, batch_no: 40, loss: 0.6051408648490906, avg_loss: 0.6769659519195557\n",
      "step_no: 527, epoch: 2, batch_no: 41, loss: 0.5379757881164551, avg_loss: 0.673656702041626\n",
      "step_no: 528, epoch: 2, batch_no: 42, loss: 0.5420323610305786, avg_loss: 0.670595645904541\n",
      "step_no: 529, epoch: 2, batch_no: 43, loss: 0.8661553859710693, avg_loss: 0.6750402450561523\n",
      "step_no: 530, epoch: 2, batch_no: 44, loss: 0.5895981192588806, avg_loss: 0.6731414794921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_no: 531, epoch: 2, batch_no: 45, loss: 1.0367101430892944, avg_loss: 0.6810451745986938\n",
      "step_no: 532, epoch: 2, batch_no: 46, loss: 0.7688047885894775, avg_loss: 0.6829123497009277\n",
      "step_no: 533, epoch: 2, batch_no: 47, loss: 0.7732954025268555, avg_loss: 0.6847953796386719\n",
      "step_no: 534, epoch: 2, batch_no: 48, loss: 0.8130615949630737, avg_loss: 0.6874130368232727\n",
      "step_no: 535, epoch: 2, batch_no: 49, loss: 0.6091747283935547, avg_loss: 0.6858482360839844\n",
      "step_no: 536, epoch: 2, batch_no: 50, loss: 0.8783462047576904, avg_loss: 0.689622700214386\n",
      "step_no: 537, epoch: 2, batch_no: 51, loss: 0.5226906538009644, avg_loss: 0.6864124536514282\n",
      "step_no: 538, epoch: 2, batch_no: 52, loss: 0.692175567150116, avg_loss: 0.6865212321281433\n",
      "step_no: 539, epoch: 2, batch_no: 53, loss: 0.7111523151397705, avg_loss: 0.6869773268699646\n",
      "step_no: 540, epoch: 2, batch_no: 54, loss: 0.48283851146698, avg_loss: 0.6832656860351562\n",
      "step_no: 541, epoch: 2, batch_no: 55, loss: 0.5364618301391602, avg_loss: 0.6806442141532898\n",
      "step_no: 542, epoch: 2, batch_no: 56, loss: 0.9408471584320068, avg_loss: 0.6852090954780579\n",
      "step_no: 543, epoch: 2, batch_no: 57, loss: 0.6705755591392517, avg_loss: 0.6849567890167236\n",
      "step_no: 544, epoch: 2, batch_no: 58, loss: 0.6676042079925537, avg_loss: 0.6846626400947571\n",
      "step_no: 545, epoch: 2, batch_no: 59, loss: 0.6262124180793762, avg_loss: 0.6836885213851929\n",
      "step_no: 546, epoch: 2, batch_no: 60, loss: 0.421950101852417, avg_loss: 0.6793976426124573\n",
      "step_no: 547, epoch: 2, batch_no: 61, loss: 0.5522684454917908, avg_loss: 0.6773472428321838\n",
      "step_no: 548, epoch: 2, batch_no: 62, loss: 0.4235021471977234, avg_loss: 0.6733180284500122\n",
      "step_no: 549, epoch: 2, batch_no: 63, loss: 0.6973613500595093, avg_loss: 0.6736936569213867\n",
      "step_no: 550, epoch: 2, batch_no: 64, loss: 0.7277927994728088, avg_loss: 0.6745259761810303\n",
      "step_no: 551, epoch: 2, batch_no: 65, loss: 0.9458365440368652, avg_loss: 0.6786367297172546\n",
      "step_no: 552, epoch: 2, batch_no: 66, loss: 0.5864871144294739, avg_loss: 0.6772613525390625\n",
      "step_no: 553, epoch: 2, batch_no: 67, loss: 0.623435378074646, avg_loss: 0.6764698028564453\n",
      "step_no: 554, epoch: 2, batch_no: 68, loss: 1.261009931564331, avg_loss: 0.6849414110183716\n",
      "step_no: 555, epoch: 2, batch_no: 69, loss: 0.7646435499191284, avg_loss: 0.6860800385475159\n",
      "step_no: 556, epoch: 2, batch_no: 70, loss: 0.42486053705215454, avg_loss: 0.6824008822441101\n",
      "step_no: 557, epoch: 2, batch_no: 71, loss: 0.22029006481170654, avg_loss: 0.6759827136993408\n",
      "step_no: 558, epoch: 2, batch_no: 72, loss: 0.5434566736221313, avg_loss: 0.674167275428772\n",
      "step_no: 559, epoch: 2, batch_no: 73, loss: 0.8879328966140747, avg_loss: 0.6770560145378113\n",
      "step_no: 560, epoch: 2, batch_no: 74, loss: 0.7732462882995605, avg_loss: 0.6783385276794434\n",
      "step_no: 561, epoch: 2, batch_no: 75, loss: 0.8671259880065918, avg_loss: 0.6808225512504578\n",
      "step_no: 562, epoch: 2, batch_no: 76, loss: 0.6435067653656006, avg_loss: 0.6803379058837891\n",
      "step_no: 563, epoch: 2, batch_no: 77, loss: 0.6399408578872681, avg_loss: 0.6798200607299805\n",
      "step_no: 564, epoch: 2, batch_no: 78, loss: 0.849019467830658, avg_loss: 0.6819617748260498\n",
      "step_no: 565, epoch: 2, batch_no: 79, loss: 1.1267120838165283, avg_loss: 0.6875211596488953\n",
      "step_no: 566, epoch: 2, batch_no: 80, loss: 0.8707526326179504, avg_loss: 0.6897833347320557\n",
      "step_no: 567, epoch: 2, batch_no: 81, loss: 0.6031230688095093, avg_loss: 0.6887264251708984\n",
      "step_no: 568, epoch: 2, batch_no: 82, loss: 0.7682255506515503, avg_loss: 0.689684271812439\n",
      "step_no: 569, epoch: 2, batch_no: 83, loss: 0.44337528944015503, avg_loss: 0.6867520809173584\n",
      "step_no: 570, epoch: 2, batch_no: 84, loss: 0.5581929683685303, avg_loss: 0.6852396130561829\n",
      "step_no: 571, epoch: 2, batch_no: 85, loss: 0.8790031671524048, avg_loss: 0.6874926090240479\n",
      "step_no: 572, epoch: 2, batch_no: 86, loss: 0.6057405471801758, avg_loss: 0.6865529417991638\n",
      "step_no: 573, epoch: 2, batch_no: 87, loss: 1.0460790395736694, avg_loss: 0.6906384825706482\n",
      "step_no: 574, epoch: 2, batch_no: 88, loss: 0.8370143175125122, avg_loss: 0.6922830939292908\n",
      "step_no: 575, epoch: 2, batch_no: 89, loss: 1.0109498500823975, avg_loss: 0.6958238482475281\n",
      "step_no: 576, epoch: 2, batch_no: 90, loss: 0.7729465961456299, avg_loss: 0.6966713666915894\n",
      "step_no: 577, epoch: 2, batch_no: 91, loss: 0.960695207118988, avg_loss: 0.6995412111282349\n",
      "step_no: 578, epoch: 2, batch_no: 92, loss: 0.8082767724990845, avg_loss: 0.7007103562355042\n",
      "step_no: 579, epoch: 2, batch_no: 93, loss: 0.6841333508491516, avg_loss: 0.7005339860916138\n",
      "step_no: 580, epoch: 2, batch_no: 94, loss: 0.37682220339775085, avg_loss: 0.697126567363739\n",
      "step_no: 581, epoch: 2, batch_no: 95, loss: 0.6208383440971375, avg_loss: 0.6963319182395935\n",
      "step_no: 582, epoch: 2, batch_no: 96, loss: 0.6867677569389343, avg_loss: 0.6962332725524902\n",
      "step_no: 583, epoch: 2, batch_no: 97, loss: 0.7096954584121704, avg_loss: 0.6963706612586975\n",
      "step_no: 584, epoch: 2, batch_no: 98, loss: 0.4403447210788727, avg_loss: 0.6937845349311829\n",
      "step_no: 585, epoch: 2, batch_no: 99, loss: 0.747424840927124, avg_loss: 0.6943209171295166\n",
      "step_no: 586, epoch: 2, batch_no: 100, loss: 0.7994810342788696, avg_loss: 0.6953620910644531\n",
      "step_no: 587, epoch: 2, batch_no: 101, loss: 0.6485903859138489, avg_loss: 0.6949036121368408\n",
      "step_no: 588, epoch: 2, batch_no: 102, loss: 0.8545650243759155, avg_loss: 0.6964537501335144\n",
      "step_no: 589, epoch: 2, batch_no: 103, loss: 1.1725523471832275, avg_loss: 0.7010316252708435\n",
      "step_no: 590, epoch: 2, batch_no: 104, loss: 0.9352444410324097, avg_loss: 0.703262209892273\n",
      "step_no: 591, epoch: 2, batch_no: 105, loss: 0.6984872817993164, avg_loss: 0.7032171487808228\n",
      "step_no: 592, epoch: 2, batch_no: 106, loss: 0.6608963012695312, avg_loss: 0.7028216123580933\n",
      "step_no: 593, epoch: 2, batch_no: 107, loss: 0.43156370520591736, avg_loss: 0.7003099918365479\n",
      "step_no: 594, epoch: 2, batch_no: 108, loss: 0.9202526807785034, avg_loss: 0.7023277282714844\n",
      "step_no: 595, epoch: 2, batch_no: 109, loss: 0.9173080325126648, avg_loss: 0.7042821049690247\n",
      "step_no: 596, epoch: 2, batch_no: 110, loss: 0.8550763726234436, avg_loss: 0.7056406736373901\n",
      "step_no: 597, epoch: 2, batch_no: 111, loss: 0.26829248666763306, avg_loss: 0.70173579454422\n",
      "step_no: 598, epoch: 2, batch_no: 112, loss: 0.5139994621276855, avg_loss: 0.7000743746757507\n",
      "step_no: 599, epoch: 2, batch_no: 113, loss: 0.7232268452644348, avg_loss: 0.7002775073051453\n",
      "0.41039040063326854\n",
      "step_no: 600, epoch: 2, batch_no: 114, loss: 0.778136134147644, avg_loss: 0.7009544968605042\n",
      "step_no: 601, epoch: 2, batch_no: 115, loss: 0.586835503578186, avg_loss: 0.6999707818031311\n",
      "step_no: 602, epoch: 2, batch_no: 116, loss: 0.6536428928375244, avg_loss: 0.6995748281478882\n",
      "step_no: 603, epoch: 2, batch_no: 117, loss: 0.808151125907898, avg_loss: 0.7004949450492859\n",
      "step_no: 604, epoch: 2, batch_no: 118, loss: 0.62017422914505, avg_loss: 0.6998199820518494\n",
      "step_no: 605, epoch: 2, batch_no: 119, loss: 0.9497816562652588, avg_loss: 0.7019029855728149\n",
      "step_no: 606, epoch: 2, batch_no: 120, loss: 0.642821192741394, avg_loss: 0.7014146447181702\n",
      "step_no: 607, epoch: 2, batch_no: 121, loss: 0.9916138648986816, avg_loss: 0.7037933468818665\n",
      "step_no: 608, epoch: 2, batch_no: 122, loss: 0.5629190802574158, avg_loss: 0.7026480436325073\n",
      "step_no: 609, epoch: 2, batch_no: 123, loss: 0.6873670220375061, avg_loss: 0.7025248408317566\n",
      "step_no: 610, epoch: 2, batch_no: 124, loss: 0.7227736115455627, avg_loss: 0.7026868462562561\n",
      "step_no: 611, epoch: 2, batch_no: 125, loss: 0.8915924429893494, avg_loss: 0.7041861414909363\n",
      "step_no: 612, epoch: 2, batch_no: 126, loss: 0.8669562935829163, avg_loss: 0.7054677605628967\n",
      "step_no: 613, epoch: 2, batch_no: 127, loss: 1.1563310623168945, avg_loss: 0.7089901566505432\n",
      "step_no: 614, epoch: 2, batch_no: 128, loss: 0.8350168466567993, avg_loss: 0.7099670767784119\n",
      "step_no: 615, epoch: 2, batch_no: 129, loss: 0.8788292407989502, avg_loss: 0.711266040802002\n",
      "step_no: 616, epoch: 2, batch_no: 130, loss: 0.6269528865814209, avg_loss: 0.7106224298477173\n",
      "step_no: 617, epoch: 2, batch_no: 131, loss: 1.2208579778671265, avg_loss: 0.7144878506660461\n",
      "step_no: 618, epoch: 2, batch_no: 132, loss: 0.8060487508773804, avg_loss: 0.7151762247085571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_no: 619, epoch: 2, batch_no: 133, loss: 0.8936689496040344, avg_loss: 0.7165082693099976\n",
      "step_no: 620, epoch: 2, batch_no: 134, loss: 0.9214319586753845, avg_loss: 0.7180262207984924\n",
      "step_no: 621, epoch: 2, batch_no: 135, loss: 0.8089454174041748, avg_loss: 0.7186947464942932\n",
      "step_no: 622, epoch: 2, batch_no: 136, loss: 0.49909889698028564, avg_loss: 0.7170918583869934\n",
      "step_no: 623, epoch: 2, batch_no: 137, loss: 0.6097792983055115, avg_loss: 0.7163142561912537\n",
      "step_no: 624, epoch: 2, batch_no: 138, loss: 0.5904964804649353, avg_loss: 0.7154091000556946\n",
      "step_no: 625, epoch: 2, batch_no: 139, loss: 0.6403546333312988, avg_loss: 0.7148730158805847\n",
      "step_no: 626, epoch: 2, batch_no: 140, loss: 0.8489046096801758, avg_loss: 0.7158235907554626\n",
      "step_no: 627, epoch: 2, batch_no: 141, loss: 0.6174882650375366, avg_loss: 0.7151311039924622\n",
      "step_no: 628, epoch: 2, batch_no: 142, loss: 0.6728795170783997, avg_loss: 0.7148356437683105\n",
      "step_no: 629, epoch: 2, batch_no: 143, loss: 0.7192246913909912, avg_loss: 0.7148661017417908\n",
      "step_no: 630, epoch: 2, batch_no: 144, loss: 0.5520839691162109, avg_loss: 0.7137435078620911\n",
      "step_no: 631, epoch: 2, batch_no: 145, loss: 0.5063707828521729, avg_loss: 0.7123231291770935\n",
      "step_no: 632, epoch: 2, batch_no: 146, loss: 0.7983971834182739, avg_loss: 0.7129086852073669\n",
      "step_no: 633, epoch: 2, batch_no: 147, loss: 0.9080639481544495, avg_loss: 0.7142273187637329\n",
      "step_no: 634, epoch: 2, batch_no: 148, loss: 0.634003221988678, avg_loss: 0.7136889100074768\n",
      "step_no: 635, epoch: 2, batch_no: 149, loss: 0.775854229927063, avg_loss: 0.7141033411026001\n",
      "step_no: 636, epoch: 2, batch_no: 150, loss: 0.644262969493866, avg_loss: 0.7136408090591431\n",
      "step_no: 637, epoch: 2, batch_no: 151, loss: 0.6615404486656189, avg_loss: 0.7132980227470398\n",
      "step_no: 638, epoch: 2, batch_no: 152, loss: 0.5368335247039795, avg_loss: 0.712144672870636\n",
      "step_no: 639, epoch: 2, batch_no: 153, loss: 0.651781439781189, avg_loss: 0.711752712726593\n",
      "step_no: 640, epoch: 2, batch_no: 154, loss: 0.7242875695228577, avg_loss: 0.7118335962295532\n",
      "step_no: 641, epoch: 2, batch_no: 155, loss: 0.43885722756385803, avg_loss: 0.7100837230682373\n",
      "step_no: 642, epoch: 2, batch_no: 156, loss: 1.2515017986297607, avg_loss: 0.7135322690010071\n",
      "step_no: 643, epoch: 2, batch_no: 157, loss: 0.8598644137382507, avg_loss: 0.7144584059715271\n",
      "step_no: 644, epoch: 2, batch_no: 158, loss: 0.48899948596954346, avg_loss: 0.7130404114723206\n",
      "step_no: 645, epoch: 2, batch_no: 159, loss: 0.6674694418907166, avg_loss: 0.7127556204795837\n",
      "step_no: 646, epoch: 2, batch_no: 160, loss: 1.04268479347229, avg_loss: 0.7148048877716064\n",
      "step_no: 647, epoch: 2, batch_no: 161, loss: 0.5383291840553284, avg_loss: 0.7137155532836914\n",
      "step_no: 648, epoch: 2, batch_no: 162, loss: 0.7328692674636841, avg_loss: 0.7138330340385437\n",
      "step_no: 649, epoch: 2, batch_no: 163, loss: 0.7314508557319641, avg_loss: 0.7139405012130737\n",
      "step_no: 650, epoch: 2, batch_no: 164, loss: 0.9426321983337402, avg_loss: 0.7153264880180359\n",
      "step_no: 651, epoch: 2, batch_no: 165, loss: 0.49437081813812256, avg_loss: 0.7139954566955566\n",
      "step_no: 652, epoch: 2, batch_no: 166, loss: 0.7554200887680054, avg_loss: 0.7142435312271118\n",
      "step_no: 653, epoch: 2, batch_no: 167, loss: 0.3739931881427765, avg_loss: 0.7122182250022888\n",
      "step_no: 654, epoch: 2, batch_no: 168, loss: 1.1230790615081787, avg_loss: 0.7146493196487427\n",
      "step_no: 655, epoch: 2, batch_no: 169, loss: 1.050463318824768, avg_loss: 0.7166246771812439\n",
      "step_no: 656, epoch: 2, batch_no: 170, loss: 0.8542100787162781, avg_loss: 0.7174292802810669\n",
      "step_no: 657, epoch: 2, batch_no: 171, loss: 0.7800016403198242, avg_loss: 0.7177930474281311\n",
      "step_no: 658, epoch: 2, batch_no: 172, loss: 0.8148618340492249, avg_loss: 0.7183541655540466\n",
      "step_no: 659, epoch: 2, batch_no: 173, loss: 0.8809124231338501, avg_loss: 0.7192884087562561\n",
      "step_no: 660, epoch: 2, batch_no: 174, loss: 0.7235763669013977, avg_loss: 0.7193129062652588\n",
      "step_no: 661, epoch: 2, batch_no: 175, loss: 0.7542223930358887, avg_loss: 0.7195112705230713\n",
      "step_no: 662, epoch: 2, batch_no: 176, loss: 0.599696934223175, avg_loss: 0.7188343405723572\n",
      "step_no: 663, epoch: 2, batch_no: 177, loss: 0.8007068037986755, avg_loss: 0.7192942500114441\n",
      "step_no: 664, epoch: 2, batch_no: 178, loss: 1.2570549249649048, avg_loss: 0.7222984433174133\n",
      "step_no: 665, epoch: 2, batch_no: 179, loss: 0.9084781408309937, avg_loss: 0.7233328223228455\n",
      "step_no: 666, epoch: 2, batch_no: 180, loss: 0.5776255130767822, avg_loss: 0.722527801990509\n",
      "step_no: 667, epoch: 2, batch_no: 181, loss: 0.7616461515426636, avg_loss: 0.7227427363395691\n",
      "step_no: 668, epoch: 2, batch_no: 182, loss: 0.7860321998596191, avg_loss: 0.7230885028839111\n",
      "step_no: 669, epoch: 2, batch_no: 183, loss: 0.8850471377372742, avg_loss: 0.7239686846733093\n",
      "step_no: 670, epoch: 2, batch_no: 184, loss: 0.687814474105835, avg_loss: 0.7237733006477356\n",
      "step_no: 671, epoch: 2, batch_no: 185, loss: 0.6292036771774292, avg_loss: 0.7232648134231567\n",
      "step_no: 672, epoch: 2, batch_no: 186, loss: 0.7878110408782959, avg_loss: 0.723609983921051\n",
      "step_no: 673, epoch: 2, batch_no: 187, loss: 0.7399901151657104, avg_loss: 0.7236970663070679\n",
      "step_no: 674, epoch: 2, batch_no: 188, loss: 0.445490300655365, avg_loss: 0.7222251296043396\n",
      "step_no: 675, epoch: 2, batch_no: 189, loss: 0.6868239641189575, avg_loss: 0.7220388650894165\n",
      "step_no: 676, epoch: 2, batch_no: 190, loss: 0.6658416986465454, avg_loss: 0.7217446565628052\n",
      "step_no: 677, epoch: 2, batch_no: 191, loss: 0.8735430240631104, avg_loss: 0.7225353121757507\n",
      "step_no: 678, epoch: 2, batch_no: 192, loss: 0.6841380000114441, avg_loss: 0.7223363518714905\n",
      "step_no: 679, epoch: 2, batch_no: 193, loss: 0.9602024555206299, avg_loss: 0.723562479019165\n",
      "step_no: 680, epoch: 2, batch_no: 194, loss: 0.5363572239875793, avg_loss: 0.7226024866104126\n",
      "step_no: 681, epoch: 2, batch_no: 195, loss: 1.083717703819275, avg_loss: 0.7244449257850647\n",
      "step_no: 682, epoch: 2, batch_no: 196, loss: 1.0455398559570312, avg_loss: 0.7260748147964478\n",
      "step_no: 683, epoch: 2, batch_no: 197, loss: 0.4726560413837433, avg_loss: 0.7247949242591858\n",
      "step_no: 684, epoch: 2, batch_no: 198, loss: 0.43092912435531616, avg_loss: 0.7233182191848755\n",
      "step_no: 685, epoch: 2, batch_no: 199, loss: 0.4805459678173065, avg_loss: 0.7221043109893799\n",
      "step_no: 686, epoch: 2, batch_no: 200, loss: 0.6957032680511475, avg_loss: 0.7219730019569397\n",
      "step_no: 687, epoch: 2, batch_no: 201, loss: 1.2205287218093872, avg_loss: 0.7244411110877991\n",
      "step_no: 688, epoch: 2, batch_no: 202, loss: 1.199964165687561, avg_loss: 0.7267836332321167\n",
      "step_no: 689, epoch: 2, batch_no: 203, loss: 0.555581271648407, avg_loss: 0.725944459438324\n",
      "step_no: 690, epoch: 2, batch_no: 204, loss: 0.8214507102966309, avg_loss: 0.7264103889465332\n",
      "step_no: 691, epoch: 2, batch_no: 205, loss: 0.589011549949646, avg_loss: 0.7257433533668518\n",
      "step_no: 692, epoch: 2, batch_no: 206, loss: 0.8219435214996338, avg_loss: 0.7262080907821655\n",
      "step_no: 693, epoch: 2, batch_no: 207, loss: 0.47864067554473877, avg_loss: 0.7250178456306458\n",
      "step_no: 694, epoch: 2, batch_no: 208, loss: 0.6053547263145447, avg_loss: 0.7244452834129333\n",
      "step_no: 695, epoch: 2, batch_no: 209, loss: 0.6919774413108826, avg_loss: 0.7242907285690308\n",
      "step_no: 696, epoch: 2, batch_no: 210, loss: 0.8871870636940002, avg_loss: 0.7250627279281616\n",
      "step_no: 697, epoch: 2, batch_no: 211, loss: 0.5614461898803711, avg_loss: 0.7242909669876099\n",
      "step_no: 698, epoch: 2, batch_no: 212, loss: 0.599308431148529, avg_loss: 0.7237041592597961\n",
      "step_no: 699, epoch: 2, batch_no: 213, loss: 0.8002681732177734, avg_loss: 0.724061906337738\n",
      "0.41916252778152185\n",
      "step_no: 700, epoch: 2, batch_no: 214, loss: 0.4221462905406952, avg_loss: 0.7226576805114746\n",
      "step_no: 701, epoch: 2, batch_no: 215, loss: 1.0118088722229004, avg_loss: 0.7239963412284851\n",
      "step_no: 702, epoch: 2, batch_no: 216, loss: 0.76988685131073, avg_loss: 0.7242078185081482\n",
      "step_no: 703, epoch: 2, batch_no: 217, loss: 0.890820324420929, avg_loss: 0.7249720692634583\n",
      "step_no: 704, epoch: 2, batch_no: 218, loss: 0.839557409286499, avg_loss: 0.7254952788352966\n",
      "step_no: 705, epoch: 2, batch_no: 219, loss: 0.5697556138038635, avg_loss: 0.7247874140739441\n",
      "step_no: 706, epoch: 2, batch_no: 220, loss: 0.8864261507987976, avg_loss: 0.7255188822746277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_no: 707, epoch: 2, batch_no: 221, loss: 0.6530042886734009, avg_loss: 0.7251921892166138\n",
      "step_no: 708, epoch: 2, batch_no: 222, loss: 0.8682955503463745, avg_loss: 0.7258339524269104\n",
      "step_no: 709, epoch: 2, batch_no: 223, loss: 0.8069874048233032, avg_loss: 0.7261962294578552\n",
      "step_no: 710, epoch: 2, batch_no: 224, loss: 0.6614363789558411, avg_loss: 0.7259083986282349\n",
      "step_no: 711, epoch: 2, batch_no: 225, loss: 0.692332923412323, avg_loss: 0.7257598638534546\n",
      "step_no: 712, epoch: 2, batch_no: 226, loss: 0.7294690012931824, avg_loss: 0.7257761359214783\n",
      "step_no: 713, epoch: 2, batch_no: 227, loss: 1.1299498081207275, avg_loss: 0.7275488376617432\n",
      "step_no: 714, epoch: 2, batch_no: 228, loss: 0.5374091267585754, avg_loss: 0.726718544960022\n",
      "step_no: 715, epoch: 2, batch_no: 229, loss: 0.4349885582923889, avg_loss: 0.7254500985145569\n",
      "step_no: 716, epoch: 2, batch_no: 230, loss: 0.6222731471061707, avg_loss: 0.7250034809112549\n",
      "step_no: 717, epoch: 2, batch_no: 231, loss: 0.7431102395057678, avg_loss: 0.7250815033912659\n",
      "step_no: 718, epoch: 2, batch_no: 232, loss: 0.6962324380874634, avg_loss: 0.7249576449394226\n",
      "step_no: 719, epoch: 2, batch_no: 233, loss: 0.8029986619949341, avg_loss: 0.7252911925315857\n",
      "step_no: 720, epoch: 2, batch_no: 234, loss: 0.6053615212440491, avg_loss: 0.724780797958374\n",
      "step_no: 721, epoch: 2, batch_no: 235, loss: 0.5631635785102844, avg_loss: 0.7240959405899048\n",
      "step_no: 722, epoch: 2, batch_no: 236, loss: 0.8869988322257996, avg_loss: 0.7247832417488098\n",
      "step_no: 723, epoch: 2, batch_no: 237, loss: 0.874700665473938, avg_loss: 0.7254132032394409\n",
      "step_no: 724, epoch: 2, batch_no: 238, loss: 0.5733427405357361, avg_loss: 0.7247768640518188\n",
      "step_no: 725, epoch: 2, batch_no: 239, loss: 0.6706033945083618, avg_loss: 0.724551260471344\n",
      "step_no: 726, epoch: 2, batch_no: 240, loss: 0.6307510733604431, avg_loss: 0.7241620421409607\n",
      "step_no: 727, epoch: 2, batch_no: 241, loss: 0.642655611038208, avg_loss: 0.7238251566886902\n",
      "step_no: 728, epoch: 2, batch_no: 242, loss: 0.9457985162734985, avg_loss: 0.7247386574745178\n",
      "step_no: 729, epoch: 3, batch_no: 0, loss: 0.8963941931724548, avg_loss: 0.8963941931724548\n",
      "step_no: 730, epoch: 3, batch_no: 1, loss: 0.5927006602287292, avg_loss: 0.744547426700592\n",
      "step_no: 731, epoch: 3, batch_no: 2, loss: 0.829612135887146, avg_loss: 0.7729023694992065\n",
      "step_no: 732, epoch: 3, batch_no: 3, loss: 0.5632504224777222, avg_loss: 0.7204893827438354\n",
      "step_no: 733, epoch: 3, batch_no: 4, loss: 0.8357953429222107, avg_loss: 0.7435505986213684\n",
      "step_no: 734, epoch: 3, batch_no: 5, loss: 1.3827829360961914, avg_loss: 0.8500893115997314\n",
      "step_no: 735, epoch: 3, batch_no: 6, loss: 0.6950641870498657, avg_loss: 0.8279429078102112\n",
      "step_no: 736, epoch: 3, batch_no: 7, loss: 0.8523329496383667, avg_loss: 0.8309916257858276\n",
      "step_no: 737, epoch: 3, batch_no: 8, loss: 1.0859405994415283, avg_loss: 0.859319269657135\n",
      "step_no: 738, epoch: 3, batch_no: 9, loss: 0.6298824548721313, avg_loss: 0.8363756537437439\n",
      "step_no: 739, epoch: 3, batch_no: 10, loss: 0.7215093970298767, avg_loss: 0.8259332180023193\n",
      "step_no: 740, epoch: 3, batch_no: 11, loss: 0.8867083191871643, avg_loss: 0.8309978246688843\n",
      "step_no: 741, epoch: 3, batch_no: 12, loss: 0.9314282536506653, avg_loss: 0.8387232422828674\n",
      "step_no: 742, epoch: 3, batch_no: 13, loss: 0.701294481754303, avg_loss: 0.8289068937301636\n",
      "step_no: 743, epoch: 3, batch_no: 14, loss: 0.6380731463432312, avg_loss: 0.8161846399307251\n",
      "step_no: 744, epoch: 3, batch_no: 15, loss: 0.6807149648666382, avg_loss: 0.8077177405357361\n",
      "step_no: 745, epoch: 3, batch_no: 16, loss: 0.4277666211128235, avg_loss: 0.7853676676750183\n",
      "step_no: 746, epoch: 3, batch_no: 17, loss: 0.7325699329376221, avg_loss: 0.7824344635009766\n",
      "step_no: 747, epoch: 3, batch_no: 18, loss: 0.6372185945510864, avg_loss: 0.7747915387153625\n",
      "step_no: 748, epoch: 3, batch_no: 19, loss: 0.9467906951904297, avg_loss: 0.7833914756774902\n",
      "step_no: 749, epoch: 3, batch_no: 20, loss: 0.7066735625267029, avg_loss: 0.779738187789917\n",
      "step_no: 750, epoch: 3, batch_no: 21, loss: 0.6097520589828491, avg_loss: 0.7720115780830383\n",
      "step_no: 751, epoch: 3, batch_no: 22, loss: 0.7486627101898193, avg_loss: 0.7709964513778687\n",
      "step_no: 752, epoch: 3, batch_no: 23, loss: 0.5355207324028015, avg_loss: 0.7611849308013916\n",
      "step_no: 753, epoch: 3, batch_no: 24, loss: 0.4371626377105713, avg_loss: 0.7482240200042725\n",
      "step_no: 754, epoch: 3, batch_no: 25, loss: 0.7453413009643555, avg_loss: 0.748113214969635\n",
      "step_no: 755, epoch: 3, batch_no: 26, loss: 0.5619381070137024, avg_loss: 0.7412177920341492\n",
      "step_no: 756, epoch: 3, batch_no: 27, loss: 0.7736853361129761, avg_loss: 0.7423774003982544\n",
      "step_no: 757, epoch: 3, batch_no: 28, loss: 0.49158579111099243, avg_loss: 0.7337294220924377\n",
      "step_no: 758, epoch: 3, batch_no: 29, loss: 0.7376370429992676, avg_loss: 0.733859658241272\n",
      "step_no: 759, epoch: 3, batch_no: 30, loss: 0.6089126467704773, avg_loss: 0.7298290729522705\n",
      "step_no: 760, epoch: 3, batch_no: 31, loss: 0.9417049884796143, avg_loss: 0.7364502549171448\n",
      "step_no: 761, epoch: 3, batch_no: 32, loss: 1.0052146911621094, avg_loss: 0.7445946335792542\n",
      "step_no: 762, epoch: 3, batch_no: 33, loss: 0.9403125047683716, avg_loss: 0.750351071357727\n",
      "step_no: 763, epoch: 3, batch_no: 34, loss: 0.9198392629623413, avg_loss: 0.7551935911178589\n",
      "step_no: 764, epoch: 3, batch_no: 35, loss: 0.5758997201919556, avg_loss: 0.7502132058143616\n",
      "step_no: 765, epoch: 3, batch_no: 36, loss: 0.7427307367324829, avg_loss: 0.7500110268592834\n",
      "step_no: 766, epoch: 3, batch_no: 37, loss: 1.2158548831939697, avg_loss: 0.7622700333595276\n",
      "step_no: 767, epoch: 3, batch_no: 38, loss: 0.5051988959312439, avg_loss: 0.7556784749031067\n",
      "step_no: 768, epoch: 3, batch_no: 39, loss: 0.6193251609802246, avg_loss: 0.7522696852684021\n",
      "step_no: 769, epoch: 3, batch_no: 40, loss: 0.8243906497955322, avg_loss: 0.7540286779403687\n",
      "step_no: 770, epoch: 3, batch_no: 41, loss: 0.9345163702964783, avg_loss: 0.7583260536193848\n",
      "step_no: 771, epoch: 3, batch_no: 42, loss: 0.596601665019989, avg_loss: 0.7545650601387024\n",
      "step_no: 772, epoch: 3, batch_no: 43, loss: 0.9448285698890686, avg_loss: 0.7588891983032227\n",
      "step_no: 773, epoch: 3, batch_no: 44, loss: 0.6559848785400391, avg_loss: 0.7566025257110596\n",
      "step_no: 774, epoch: 3, batch_no: 45, loss: 1.0118324756622314, avg_loss: 0.7621510028839111\n",
      "step_no: 775, epoch: 3, batch_no: 46, loss: 0.7444308996200562, avg_loss: 0.761773943901062\n",
      "step_no: 776, epoch: 3, batch_no: 47, loss: 0.7939700484275818, avg_loss: 0.7624446749687195\n",
      "step_no: 777, epoch: 3, batch_no: 48, loss: 0.805609941482544, avg_loss: 0.763325572013855\n",
      "step_no: 778, epoch: 3, batch_no: 49, loss: 0.5278685688972473, avg_loss: 0.7586164474487305\n",
      "step_no: 779, epoch: 3, batch_no: 50, loss: 0.946337878704071, avg_loss: 0.7622973322868347\n",
      "step_no: 780, epoch: 3, batch_no: 51, loss: 0.7816347479820251, avg_loss: 0.7626692056655884\n",
      "step_no: 781, epoch: 3, batch_no: 52, loss: 0.5557856559753418, avg_loss: 0.7587657570838928\n",
      "step_no: 782, epoch: 3, batch_no: 53, loss: 0.9091488718986511, avg_loss: 0.7615506052970886\n",
      "step_no: 783, epoch: 3, batch_no: 54, loss: 0.42555665969848633, avg_loss: 0.7554416060447693\n",
      "step_no: 784, epoch: 3, batch_no: 55, loss: 0.8117294907569885, avg_loss: 0.7564467787742615\n",
      "step_no: 785, epoch: 3, batch_no: 56, loss: 0.8655686378479004, avg_loss: 0.7583612203598022\n",
      "step_no: 786, epoch: 3, batch_no: 57, loss: 0.6469801664352417, avg_loss: 0.7564408779144287\n",
      "step_no: 787, epoch: 3, batch_no: 58, loss: 0.554513156414032, avg_loss: 0.753018319606781\n",
      "step_no: 788, epoch: 3, batch_no: 59, loss: 0.697555661201477, avg_loss: 0.7520939707756042\n",
      "step_no: 789, epoch: 3, batch_no: 60, loss: 0.6814333200454712, avg_loss: 0.7509355545043945\n",
      "step_no: 790, epoch: 3, batch_no: 61, loss: 0.7154266238212585, avg_loss: 0.7503628730773926\n",
      "step_no: 791, epoch: 3, batch_no: 62, loss: 0.6804072856903076, avg_loss: 0.7492525577545166\n",
      "step_no: 792, epoch: 3, batch_no: 63, loss: 0.7798535227775574, avg_loss: 0.74973064661026\n",
      "step_no: 793, epoch: 3, batch_no: 64, loss: 0.5927689075469971, avg_loss: 0.7473158836364746\n",
      "step_no: 794, epoch: 3, batch_no: 65, loss: 0.7888333201408386, avg_loss: 0.7479449510574341\n",
      "step_no: 795, epoch: 3, batch_no: 66, loss: 0.8493624329566956, avg_loss: 0.7494585514068604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_no: 796, epoch: 3, batch_no: 67, loss: 0.3514384925365448, avg_loss: 0.7436053156852722\n",
      "step_no: 797, epoch: 3, batch_no: 68, loss: 0.4727587401866913, avg_loss: 0.7396800518035889\n",
      "step_no: 798, epoch: 3, batch_no: 69, loss: 0.7615422010421753, avg_loss: 0.7399923801422119\n",
      "step_no: 799, epoch: 3, batch_no: 70, loss: 0.9693300127983093, avg_loss: 0.7432224750518799\n",
      "0.40918319162620304\n",
      "step_no: 800, epoch: 3, batch_no: 71, loss: 0.7591568827629089, avg_loss: 0.7434437870979309\n",
      "step_no: 801, epoch: 3, batch_no: 72, loss: 1.0319501161575317, avg_loss: 0.747395932674408\n",
      "step_no: 802, epoch: 3, batch_no: 73, loss: 0.6341413259506226, avg_loss: 0.7458654642105103\n",
      "step_no: 803, epoch: 3, batch_no: 74, loss: 0.7233393788337708, avg_loss: 0.7455651164054871\n",
      "step_no: 804, epoch: 3, batch_no: 75, loss: 0.84920334815979, avg_loss: 0.7469287514686584\n",
      "step_no: 805, epoch: 3, batch_no: 76, loss: 0.8114935159683228, avg_loss: 0.7477672696113586\n",
      "step_no: 806, epoch: 3, batch_no: 77, loss: 0.4632052481174469, avg_loss: 0.7441189885139465\n",
      "step_no: 807, epoch: 3, batch_no: 78, loss: 0.49735480546951294, avg_loss: 0.740995466709137\n",
      "step_no: 808, epoch: 3, batch_no: 79, loss: 0.663852334022522, avg_loss: 0.7400311827659607\n",
      "step_no: 809, epoch: 3, batch_no: 80, loss: 0.422939270734787, avg_loss: 0.7361164093017578\n",
      "step_no: 810, epoch: 3, batch_no: 81, loss: 0.9228788614273071, avg_loss: 0.7383939623832703\n",
      "step_no: 811, epoch: 3, batch_no: 82, loss: 0.6123262643814087, avg_loss: 0.7368751168251038\n",
      "step_no: 812, epoch: 3, batch_no: 83, loss: 0.6907559633255005, avg_loss: 0.7363261580467224\n",
      "step_no: 813, epoch: 3, batch_no: 84, loss: 1.1520435810089111, avg_loss: 0.7412168979644775\n",
      "step_no: 814, epoch: 3, batch_no: 85, loss: 0.5735679268836975, avg_loss: 0.7392674684524536\n",
      "step_no: 815, epoch: 3, batch_no: 86, loss: 0.6000399589538574, avg_loss: 0.7376671433448792\n",
      "step_no: 816, epoch: 3, batch_no: 87, loss: 0.8893903493881226, avg_loss: 0.7393912672996521\n",
      "step_no: 817, epoch: 3, batch_no: 88, loss: 0.8094814419746399, avg_loss: 0.7401787638664246\n",
      "step_no: 818, epoch: 3, batch_no: 89, loss: 0.3471428155899048, avg_loss: 0.735811710357666\n",
      "step_no: 819, epoch: 3, batch_no: 90, loss: 0.8356703519821167, avg_loss: 0.7369090914726257\n",
      "step_no: 820, epoch: 3, batch_no: 91, loss: 0.6607627272605896, avg_loss: 0.7360813617706299\n",
      "step_no: 821, epoch: 3, batch_no: 92, loss: 0.512797474861145, avg_loss: 0.7336803674697876\n",
      "step_no: 822, epoch: 3, batch_no: 93, loss: 0.9902557134628296, avg_loss: 0.7364099025726318\n",
      "step_no: 823, epoch: 3, batch_no: 94, loss: 0.7273811101913452, avg_loss: 0.7363148927688599\n",
      "step_no: 824, epoch: 3, batch_no: 95, loss: 0.6505582332611084, avg_loss: 0.7354215979576111\n",
      "step_no: 825, epoch: 3, batch_no: 96, loss: 0.5637833476066589, avg_loss: 0.7336520552635193\n",
      "step_no: 826, epoch: 3, batch_no: 97, loss: 0.6765964031219482, avg_loss: 0.733069896697998\n",
      "step_no: 827, epoch: 3, batch_no: 98, loss: 0.8601031303405762, avg_loss: 0.7343530654907227\n",
      "step_no: 828, epoch: 3, batch_no: 99, loss: 0.7134546041488647, avg_loss: 0.7341440320014954\n",
      "step_no: 829, epoch: 3, batch_no: 100, loss: 0.6417423486709595, avg_loss: 0.7332291603088379\n",
      "step_no: 830, epoch: 3, batch_no: 101, loss: 0.3807288408279419, avg_loss: 0.7297732830047607\n",
      "step_no: 831, epoch: 3, batch_no: 102, loss: 0.6521985530853271, avg_loss: 0.7290201187133789\n",
      "step_no: 832, epoch: 3, batch_no: 103, loss: 0.8158487677574158, avg_loss: 0.7298550605773926\n",
      "step_no: 833, epoch: 3, batch_no: 104, loss: 0.8006483316421509, avg_loss: 0.7305293083190918\n",
      "step_no: 834, epoch: 3, batch_no: 105, loss: 0.7793103456497192, avg_loss: 0.7309895157814026\n",
      "step_no: 835, epoch: 3, batch_no: 106, loss: 0.6575190424919128, avg_loss: 0.7303028106689453\n",
      "step_no: 836, epoch: 3, batch_no: 107, loss: 0.7692708969116211, avg_loss: 0.7306636571884155\n",
      "step_no: 837, epoch: 3, batch_no: 108, loss: 0.560995876789093, avg_loss: 0.7291070222854614\n",
      "step_no: 838, epoch: 3, batch_no: 109, loss: 0.735527753829956, avg_loss: 0.7291654348373413\n",
      "step_no: 839, epoch: 3, batch_no: 110, loss: 0.9897599816322327, avg_loss: 0.7315131425857544\n",
      "step_no: 840, epoch: 3, batch_no: 111, loss: 0.6870326399803162, avg_loss: 0.7311160564422607\n",
      "step_no: 841, epoch: 3, batch_no: 112, loss: 0.6727249622344971, avg_loss: 0.7305992841720581\n",
      "step_no: 842, epoch: 3, batch_no: 113, loss: 0.5850560069084167, avg_loss: 0.7293225526809692\n",
      "step_no: 843, epoch: 3, batch_no: 114, loss: 0.7084681987762451, avg_loss: 0.729141116142273\n",
      "step_no: 844, epoch: 3, batch_no: 115, loss: 0.8566074967384338, avg_loss: 0.730239987373352\n",
      "step_no: 845, epoch: 3, batch_no: 116, loss: 0.8762450814247131, avg_loss: 0.7314879298210144\n",
      "step_no: 846, epoch: 3, batch_no: 117, loss: 0.9835831522941589, avg_loss: 0.733624279499054\n",
      "step_no: 847, epoch: 3, batch_no: 118, loss: 0.5270208120346069, avg_loss: 0.7318881750106812\n",
      "step_no: 848, epoch: 3, batch_no: 119, loss: 0.5736365914344788, avg_loss: 0.7305694222450256\n",
      "step_no: 849, epoch: 3, batch_no: 120, loss: 0.9801229238510132, avg_loss: 0.7326318025588989\n",
      "step_no: 850, epoch: 3, batch_no: 121, loss: 0.5879647135734558, avg_loss: 0.7314460277557373\n",
      "step_no: 851, epoch: 3, batch_no: 122, loss: 0.619138240814209, avg_loss: 0.7305330038070679\n",
      "step_no: 852, epoch: 3, batch_no: 123, loss: 0.7448409795761108, avg_loss: 0.730648398399353\n",
      "step_no: 853, epoch: 3, batch_no: 124, loss: 0.9071547985076904, avg_loss: 0.7320604920387268\n",
      "step_no: 854, epoch: 3, batch_no: 125, loss: 0.9437334537506104, avg_loss: 0.7337404489517212\n",
      "step_no: 855, epoch: 3, batch_no: 126, loss: 0.5438705086708069, avg_loss: 0.7322453856468201\n",
      "step_no: 856, epoch: 3, batch_no: 127, loss: 0.825718879699707, avg_loss: 0.7329756617546082\n",
      "step_no: 857, epoch: 3, batch_no: 128, loss: 0.9066351056098938, avg_loss: 0.7343218326568604\n",
      "step_no: 858, epoch: 3, batch_no: 129, loss: 0.6653430461883545, avg_loss: 0.7337912321090698\n",
      "step_no: 859, epoch: 3, batch_no: 130, loss: 0.8561644554138184, avg_loss: 0.7347253561019897\n",
      "step_no: 860, epoch: 3, batch_no: 131, loss: 0.8807697892189026, avg_loss: 0.7358317971229553\n",
      "step_no: 861, epoch: 3, batch_no: 132, loss: 1.020569086074829, avg_loss: 0.7379726767539978\n",
      "step_no: 862, epoch: 3, batch_no: 133, loss: 1.31553053855896, avg_loss: 0.7422827482223511\n",
      "step_no: 863, epoch: 3, batch_no: 134, loss: 0.992960512638092, avg_loss: 0.7441396117210388\n",
      "step_no: 864, epoch: 3, batch_no: 135, loss: 0.7033597230911255, avg_loss: 0.7438398003578186\n",
      "step_no: 865, epoch: 3, batch_no: 136, loss: 0.6167911887168884, avg_loss: 0.7429124116897583\n",
      "step_no: 866, epoch: 3, batch_no: 137, loss: 0.3553183376789093, avg_loss: 0.7401037216186523\n",
      "step_no: 867, epoch: 3, batch_no: 138, loss: 1.016278624534607, avg_loss: 0.74209064245224\n",
      "step_no: 868, epoch: 3, batch_no: 139, loss: 0.6147235631942749, avg_loss: 0.7411808371543884\n",
      "step_no: 869, epoch: 3, batch_no: 140, loss: 0.47243285179138184, avg_loss: 0.7392748594284058\n",
      "step_no: 870, epoch: 3, batch_no: 141, loss: 0.7860385179519653, avg_loss: 0.7396041750907898\n",
      "step_no: 871, epoch: 3, batch_no: 142, loss: 0.8466865420341492, avg_loss: 0.7403530478477478\n",
      "step_no: 872, epoch: 3, batch_no: 143, loss: 0.3827328681945801, avg_loss: 0.7378695607185364\n",
      "step_no: 873, epoch: 3, batch_no: 144, loss: 0.6525964736938477, avg_loss: 0.7372815012931824\n",
      "step_no: 874, epoch: 3, batch_no: 145, loss: 0.8277158141136169, avg_loss: 0.7379008531570435\n",
      "step_no: 875, epoch: 3, batch_no: 146, loss: 0.7743579149246216, avg_loss: 0.7381488680839539\n",
      "step_no: 876, epoch: 3, batch_no: 147, loss: 1.0109357833862305, avg_loss: 0.739992082118988\n",
      "step_no: 877, epoch: 3, batch_no: 148, loss: 0.8054717183113098, avg_loss: 0.7404314875602722\n",
      "step_no: 878, epoch: 3, batch_no: 149, loss: 0.32997429370880127, avg_loss: 0.7376950979232788\n",
      "step_no: 879, epoch: 3, batch_no: 150, loss: 0.8094850182533264, avg_loss: 0.7381705641746521\n",
      "step_no: 880, epoch: 3, batch_no: 151, loss: 1.0992940664291382, avg_loss: 0.7405464053153992\n",
      "step_no: 881, epoch: 3, batch_no: 152, loss: 0.9728371500968933, avg_loss: 0.7420646548271179\n",
      "step_no: 882, epoch: 3, batch_no: 153, loss: 0.7994289994239807, avg_loss: 0.7424371242523193\n",
      "step_no: 883, epoch: 3, batch_no: 154, loss: 0.7196782827377319, avg_loss: 0.7422903180122375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_no: 884, epoch: 3, batch_no: 155, loss: 0.6801006197929382, avg_loss: 0.7418916821479797\n",
      "step_no: 885, epoch: 3, batch_no: 156, loss: 0.5648625493049622, avg_loss: 0.7407641410827637\n",
      "step_no: 886, epoch: 3, batch_no: 157, loss: 0.6691088676452637, avg_loss: 0.7403106093406677\n",
      "step_no: 887, epoch: 3, batch_no: 158, loss: 0.7336923480033875, avg_loss: 0.7402689456939697\n",
      "step_no: 888, epoch: 3, batch_no: 159, loss: 0.6505017876625061, avg_loss: 0.7397079467773438\n",
      "step_no: 889, epoch: 3, batch_no: 160, loss: 0.6854408979415894, avg_loss: 0.7393708825111389\n",
      "step_no: 890, epoch: 3, batch_no: 161, loss: 0.4531833529472351, avg_loss: 0.7376043200492859\n",
      "step_no: 891, epoch: 3, batch_no: 162, loss: 0.3367767930030823, avg_loss: 0.7351452112197876\n",
      "step_no: 892, epoch: 3, batch_no: 163, loss: 0.5806828141212463, avg_loss: 0.7342033386230469\n",
      "step_no: 893, epoch: 3, batch_no: 164, loss: 0.8747270107269287, avg_loss: 0.7350550293922424\n",
      "step_no: 894, epoch: 3, batch_no: 165, loss: 0.48852843046188354, avg_loss: 0.733569860458374\n",
      "step_no: 895, epoch: 3, batch_no: 166, loss: 0.9190206527709961, avg_loss: 0.7346804141998291\n",
      "step_no: 896, epoch: 3, batch_no: 167, loss: 0.37002378702163696, avg_loss: 0.7325098514556885\n",
      "step_no: 897, epoch: 3, batch_no: 168, loss: 0.49810752272605896, avg_loss: 0.7311228513717651\n",
      "step_no: 898, epoch: 3, batch_no: 169, loss: 0.7834838032722473, avg_loss: 0.7314308881759644\n",
      "step_no: 899, epoch: 3, batch_no: 170, loss: 0.4986753463745117, avg_loss: 0.7300696969032288\n",
      "0.4273054232818129\n",
      "step_no: 900, epoch: 3, batch_no: 171, loss: 0.681878387928009, avg_loss: 0.7297894954681396\n",
      "step_no: 901, epoch: 3, batch_no: 172, loss: 0.7419270277023315, avg_loss: 0.7298596501350403\n",
      "step_no: 902, epoch: 3, batch_no: 173, loss: 1.0688608884811401, avg_loss: 0.7318079471588135\n",
      "step_no: 903, epoch: 3, batch_no: 174, loss: 0.8028841018676758, avg_loss: 0.732214093208313\n",
      "step_no: 904, epoch: 3, batch_no: 175, loss: 0.6536841988563538, avg_loss: 0.7317679524421692\n",
      "step_no: 905, epoch: 3, batch_no: 176, loss: 0.7791434526443481, avg_loss: 0.7320355772972107\n",
      "step_no: 906, epoch: 3, batch_no: 177, loss: 0.7532266974449158, avg_loss: 0.7321546077728271\n",
      "step_no: 907, epoch: 3, batch_no: 178, loss: 0.5973758101463318, avg_loss: 0.7314016222953796\n",
      "step_no: 908, epoch: 3, batch_no: 179, loss: 0.7933210730552673, avg_loss: 0.7317456603050232\n",
      "step_no: 909, epoch: 3, batch_no: 180, loss: 0.8133775591850281, avg_loss: 0.7321967482566833\n",
      "step_no: 910, epoch: 3, batch_no: 181, loss: 0.6969471573829651, avg_loss: 0.7320030331611633\n",
      "step_no: 911, epoch: 3, batch_no: 182, loss: 0.6818275451660156, avg_loss: 0.7317287921905518\n",
      "step_no: 912, epoch: 3, batch_no: 183, loss: 0.7901142239570618, avg_loss: 0.7320461273193359\n",
      "step_no: 913, epoch: 3, batch_no: 184, loss: 0.8056308031082153, avg_loss: 0.7324439287185669\n",
      "step_no: 914, epoch: 3, batch_no: 185, loss: 0.8080968856811523, avg_loss: 0.7328506112098694\n",
      "step_no: 915, epoch: 3, batch_no: 186, loss: 0.6951144933700562, avg_loss: 0.7326487898826599\n",
      "step_no: 916, epoch: 3, batch_no: 187, loss: 0.804181694984436, avg_loss: 0.733029305934906\n",
      "step_no: 917, epoch: 3, batch_no: 188, loss: 0.8141703009605408, avg_loss: 0.7334585785865784\n",
      "step_no: 918, epoch: 3, batch_no: 189, loss: 0.623358964920044, avg_loss: 0.732879102230072\n",
      "step_no: 919, epoch: 3, batch_no: 190, loss: 0.8582745790481567, avg_loss: 0.7335355877876282\n",
      "step_no: 920, epoch: 3, batch_no: 191, loss: 0.6644486784934998, avg_loss: 0.7331757545471191\n",
      "step_no: 921, epoch: 3, batch_no: 192, loss: 1.0882174968719482, avg_loss: 0.7350152730941772\n",
      "step_no: 922, epoch: 3, batch_no: 193, loss: 0.6688176393508911, avg_loss: 0.7346740961074829\n",
      "step_no: 923, epoch: 3, batch_no: 194, loss: 0.7855212688446045, avg_loss: 0.73493492603302\n",
      "step_no: 924, epoch: 3, batch_no: 195, loss: 0.7637613415718079, avg_loss: 0.7350819706916809\n",
      "step_no: 925, epoch: 3, batch_no: 196, loss: 0.4280683994293213, avg_loss: 0.7335234880447388\n",
      "step_no: 926, epoch: 3, batch_no: 197, loss: 0.7228934168815613, avg_loss: 0.7334699034690857\n",
      "step_no: 927, epoch: 3, batch_no: 198, loss: 0.9305259585380554, avg_loss: 0.7344601154327393\n",
      "step_no: 928, epoch: 3, batch_no: 199, loss: 0.715486466884613, avg_loss: 0.7343652248382568\n",
      "step_no: 929, epoch: 3, batch_no: 200, loss: 0.48830437660217285, avg_loss: 0.7331410646438599\n",
      "step_no: 930, epoch: 3, batch_no: 201, loss: 0.6340669393539429, avg_loss: 0.7326505780220032\n",
      "step_no: 931, epoch: 3, batch_no: 202, loss: 0.8535951375961304, avg_loss: 0.733246386051178\n",
      "step_no: 932, epoch: 3, batch_no: 203, loss: 0.4759826362133026, avg_loss: 0.731985330581665\n",
      "step_no: 933, epoch: 3, batch_no: 204, loss: 0.7986583709716797, avg_loss: 0.7323105335235596\n",
      "step_no: 934, epoch: 3, batch_no: 205, loss: 0.5391198396682739, avg_loss: 0.7313727140426636\n",
      "step_no: 935, epoch: 3, batch_no: 206, loss: 0.7706551551818848, avg_loss: 0.7315624952316284\n",
      "step_no: 936, epoch: 3, batch_no: 207, loss: 0.6556724309921265, avg_loss: 0.7311976552009583\n",
      "step_no: 937, epoch: 3, batch_no: 208, loss: 0.9833076000213623, avg_loss: 0.7324038743972778\n",
      "step_no: 938, epoch: 3, batch_no: 209, loss: 0.9112940430641174, avg_loss: 0.7332558035850525\n",
      "step_no: 939, epoch: 3, batch_no: 210, loss: 0.8187706470489502, avg_loss: 0.7336611151695251\n",
      "step_no: 940, epoch: 3, batch_no: 211, loss: 0.5714050531387329, avg_loss: 0.7328957915306091\n",
      "step_no: 941, epoch: 3, batch_no: 212, loss: 0.7767005562782288, avg_loss: 0.7331014275550842\n",
      "step_no: 942, epoch: 3, batch_no: 213, loss: 1.030404806137085, avg_loss: 0.734490692615509\n",
      "step_no: 943, epoch: 3, batch_no: 214, loss: 0.7979046106338501, avg_loss: 0.7347856163978577\n",
      "step_no: 944, epoch: 3, batch_no: 215, loss: 0.7891055941581726, avg_loss: 0.7350371479988098\n",
      "step_no: 945, epoch: 3, batch_no: 216, loss: 0.6759227514266968, avg_loss: 0.7347646951675415\n",
      "step_no: 946, epoch: 3, batch_no: 217, loss: 0.4895201027393341, avg_loss: 0.733639657497406\n",
      "step_no: 947, epoch: 3, batch_no: 218, loss: 0.5842183828353882, avg_loss: 0.7329573631286621\n",
      "step_no: 948, epoch: 3, batch_no: 219, loss: 0.5006272196769714, avg_loss: 0.7319013476371765\n",
      "step_no: 949, epoch: 3, batch_no: 220, loss: 0.6050585508346558, avg_loss: 0.7313274145126343\n",
      "step_no: 950, epoch: 3, batch_no: 221, loss: 0.8881510496139526, avg_loss: 0.7320338487625122\n",
      "step_no: 951, epoch: 3, batch_no: 222, loss: 0.8389650583267212, avg_loss: 0.7325133085250854\n",
      "step_no: 952, epoch: 3, batch_no: 223, loss: 0.6762279272079468, avg_loss: 0.7322620153427124\n",
      "step_no: 953, epoch: 3, batch_no: 224, loss: 0.5552510023117065, avg_loss: 0.731475293636322\n",
      "step_no: 954, epoch: 3, batch_no: 225, loss: 0.6603209376335144, avg_loss: 0.7311604619026184\n",
      "step_no: 955, epoch: 3, batch_no: 226, loss: 0.6917105317115784, avg_loss: 0.7309866547584534\n",
      "step_no: 956, epoch: 3, batch_no: 227, loss: 1.1144688129425049, avg_loss: 0.7326686382293701\n",
      "step_no: 957, epoch: 3, batch_no: 228, loss: 0.5350654721260071, avg_loss: 0.7318057417869568\n",
      "step_no: 958, epoch: 3, batch_no: 229, loss: 1.1363755464553833, avg_loss: 0.7335647344589233\n",
      "step_no: 959, epoch: 3, batch_no: 230, loss: 0.7945212125778198, avg_loss: 0.7338286638259888\n",
      "step_no: 960, epoch: 3, batch_no: 231, loss: 0.6836963891983032, avg_loss: 0.733612596988678\n",
      "step_no: 961, epoch: 3, batch_no: 232, loss: 0.2968308925628662, avg_loss: 0.7317379713058472\n",
      "step_no: 962, epoch: 3, batch_no: 233, loss: 1.0546724796295166, avg_loss: 0.7331180572509766\n",
      "step_no: 963, epoch: 3, batch_no: 234, loss: 0.9794549942016602, avg_loss: 0.7341662645339966\n",
      "step_no: 964, epoch: 3, batch_no: 235, loss: 0.8077545166015625, avg_loss: 0.7344781160354614\n",
      "step_no: 965, epoch: 3, batch_no: 236, loss: 0.9819122552871704, avg_loss: 0.7355221509933472\n",
      "step_no: 966, epoch: 3, batch_no: 237, loss: 0.7592251896858215, avg_loss: 0.7356218099594116\n",
      "step_no: 967, epoch: 3, batch_no: 238, loss: 0.5165672302246094, avg_loss: 0.7347052693367004\n",
      "step_no: 968, epoch: 3, batch_no: 239, loss: 0.5723311901092529, avg_loss: 0.7340287566184998\n",
      "step_no: 969, epoch: 3, batch_no: 240, loss: 0.7386165857315063, avg_loss: 0.7340477705001831\n",
      "step_no: 970, epoch: 3, batch_no: 241, loss: 0.5303410887718201, avg_loss: 0.7332059144973755\n",
      "step_no: 971, epoch: 3, batch_no: 242, loss: 0.5103805661201477, avg_loss: 0.7322888970375061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_no: 972, epoch: 4, batch_no: 0, loss: 0.6469544768333435, avg_loss: 0.6469544768333435\n",
      "step_no: 973, epoch: 4, batch_no: 1, loss: 0.548209011554718, avg_loss: 0.5975817441940308\n",
      "step_no: 974, epoch: 4, batch_no: 2, loss: 0.5684092044830322, avg_loss: 0.5878576040267944\n",
      "step_no: 975, epoch: 4, batch_no: 3, loss: 0.7276942729949951, avg_loss: 0.6228167414665222\n",
      "step_no: 976, epoch: 4, batch_no: 4, loss: 1.0956902503967285, avg_loss: 0.7173914313316345\n",
      "step_no: 977, epoch: 4, batch_no: 5, loss: 0.4638321101665497, avg_loss: 0.6751315593719482\n",
      "step_no: 978, epoch: 4, batch_no: 6, loss: 0.6775146126747131, avg_loss: 0.6754720211029053\n",
      "step_no: 979, epoch: 4, batch_no: 7, loss: 0.607276439666748, avg_loss: 0.6669475436210632\n",
      "step_no: 980, epoch: 4, batch_no: 8, loss: 1.0398412942886353, avg_loss: 0.7083801627159119\n",
      "step_no: 981, epoch: 4, batch_no: 9, loss: 1.0365979671478271, avg_loss: 0.7412019968032837\n",
      "step_no: 982, epoch: 4, batch_no: 10, loss: 0.5283460021018982, avg_loss: 0.7218514680862427\n",
      "step_no: 983, epoch: 4, batch_no: 11, loss: 0.6345211267471313, avg_loss: 0.7145739793777466\n",
      "step_no: 984, epoch: 4, batch_no: 12, loss: 0.5738530158996582, avg_loss: 0.7037492990493774\n",
      "step_no: 985, epoch: 4, batch_no: 13, loss: 0.8051239848136902, avg_loss: 0.7109903693199158\n",
      "step_no: 986, epoch: 4, batch_no: 14, loss: 0.8568984866142273, avg_loss: 0.7207176089286804\n",
      "step_no: 987, epoch: 4, batch_no: 15, loss: 0.8377991914749146, avg_loss: 0.7280351519584656\n",
      "step_no: 988, epoch: 4, batch_no: 16, loss: 0.9542780518531799, avg_loss: 0.7413435578346252\n",
      "step_no: 989, epoch: 4, batch_no: 17, loss: 0.7520285248756409, avg_loss: 0.7419371604919434\n",
      "step_no: 990, epoch: 4, batch_no: 18, loss: 0.945816695690155, avg_loss: 0.7526676654815674\n",
      "step_no: 991, epoch: 4, batch_no: 19, loss: 1.070115327835083, avg_loss: 0.76854008436203\n",
      "step_no: 992, epoch: 4, batch_no: 20, loss: 0.63987797498703, avg_loss: 0.7624133229255676\n",
      "step_no: 993, epoch: 4, batch_no: 21, loss: 0.7357932925224304, avg_loss: 0.7612033486366272\n",
      "step_no: 994, epoch: 4, batch_no: 22, loss: 0.3639755845069885, avg_loss: 0.7439325451850891\n",
      "step_no: 995, epoch: 4, batch_no: 23, loss: 0.585443377494812, avg_loss: 0.7373288869857788\n",
      "step_no: 996, epoch: 4, batch_no: 24, loss: 0.6596251726150513, avg_loss: 0.7342207431793213\n",
      "step_no: 997, epoch: 4, batch_no: 25, loss: 0.8227442502975464, avg_loss: 0.7376255393028259\n",
      "step_no: 998, epoch: 4, batch_no: 26, loss: 1.0131078958511353, avg_loss: 0.7478285431861877\n",
      "step_no: 999, epoch: 4, batch_no: 27, loss: 0.7074254751205444, avg_loss: 0.7463856339454651\n",
      "0.404446282203975\n",
      "step_no: 1000, epoch: 4, batch_no: 28, loss: 0.8735561370849609, avg_loss: 0.7507707476615906\n",
      "step_no: 1001, epoch: 4, batch_no: 29, loss: 0.8545644283294678, avg_loss: 0.7542306184768677\n",
      "step_no: 1002, epoch: 4, batch_no: 30, loss: 0.8148981928825378, avg_loss: 0.7561875581741333\n",
      "step_no: 1003, epoch: 4, batch_no: 31, loss: 0.36097946763038635, avg_loss: 0.743837296962738\n",
      "step_no: 1004, epoch: 4, batch_no: 32, loss: 1.101121425628662, avg_loss: 0.7546641230583191\n",
      "step_no: 1005, epoch: 4, batch_no: 33, loss: 0.6346603631973267, avg_loss: 0.7511345744132996\n",
      "step_no: 1006, epoch: 4, batch_no: 34, loss: 0.6713451147079468, avg_loss: 0.7488548755645752\n",
      "step_no: 1007, epoch: 4, batch_no: 35, loss: 0.7775154113769531, avg_loss: 0.7496510148048401\n",
      "step_no: 1008, epoch: 4, batch_no: 36, loss: 0.47840580344200134, avg_loss: 0.7423200607299805\n",
      "step_no: 1009, epoch: 4, batch_no: 37, loss: 0.9241205453872681, avg_loss: 0.7471042275428772\n",
      "step_no: 1010, epoch: 4, batch_no: 38, loss: 0.33143317699432373, avg_loss: 0.7364460229873657\n",
      "step_no: 1011, epoch: 4, batch_no: 39, loss: 0.7484360933303833, avg_loss: 0.7367457747459412\n",
      "step_no: 1012, epoch: 4, batch_no: 40, loss: 0.7130290269851685, avg_loss: 0.7361672520637512\n",
      "step_no: 1013, epoch: 4, batch_no: 41, loss: 0.5707818865776062, avg_loss: 0.7322295904159546\n",
      "step_no: 1014, epoch: 4, batch_no: 42, loss: 0.8166480660438538, avg_loss: 0.7341927886009216\n",
      "step_no: 1015, epoch: 4, batch_no: 43, loss: 0.9362887740135193, avg_loss: 0.7387858629226685\n",
      "step_no: 1016, epoch: 4, batch_no: 44, loss: 0.5894830822944641, avg_loss: 0.7354679703712463\n",
      "step_no: 1017, epoch: 4, batch_no: 45, loss: 0.9292421340942383, avg_loss: 0.7396804094314575\n",
      "step_no: 1018, epoch: 4, batch_no: 46, loss: 0.8922950625419617, avg_loss: 0.7429275512695312\n",
      "step_no: 1019, epoch: 4, batch_no: 47, loss: 0.6413840055465698, avg_loss: 0.7408120632171631\n",
      "step_no: 1020, epoch: 4, batch_no: 48, loss: 0.5120904445648193, avg_loss: 0.7361442446708679\n",
      "step_no: 1021, epoch: 4, batch_no: 49, loss: 0.5263547301292419, avg_loss: 0.7319484353065491\n",
      "step_no: 1022, epoch: 4, batch_no: 50, loss: 0.7746263742446899, avg_loss: 0.7327853441238403\n",
      "step_no: 1023, epoch: 4, batch_no: 51, loss: 0.895760178565979, avg_loss: 0.7359194755554199\n",
      "step_no: 1024, epoch: 4, batch_no: 52, loss: 0.772953987121582, avg_loss: 0.7366182208061218\n",
      "step_no: 1025, epoch: 4, batch_no: 53, loss: 0.5815194845199585, avg_loss: 0.7337459921836853\n",
      "step_no: 1026, epoch: 4, batch_no: 54, loss: 0.9212687015533447, avg_loss: 0.7371554970741272\n",
      "step_no: 1027, epoch: 4, batch_no: 55, loss: 0.44426438212394714, avg_loss: 0.7319253087043762\n",
      "step_no: 1028, epoch: 4, batch_no: 56, loss: 0.7872288227081299, avg_loss: 0.7328954935073853\n",
      "step_no: 1029, epoch: 4, batch_no: 57, loss: 0.843586802482605, avg_loss: 0.7348039746284485\n",
      "step_no: 1030, epoch: 4, batch_no: 58, loss: 0.6390886306762695, avg_loss: 0.7331816554069519\n",
      "step_no: 1031, epoch: 4, batch_no: 59, loss: 0.9217224717140198, avg_loss: 0.7363240122795105\n",
      "step_no: 1032, epoch: 4, batch_no: 60, loss: 0.8021836876869202, avg_loss: 0.7374036312103271\n",
      "step_no: 1033, epoch: 4, batch_no: 61, loss: 0.6589889526367188, avg_loss: 0.7361388802528381\n",
      "step_no: 1034, epoch: 4, batch_no: 62, loss: 0.8407919406890869, avg_loss: 0.7378001809120178\n",
      "step_no: 1035, epoch: 4, batch_no: 63, loss: 0.8544184565544128, avg_loss: 0.7396222949028015\n",
      "step_no: 1036, epoch: 4, batch_no: 64, loss: 0.8475428819656372, avg_loss: 0.74128258228302\n",
      "step_no: 1037, epoch: 4, batch_no: 65, loss: 0.6895511150360107, avg_loss: 0.7404988408088684\n",
      "step_no: 1038, epoch: 4, batch_no: 66, loss: 0.5808185338973999, avg_loss: 0.7381154894828796\n",
      "step_no: 1039, epoch: 4, batch_no: 67, loss: 0.6382927298545837, avg_loss: 0.7366475462913513\n",
      "step_no: 1040, epoch: 4, batch_no: 68, loss: 0.9516803026199341, avg_loss: 0.7397639751434326\n",
      "step_no: 1041, epoch: 4, batch_no: 69, loss: 0.8617286086082458, avg_loss: 0.7415062785148621\n",
      "step_no: 1042, epoch: 4, batch_no: 70, loss: 0.428913414478302, avg_loss: 0.7371035814285278\n",
      "step_no: 1043, epoch: 4, batch_no: 71, loss: 1.0106655359268188, avg_loss: 0.7409030795097351\n",
      "step_no: 1044, epoch: 4, batch_no: 72, loss: 0.7736039757728577, avg_loss: 0.7413510084152222\n",
      "step_no: 1045, epoch: 4, batch_no: 73, loss: 0.7328319549560547, avg_loss: 0.7412359118461609\n",
      "step_no: 1046, epoch: 4, batch_no: 74, loss: 0.5263503193855286, avg_loss: 0.7383707761764526\n",
      "step_no: 1047, epoch: 4, batch_no: 75, loss: 0.7105687260627747, avg_loss: 0.7380049228668213\n",
      "step_no: 1048, epoch: 4, batch_no: 76, loss: 0.6696761846542358, avg_loss: 0.73711758852005\n",
      "step_no: 1049, epoch: 4, batch_no: 77, loss: 0.8186426758766174, avg_loss: 0.7381627559661865\n",
      "step_no: 1050, epoch: 4, batch_no: 78, loss: 0.874275267124176, avg_loss: 0.739885687828064\n",
      "step_no: 1051, epoch: 4, batch_no: 79, loss: 0.7930080890655518, avg_loss: 0.7405497431755066\n",
      "step_no: 1052, epoch: 4, batch_no: 80, loss: 0.9300655126571655, avg_loss: 0.742889404296875\n",
      "step_no: 1053, epoch: 4, batch_no: 81, loss: 0.5328936576843262, avg_loss: 0.7403284311294556\n",
      "step_no: 1054, epoch: 4, batch_no: 82, loss: 0.4490061402320862, avg_loss: 0.7368185520172119\n",
      "step_no: 1055, epoch: 4, batch_no: 83, loss: 0.5635542273521423, avg_loss: 0.7347558736801147\n",
      "step_no: 1056, epoch: 4, batch_no: 84, loss: 0.5602015256881714, avg_loss: 0.7327023148536682\n",
      "step_no: 1057, epoch: 4, batch_no: 85, loss: 0.5422031283378601, avg_loss: 0.7304871678352356\n",
      "step_no: 1058, epoch: 4, batch_no: 86, loss: 0.7471762299537659, avg_loss: 0.7306789755821228\n",
      "step_no: 1059, epoch: 4, batch_no: 87, loss: 0.9848297834396362, avg_loss: 0.7335670590400696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_no: 1060, epoch: 4, batch_no: 88, loss: 1.0750116109848022, avg_loss: 0.7374035120010376\n",
      "step_no: 1061, epoch: 4, batch_no: 89, loss: 0.8467787504196167, avg_loss: 0.7386188507080078\n",
      "step_no: 1062, epoch: 4, batch_no: 90, loss: 0.5917951464653015, avg_loss: 0.7370054125785828\n",
      "step_no: 1063, epoch: 4, batch_no: 91, loss: 0.5788564085960388, avg_loss: 0.7352864146232605\n",
      "step_no: 1064, epoch: 4, batch_no: 92, loss: 0.6679075956344604, avg_loss: 0.7345618605613708\n",
      "step_no: 1065, epoch: 4, batch_no: 93, loss: 1.1160869598388672, avg_loss: 0.7386206388473511\n",
      "step_no: 1066, epoch: 4, batch_no: 94, loss: 0.5661671161651611, avg_loss: 0.7368054389953613\n",
      "step_no: 1067, epoch: 4, batch_no: 95, loss: 0.48950129747390747, avg_loss: 0.734229326248169\n",
      "step_no: 1068, epoch: 4, batch_no: 96, loss: 0.5294654965400696, avg_loss: 0.7321183085441589\n",
      "step_no: 1069, epoch: 4, batch_no: 97, loss: 0.5469132661819458, avg_loss: 0.7302284836769104\n",
      "step_no: 1070, epoch: 4, batch_no: 98, loss: 0.46937471628189087, avg_loss: 0.7275936007499695\n",
      "step_no: 1071, epoch: 4, batch_no: 99, loss: 1.1530534029006958, avg_loss: 0.7318481802940369\n",
      "step_no: 1072, epoch: 4, batch_no: 100, loss: 0.6579676270484924, avg_loss: 0.7311167120933533\n",
      "step_no: 1073, epoch: 4, batch_no: 101, loss: 0.5781789422035217, avg_loss: 0.7296173572540283\n",
      "step_no: 1074, epoch: 4, batch_no: 102, loss: 0.5429980158805847, avg_loss: 0.727805495262146\n",
      "step_no: 1075, epoch: 4, batch_no: 103, loss: 0.9934313893318176, avg_loss: 0.7303596138954163\n",
      "step_no: 1076, epoch: 4, batch_no: 104, loss: 0.7878355383872986, avg_loss: 0.7309070229530334\n",
      "step_no: 1077, epoch: 4, batch_no: 105, loss: 0.6894136071205139, avg_loss: 0.7305155396461487\n",
      "step_no: 1078, epoch: 4, batch_no: 106, loss: 0.823542058467865, avg_loss: 0.7313848733901978\n",
      "step_no: 1079, epoch: 4, batch_no: 107, loss: 1.2447643280029297, avg_loss: 0.7361384630203247\n",
      "step_no: 1080, epoch: 4, batch_no: 108, loss: 0.6559855341911316, avg_loss: 0.7354030609130859\n",
      "step_no: 1081, epoch: 4, batch_no: 109, loss: 0.789081335067749, avg_loss: 0.7358909845352173\n",
      "step_no: 1082, epoch: 4, batch_no: 110, loss: 0.7611507773399353, avg_loss: 0.7361186146736145\n",
      "step_no: 1083, epoch: 4, batch_no: 111, loss: 0.6586856842041016, avg_loss: 0.7354272603988647\n",
      "step_no: 1084, epoch: 4, batch_no: 112, loss: 0.9564715623855591, avg_loss: 0.7373834252357483\n",
      "step_no: 1085, epoch: 4, batch_no: 113, loss: 0.6912254691123962, avg_loss: 0.7369785308837891\n",
      "step_no: 1086, epoch: 4, batch_no: 114, loss: 0.7443192005157471, avg_loss: 0.737042248249054\n",
      "step_no: 1087, epoch: 4, batch_no: 115, loss: 0.5872400403022766, avg_loss: 0.7357509136199951\n",
      "step_no: 1088, epoch: 4, batch_no: 116, loss: 0.501482367515564, avg_loss: 0.7337486147880554\n",
      "step_no: 1089, epoch: 4, batch_no: 117, loss: 0.640738844871521, avg_loss: 0.7329604029655457\n",
      "step_no: 1090, epoch: 4, batch_no: 118, loss: 0.7233627438545227, avg_loss: 0.7328798174858093\n",
      "step_no: 1091, epoch: 4, batch_no: 119, loss: 0.7395980954170227, avg_loss: 0.7329358458518982\n",
      "step_no: 1092, epoch: 4, batch_no: 120, loss: 0.7527973651885986, avg_loss: 0.7330999374389648\n",
      "step_no: 1093, epoch: 4, batch_no: 121, loss: 0.9255892634391785, avg_loss: 0.7346776723861694\n",
      "step_no: 1094, epoch: 4, batch_no: 122, loss: 0.8914743065834045, avg_loss: 0.7359524369239807\n",
      "step_no: 1095, epoch: 4, batch_no: 123, loss: 0.7039154767990112, avg_loss: 0.7356941103935242\n",
      "step_no: 1096, epoch: 4, batch_no: 124, loss: 0.9355379939079285, avg_loss: 0.7372929453849792\n",
      "step_no: 1097, epoch: 4, batch_no: 125, loss: 1.004805326461792, avg_loss: 0.7394160628318787\n",
      "step_no: 1098, epoch: 4, batch_no: 126, loss: 0.4432693421840668, avg_loss: 0.737084150314331\n",
      "step_no: 1099, epoch: 4, batch_no: 127, loss: 0.80998694896698, avg_loss: 0.7376537322998047\n",
      "0.334373899529939\n",
      "step_no: 1100, epoch: 4, batch_no: 128, loss: 0.8438130021095276, avg_loss: 0.7384766340255737\n",
      "step_no: 1101, epoch: 4, batch_no: 129, loss: 0.5220476388931274, avg_loss: 0.7368118166923523\n",
      "step_no: 1102, epoch: 4, batch_no: 130, loss: 0.4987353980541229, avg_loss: 0.7349944114685059\n",
      "step_no: 1103, epoch: 4, batch_no: 131, loss: 0.44338715076446533, avg_loss: 0.7327853441238403\n",
      "step_no: 1104, epoch: 4, batch_no: 132, loss: 1.2763994932174683, avg_loss: 0.7368726134300232\n",
      "step_no: 1105, epoch: 4, batch_no: 133, loss: 0.6068311929702759, avg_loss: 0.7359021902084351\n",
      "step_no: 1106, epoch: 4, batch_no: 134, loss: 0.7542299032211304, avg_loss: 0.7360379099845886\n",
      "step_no: 1107, epoch: 4, batch_no: 135, loss: 0.7753252387046814, avg_loss: 0.7363267540931702\n",
      "step_no: 1108, epoch: 4, batch_no: 136, loss: 0.5300338864326477, avg_loss: 0.7348210215568542\n",
      "step_no: 1109, epoch: 4, batch_no: 137, loss: 0.4871353805065155, avg_loss: 0.7330262064933777\n",
      "step_no: 1110, epoch: 4, batch_no: 138, loss: 0.7384903430938721, avg_loss: 0.7330654859542847\n",
      "step_no: 1111, epoch: 4, batch_no: 139, loss: 0.7322537899017334, avg_loss: 0.7330597043037415\n",
      "step_no: 1112, epoch: 4, batch_no: 140, loss: 1.0958290100097656, avg_loss: 0.7356324791908264\n",
      "step_no: 1113, epoch: 4, batch_no: 141, loss: 0.5305917859077454, avg_loss: 0.7341885566711426\n",
      "step_no: 1114, epoch: 4, batch_no: 142, loss: 0.7821194529533386, avg_loss: 0.7345237135887146\n",
      "step_no: 1115, epoch: 4, batch_no: 143, loss: 0.7294396162033081, avg_loss: 0.7344884276390076\n",
      "step_no: 1116, epoch: 4, batch_no: 144, loss: 0.7323547005653381, avg_loss: 0.734473705291748\n",
      "step_no: 1117, epoch: 4, batch_no: 145, loss: 0.9099953770637512, avg_loss: 0.7356759309768677\n",
      "step_no: 1118, epoch: 4, batch_no: 146, loss: 0.6902543306350708, avg_loss: 0.7353668808937073\n",
      "step_no: 1119, epoch: 4, batch_no: 147, loss: 0.6023361682891846, avg_loss: 0.7344680428504944\n",
      "step_no: 1120, epoch: 4, batch_no: 148, loss: 1.0031801462173462, avg_loss: 0.7362715005874634\n",
      "step_no: 1121, epoch: 4, batch_no: 149, loss: 0.551106870174408, avg_loss: 0.735037088394165\n",
      "step_no: 1122, epoch: 4, batch_no: 150, loss: 0.2850847840309143, avg_loss: 0.7320572733879089\n",
      "step_no: 1123, epoch: 4, batch_no: 151, loss: 0.8476555943489075, avg_loss: 0.7328178286552429\n",
      "step_no: 1124, epoch: 4, batch_no: 152, loss: 0.34170764684677124, avg_loss: 0.730261504650116\n",
      "step_no: 1125, epoch: 4, batch_no: 153, loss: 0.6275522708892822, avg_loss: 0.7295945882797241\n",
      "step_no: 1126, epoch: 4, batch_no: 154, loss: 0.9070207476615906, avg_loss: 0.7307392358779907\n",
      "step_no: 1127, epoch: 4, batch_no: 155, loss: 0.6824455261230469, avg_loss: 0.7304297089576721\n",
      "step_no: 1128, epoch: 4, batch_no: 156, loss: 0.2338237464427948, avg_loss: 0.7272666692733765\n",
      "step_no: 1129, epoch: 4, batch_no: 157, loss: 0.5755484700202942, avg_loss: 0.7263063788414001\n",
      "step_no: 1130, epoch: 4, batch_no: 158, loss: 0.5303829908370972, avg_loss: 0.7250741124153137\n",
      "step_no: 1131, epoch: 4, batch_no: 159, loss: 0.9932609796524048, avg_loss: 0.7267503142356873\n",
      "step_no: 1132, epoch: 4, batch_no: 160, loss: 0.644887387752533, avg_loss: 0.7262418866157532\n",
      "step_no: 1133, epoch: 4, batch_no: 161, loss: 0.5983346700668335, avg_loss: 0.7254523634910583\n",
      "step_no: 1134, epoch: 4, batch_no: 162, loss: 0.6824914216995239, avg_loss: 0.7251887917518616\n",
      "step_no: 1135, epoch: 4, batch_no: 163, loss: 0.8452016115188599, avg_loss: 0.725920557975769\n",
      "step_no: 1136, epoch: 4, batch_no: 164, loss: 0.9018075466156006, avg_loss: 0.7269865274429321\n",
      "step_no: 1137, epoch: 4, batch_no: 165, loss: 0.880425214767456, avg_loss: 0.7279108762741089\n",
      "step_no: 1138, epoch: 4, batch_no: 166, loss: 0.8261047601699829, avg_loss: 0.7284988760948181\n",
      "step_no: 1139, epoch: 4, batch_no: 167, loss: 0.6789187788963318, avg_loss: 0.7282037138938904\n",
      "step_no: 1140, epoch: 4, batch_no: 168, loss: 0.45877835154533386, avg_loss: 0.7266095280647278\n",
      "step_no: 1141, epoch: 4, batch_no: 169, loss: 0.9009835720062256, avg_loss: 0.7276352643966675\n",
      "step_no: 1142, epoch: 4, batch_no: 170, loss: 0.8333428502082825, avg_loss: 0.7282534241676331\n",
      "step_no: 1143, epoch: 4, batch_no: 171, loss: 0.3981281816959381, avg_loss: 0.7263340353965759\n",
      "step_no: 1144, epoch: 4, batch_no: 172, loss: 0.8080427050590515, avg_loss: 0.7268063426017761\n",
      "step_no: 1145, epoch: 4, batch_no: 173, loss: 0.649777889251709, avg_loss: 0.7263637185096741\n",
      "step_no: 1146, epoch: 4, batch_no: 174, loss: 0.8119477033615112, avg_loss: 0.7268527746200562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_no: 1147, epoch: 4, batch_no: 175, loss: 0.7188620567321777, avg_loss: 0.7268074154853821\n",
      "step_no: 1148, epoch: 4, batch_no: 176, loss: 0.8472546935081482, avg_loss: 0.7274878621101379\n",
      "step_no: 1149, epoch: 4, batch_no: 177, loss: 0.7318625450134277, avg_loss: 0.7275124192237854\n",
      "step_no: 1150, epoch: 4, batch_no: 178, loss: 0.7517955303192139, avg_loss: 0.7276480793952942\n",
      "step_no: 1151, epoch: 4, batch_no: 179, loss: 0.7554243206977844, avg_loss: 0.7278023958206177\n",
      "step_no: 1152, epoch: 4, batch_no: 180, loss: 0.9979268908500671, avg_loss: 0.7292947769165039\n",
      "step_no: 1153, epoch: 4, batch_no: 181, loss: 0.8716061115264893, avg_loss: 0.7300767302513123\n",
      "step_no: 1154, epoch: 4, batch_no: 182, loss: 0.5345759391784668, avg_loss: 0.7290083765983582\n",
      "step_no: 1155, epoch: 4, batch_no: 183, loss: 0.6644953489303589, avg_loss: 0.7286577820777893\n",
      "step_no: 1156, epoch: 4, batch_no: 184, loss: 0.43473389744758606, avg_loss: 0.7270690202713013\n",
      "step_no: 1157, epoch: 4, batch_no: 185, loss: 0.6979625225067139, avg_loss: 0.7269125580787659\n",
      "step_no: 1158, epoch: 4, batch_no: 186, loss: 1.142112135887146, avg_loss: 0.7291328310966492\n",
      "step_no: 1159, epoch: 4, batch_no: 187, loss: 0.24632777273654938, avg_loss: 0.7265646457672119\n",
      "step_no: 1160, epoch: 4, batch_no: 188, loss: 0.5396671295166016, avg_loss: 0.7255758047103882\n",
      "step_no: 1161, epoch: 4, batch_no: 189, loss: 0.8332170248031616, avg_loss: 0.726142406463623\n",
      "step_no: 1162, epoch: 4, batch_no: 190, loss: 0.44397443532943726, avg_loss: 0.7246650457382202\n",
      "step_no: 1163, epoch: 4, batch_no: 191, loss: 0.8270165920257568, avg_loss: 0.7251981496810913\n",
      "step_no: 1164, epoch: 4, batch_no: 192, loss: 0.8327596783638, avg_loss: 0.7257554531097412\n",
      "step_no: 1165, epoch: 4, batch_no: 193, loss: 0.755358874797821, avg_loss: 0.7259079813957214\n",
      "step_no: 1166, epoch: 4, batch_no: 194, loss: 0.7314855456352234, avg_loss: 0.7259366512298584\n",
      "step_no: 1167, epoch: 4, batch_no: 195, loss: 0.7501194477081299, avg_loss: 0.7260600328445435\n",
      "step_no: 1168, epoch: 4, batch_no: 196, loss: 0.4909403622150421, avg_loss: 0.724866509437561\n",
      "step_no: 1169, epoch: 4, batch_no: 197, loss: 0.6884477138519287, avg_loss: 0.7246825695037842\n",
      "step_no: 1170, epoch: 4, batch_no: 198, loss: 0.8594436645507812, avg_loss: 0.7253597378730774\n",
      "step_no: 1171, epoch: 4, batch_no: 199, loss: 0.6037099957466125, avg_loss: 0.7247514724731445\n",
      "step_no: 1172, epoch: 4, batch_no: 200, loss: 0.26565489172935486, avg_loss: 0.7224674224853516\n",
      "step_no: 1173, epoch: 4, batch_no: 201, loss: 0.8887965083122253, avg_loss: 0.7232908606529236\n",
      "step_no: 1174, epoch: 4, batch_no: 202, loss: 0.7701913118362427, avg_loss: 0.7235218286514282\n",
      "step_no: 1175, epoch: 4, batch_no: 203, loss: 0.8049724102020264, avg_loss: 0.7239211797714233\n",
      "step_no: 1176, epoch: 4, batch_no: 204, loss: 0.5676335096359253, avg_loss: 0.7231587767601013\n",
      "step_no: 1177, epoch: 4, batch_no: 205, loss: 0.6349748373031616, avg_loss: 0.7227306962013245\n",
      "step_no: 1178, epoch: 4, batch_no: 206, loss: 0.5566534996032715, avg_loss: 0.7219284176826477\n",
      "step_no: 1179, epoch: 4, batch_no: 207, loss: 0.7953501343727112, avg_loss: 0.7222813963890076\n",
      "step_no: 1180, epoch: 4, batch_no: 208, loss: 0.6170831322669983, avg_loss: 0.7217779755592346\n",
      "step_no: 1181, epoch: 4, batch_no: 209, loss: 0.9966889023780823, avg_loss: 0.7230871319770813\n",
      "step_no: 1182, epoch: 4, batch_no: 210, loss: 0.8310614824295044, avg_loss: 0.723598837852478\n",
      "step_no: 1183, epoch: 4, batch_no: 211, loss: 0.5532073378562927, avg_loss: 0.7227951288223267\n",
      "step_no: 1184, epoch: 4, batch_no: 212, loss: 0.7574419975280762, avg_loss: 0.7229577898979187\n",
      "step_no: 1185, epoch: 4, batch_no: 213, loss: 1.0599946975708008, avg_loss: 0.7245327234268188\n",
      "step_no: 1186, epoch: 4, batch_no: 214, loss: 0.5628101229667664, avg_loss: 0.7237805128097534\n",
      "step_no: 1187, epoch: 4, batch_no: 215, loss: 0.7896954417228699, avg_loss: 0.7240856289863586\n",
      "step_no: 1188, epoch: 4, batch_no: 216, loss: 0.7632148861885071, avg_loss: 0.7242659330368042\n",
      "step_no: 1189, epoch: 4, batch_no: 217, loss: 0.6524661183357239, avg_loss: 0.7239365577697754\n",
      "step_no: 1190, epoch: 4, batch_no: 218, loss: 0.6073715090751648, avg_loss: 0.7234043478965759\n",
      "step_no: 1191, epoch: 4, batch_no: 219, loss: 1.0578898191452026, avg_loss: 0.7249247431755066\n",
      "step_no: 1192, epoch: 4, batch_no: 220, loss: 1.1400868892669678, avg_loss: 0.7268033623695374\n",
      "step_no: 1193, epoch: 4, batch_no: 221, loss: 0.7631673216819763, avg_loss: 0.7269671559333801\n",
      "step_no: 1194, epoch: 4, batch_no: 222, loss: 0.6970242857933044, avg_loss: 0.7268328666687012\n",
      "step_no: 1195, epoch: 4, batch_no: 223, loss: 0.7118305563926697, avg_loss: 0.7267659306526184\n",
      "step_no: 1196, epoch: 4, batch_no: 224, loss: 0.7302567362785339, avg_loss: 0.72678142786026\n",
      "step_no: 1197, epoch: 4, batch_no: 225, loss: 0.6116378307342529, avg_loss: 0.72627192735672\n",
      "step_no: 1198, epoch: 4, batch_no: 226, loss: 0.5034312605857849, avg_loss: 0.7252902388572693\n",
      "step_no: 1199, epoch: 4, batch_no: 227, loss: 0.5919560790061951, avg_loss: 0.7247053980827332\n",
      "0.35798484091522403\n",
      "step_no: 1200, epoch: 4, batch_no: 228, loss: 0.8078587651252747, avg_loss: 0.7250685691833496\n",
      "step_no: 1201, epoch: 4, batch_no: 229, loss: 0.6188857555389404, avg_loss: 0.7246068120002747\n",
      "step_no: 1202, epoch: 4, batch_no: 230, loss: 0.7250931262969971, avg_loss: 0.7246089577674866\n",
      "step_no: 1203, epoch: 4, batch_no: 231, loss: 0.5881885290145874, avg_loss: 0.7240210175514221\n",
      "step_no: 1204, epoch: 4, batch_no: 232, loss: 0.8373498916625977, avg_loss: 0.7245073914527893\n",
      "step_no: 1205, epoch: 4, batch_no: 233, loss: 0.639668345451355, avg_loss: 0.7241448760032654\n",
      "step_no: 1206, epoch: 4, batch_no: 234, loss: 0.5119393467903137, avg_loss: 0.7232417464256287\n",
      "step_no: 1207, epoch: 4, batch_no: 235, loss: 0.5682224631309509, avg_loss: 0.7225849628448486\n",
      "step_no: 1208, epoch: 4, batch_no: 236, loss: 0.5576689839363098, avg_loss: 0.7218890190124512\n",
      "step_no: 1209, epoch: 4, batch_no: 237, loss: 0.4513460397720337, avg_loss: 0.7207523584365845\n",
      "step_no: 1210, epoch: 4, batch_no: 238, loss: 0.7205421924591064, avg_loss: 0.7207513451576233\n",
      "step_no: 1211, epoch: 4, batch_no: 239, loss: 0.7546985745429993, avg_loss: 0.7208929061889648\n",
      "step_no: 1212, epoch: 4, batch_no: 240, loss: 0.5165811777114868, avg_loss: 0.7200451493263245\n",
      "step_no: 1213, epoch: 4, batch_no: 241, loss: 0.40764087438583374, avg_loss: 0.7187541127204895\n",
      "step_no: 1214, epoch: 4, batch_no: 242, loss: 0.5472119450569153, avg_loss: 0.7180482149124146\n",
      "step_no: 1215, epoch: 5, batch_no: 0, loss: 0.5298526287078857, avg_loss: 0.5298526287078857\n",
      "step_no: 1216, epoch: 5, batch_no: 1, loss: 0.4526839256286621, avg_loss: 0.4912682771682739\n",
      "step_no: 1217, epoch: 5, batch_no: 2, loss: 0.5903940200805664, avg_loss: 0.5243102312088013\n",
      "step_no: 1218, epoch: 5, batch_no: 3, loss: 0.6747345328330994, avg_loss: 0.5619162917137146\n",
      "step_no: 1219, epoch: 5, batch_no: 4, loss: 0.8056520223617554, avg_loss: 0.6106634140014648\n",
      "step_no: 1220, epoch: 5, batch_no: 5, loss: 0.9228799343109131, avg_loss: 0.6626995205879211\n",
      "step_no: 1221, epoch: 5, batch_no: 6, loss: 0.4702075123786926, avg_loss: 0.6352006793022156\n",
      "step_no: 1222, epoch: 5, batch_no: 7, loss: 0.9654041528701782, avg_loss: 0.6764760613441467\n",
      "step_no: 1223, epoch: 5, batch_no: 8, loss: 1.0317208766937256, avg_loss: 0.715947687625885\n",
      "step_no: 1224, epoch: 5, batch_no: 9, loss: 0.7474613189697266, avg_loss: 0.7190990447998047\n",
      "step_no: 1225, epoch: 5, batch_no: 10, loss: 0.8406665921211243, avg_loss: 0.7301506996154785\n",
      "step_no: 1226, epoch: 5, batch_no: 11, loss: 0.5876739025115967, avg_loss: 0.718277633190155\n",
      "step_no: 1227, epoch: 5, batch_no: 12, loss: 0.6050613522529602, avg_loss: 0.7095687389373779\n",
      "step_no: 1228, epoch: 5, batch_no: 13, loss: 0.41168174147605896, avg_loss: 0.6882911324501038\n",
      "step_no: 1229, epoch: 5, batch_no: 14, loss: 0.7542648315429688, avg_loss: 0.6926893591880798\n",
      "step_no: 1230, epoch: 5, batch_no: 15, loss: 0.7104953527450562, avg_loss: 0.6938021779060364\n",
      "step_no: 1231, epoch: 5, batch_no: 16, loss: 0.7644608020782471, avg_loss: 0.6979585289955139\n",
      "step_no: 1232, epoch: 5, batch_no: 17, loss: 0.6189775466918945, avg_loss: 0.6935707330703735\n",
      "step_no: 1233, epoch: 5, batch_no: 18, loss: 0.733741819858551, avg_loss: 0.6956849694252014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_no: 1234, epoch: 5, batch_no: 19, loss: 0.7980104088783264, avg_loss: 0.7008013129234314\n",
      "step_no: 1235, epoch: 5, batch_no: 20, loss: 0.4737129509449005, avg_loss: 0.6899875402450562\n",
      "step_no: 1236, epoch: 5, batch_no: 21, loss: 0.8897343873977661, avg_loss: 0.6990669369697571\n",
      "step_no: 1237, epoch: 5, batch_no: 22, loss: 0.8033981323242188, avg_loss: 0.7036030888557434\n",
      "step_no: 1238, epoch: 5, batch_no: 23, loss: 1.037950873374939, avg_loss: 0.7175342440605164\n",
      "step_no: 1239, epoch: 5, batch_no: 24, loss: 0.8391400575637817, avg_loss: 0.7223984599113464\n",
      "step_no: 1240, epoch: 5, batch_no: 25, loss: 0.5159204006195068, avg_loss: 0.7144570350646973\n",
      "step_no: 1241, epoch: 5, batch_no: 26, loss: 0.78443443775177, avg_loss: 0.7170487642288208\n",
      "step_no: 1242, epoch: 5, batch_no: 27, loss: 0.5769082903862, avg_loss: 0.712043821811676\n",
      "step_no: 1243, epoch: 5, batch_no: 28, loss: 0.8654413819313049, avg_loss: 0.7173333764076233\n",
      "step_no: 1244, epoch: 5, batch_no: 29, loss: 0.6895055770874023, avg_loss: 0.7164057493209839\n",
      "step_no: 1245, epoch: 5, batch_no: 30, loss: 0.6807025074958801, avg_loss: 0.715254008769989\n",
      "step_no: 1246, epoch: 5, batch_no: 31, loss: 0.6632950901985168, avg_loss: 0.7136303186416626\n",
      "step_no: 1247, epoch: 5, batch_no: 32, loss: 0.8713526725769043, avg_loss: 0.7184098362922668\n",
      "step_no: 1248, epoch: 5, batch_no: 33, loss: 1.0473949909210205, avg_loss: 0.7280858755111694\n",
      "step_no: 1249, epoch: 5, batch_no: 34, loss: 0.6268078684806824, avg_loss: 0.7251921892166138\n",
      "step_no: 1250, epoch: 5, batch_no: 35, loss: 0.6763291954994202, avg_loss: 0.7238348722457886\n",
      "step_no: 1251, epoch: 5, batch_no: 36, loss: 0.7833906412124634, avg_loss: 0.725444495677948\n",
      "step_no: 1252, epoch: 5, batch_no: 37, loss: 0.7033639550209045, avg_loss: 0.7248634099960327\n",
      "step_no: 1253, epoch: 5, batch_no: 38, loss: 0.5202822089195251, avg_loss: 0.7196177840232849\n",
      "step_no: 1254, epoch: 5, batch_no: 39, loss: 0.667150616645813, avg_loss: 0.7183060646057129\n",
      "step_no: 1255, epoch: 5, batch_no: 40, loss: 0.913745105266571, avg_loss: 0.72307288646698\n",
      "step_no: 1256, epoch: 5, batch_no: 41, loss: 0.6422163844108582, avg_loss: 0.7211477160453796\n",
      "step_no: 1257, epoch: 5, batch_no: 42, loss: 0.7678341269493103, avg_loss: 0.7222334146499634\n",
      "step_no: 1258, epoch: 5, batch_no: 43, loss: 0.7463180422782898, avg_loss: 0.7227808833122253\n",
      "step_no: 1259, epoch: 5, batch_no: 44, loss: 0.49316293001174927, avg_loss: 0.7176782488822937\n",
      "step_no: 1260, epoch: 5, batch_no: 45, loss: 0.649280309677124, avg_loss: 0.7161913514137268\n",
      "step_no: 1261, epoch: 5, batch_no: 46, loss: 1.0230458974838257, avg_loss: 0.7227200865745544\n",
      "step_no: 1262, epoch: 5, batch_no: 47, loss: 0.7161120176315308, avg_loss: 0.7225824594497681\n",
      "step_no: 1263, epoch: 5, batch_no: 48, loss: 0.3900184631347656, avg_loss: 0.7157953977584839\n",
      "step_no: 1264, epoch: 5, batch_no: 49, loss: 0.9870601892471313, avg_loss: 0.7212206721305847\n",
      "step_no: 1265, epoch: 5, batch_no: 50, loss: 0.7210988402366638, avg_loss: 0.7212183475494385\n",
      "step_no: 1266, epoch: 5, batch_no: 51, loss: 0.9716033935546875, avg_loss: 0.7260334491729736\n",
      "step_no: 1267, epoch: 5, batch_no: 52, loss: 0.7743781805038452, avg_loss: 0.7269456386566162\n",
      "step_no: 1268, epoch: 5, batch_no: 53, loss: 0.7549784779548645, avg_loss: 0.7274647355079651\n",
      "step_no: 1269, epoch: 5, batch_no: 54, loss: 1.0620977878570557, avg_loss: 0.7335489988327026\n",
      "step_no: 1270, epoch: 5, batch_no: 55, loss: 0.6729097366333008, avg_loss: 0.7324661612510681\n",
      "step_no: 1271, epoch: 5, batch_no: 56, loss: 0.5809158682823181, avg_loss: 0.729807436466217\n",
      "step_no: 1272, epoch: 5, batch_no: 57, loss: 0.5659494996070862, avg_loss: 0.7269822359085083\n",
      "step_no: 1273, epoch: 5, batch_no: 58, loss: 0.5537813305854797, avg_loss: 0.7240465879440308\n",
      "step_no: 1274, epoch: 5, batch_no: 59, loss: 0.9152927398681641, avg_loss: 0.7272340655326843\n",
      "step_no: 1275, epoch: 5, batch_no: 60, loss: 0.6036061644554138, avg_loss: 0.7252073287963867\n",
      "step_no: 1276, epoch: 5, batch_no: 61, loss: 0.7483910322189331, avg_loss: 0.7255812287330627\n",
      "step_no: 1277, epoch: 5, batch_no: 62, loss: 1.1220476627349854, avg_loss: 0.731874406337738\n",
      "step_no: 1278, epoch: 5, batch_no: 63, loss: 0.6981472373008728, avg_loss: 0.731347382068634\n",
      "step_no: 1279, epoch: 5, batch_no: 64, loss: 0.785108208656311, avg_loss: 0.7321744561195374\n",
      "step_no: 1280, epoch: 5, batch_no: 65, loss: 0.7224552035331726, avg_loss: 0.7320271730422974\n",
      "step_no: 1281, epoch: 5, batch_no: 66, loss: 0.8043789863586426, avg_loss: 0.7331070303916931\n",
      "step_no: 1282, epoch: 5, batch_no: 67, loss: 1.299004077911377, avg_loss: 0.7414290308952332\n",
      "step_no: 1283, epoch: 5, batch_no: 68, loss: 0.4346855580806732, avg_loss: 0.7369834780693054\n",
      "step_no: 1284, epoch: 5, batch_no: 69, loss: 0.4783773720264435, avg_loss: 0.7332891225814819\n",
      "step_no: 1285, epoch: 5, batch_no: 70, loss: 0.7050336599349976, avg_loss: 0.7328911423683167\n",
      "step_no: 1286, epoch: 5, batch_no: 71, loss: 0.3269788324832916, avg_loss: 0.7272534966468811\n",
      "step_no: 1287, epoch: 5, batch_no: 72, loss: 0.5608065724372864, avg_loss: 0.7249733805656433\n",
      "step_no: 1288, epoch: 5, batch_no: 73, loss: 0.7332901954650879, avg_loss: 0.7250857949256897\n",
      "step_no: 1289, epoch: 5, batch_no: 74, loss: 0.5774449110031128, avg_loss: 0.7231172919273376\n",
      "step_no: 1290, epoch: 5, batch_no: 75, loss: 0.7860644459724426, avg_loss: 0.7239454984664917\n",
      "step_no: 1291, epoch: 5, batch_no: 76, loss: 0.39307892322540283, avg_loss: 0.719648540019989\n",
      "step_no: 1292, epoch: 5, batch_no: 77, loss: 0.648065984249115, avg_loss: 0.7187308669090271\n",
      "step_no: 1293, epoch: 5, batch_no: 78, loss: 0.8026924729347229, avg_loss: 0.7197936773300171\n",
      "step_no: 1294, epoch: 5, batch_no: 79, loss: 0.9494494199752808, avg_loss: 0.7226642966270447\n",
      "step_no: 1295, epoch: 5, batch_no: 80, loss: 0.6340039372444153, avg_loss: 0.7215697169303894\n",
      "step_no: 1296, epoch: 5, batch_no: 81, loss: 0.942490816116333, avg_loss: 0.7242638468742371\n",
      "step_no: 1297, epoch: 5, batch_no: 82, loss: 0.7830867171287537, avg_loss: 0.7249725461006165\n",
      "step_no: 1298, epoch: 5, batch_no: 83, loss: 1.0088136196136475, avg_loss: 0.7283515930175781\n",
      "step_no: 1299, epoch: 5, batch_no: 84, loss: 0.5239824056625366, avg_loss: 0.7259472608566284\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-458e11723266>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mf1s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mval_batch_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                     \u001b[0mraw_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatent_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                     \u001b[0;31m#print(f\"raw_predictions: {raw_predictions.shape}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phenetics2/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phenetics2/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phenetics2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phenetics2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-36d18dce0046>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mabstract\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstracts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LEN_ABSTRACT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mclaims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclaims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LEN_CLAIMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-9675e5bf65d4>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(tokenizer, text, max_len)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         inputs = tokenizer.encode_plus(\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phenetics2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2285\u001b[0m         )\n\u001b[1;32m   2286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2287\u001b[0;31m         return self._encode_plus(\n\u001b[0m\u001b[1;32m   2288\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2289\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phenetics2/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mis_split_into_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"is_pretokenized\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phenetics2/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phenetics2/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mno_split_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_on_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_split_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phenetics2/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36msplit_on_tokens\u001b[0;34m(tok_list, text)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mtext_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             return list(\n\u001b[0m\u001b[1;32m    343\u001b[0m                 itertools.chain.from_iterable(\n\u001b[1;32m    344\u001b[0m                     (\n",
      "\u001b[0;32m~/anaconda3/envs/phenetics2/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    343\u001b[0m                 itertools.chain.from_iterable(\n\u001b[1;32m    344\u001b[0m                     (\n\u001b[0;32m--> 345\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                     )\n",
      "\u001b[0;32m~/anaconda3/envs/phenetics2/lib/python3.8/site-packages/transformers/tokenization_bert.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;31m# If the token is part of the never_split set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phenetics2/lib/python3.8/site-packages/transformers/tokenization_bert.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# union() returns a new set by concatenating the two sets.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mnever_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnever_split\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phenetics2/lib/python3.8/site-packages/transformers/tokenization_bert.py\u001b[0m in \u001b[0;36m_clean_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0xFFFD\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_control\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_is_whitespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phenetics2/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_is_control\u001b[0;34m(char)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\\r\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
    "                    float( len(set_true.union(set_pred)) )\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)\n",
    "\n",
    "optimizer = torch.optim.AdamW(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "#loss_function = torch.nn.CosineEmbeddingLoss()\n",
    "\n",
    "miner = miners.MultiSimilarityMiner()\n",
    "loss_function = losses.TripletMarginLoss()\n",
    "classifier_function = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir=logdir, flush_secs=30)\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "step = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0\n",
    "    for batch_no, batch in enumerate(training_loader):\n",
    "        \n",
    "        abstract = batch['abstract']#.to(device)\n",
    "        claims = batch['claims']#.to(device)\n",
    "        people = batch['people']#.to(device)\n",
    "        citing = batch['citing']#.to(device)\n",
    "        embedded_cpc = batch['embedded_cpc']#.to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        patent_emb = model.encode_patent(abstract=abstract, claims=claims, embedded_cpc=embedded_cpc, people=people, citing=citing)\n",
    "        label_embeddings = model.encode_label(subset_tokenized)\n",
    "        \n",
    "        #hard_pairs = miner(patent_emb, labels)\n",
    "        #loss = loss_function(patent_emb, labels, hard_pairs)\n",
    "    \n",
    "        loss = 0\n",
    "        for label_idx in range(NUM_LABELS):\n",
    "            #label_emb = label_embeddings[label_idx].repeat(4,1)\n",
    "            current_labels = labels[:, label_idx]\n",
    "            hard_pairs = miner(patent_emb, current_labels)\n",
    "            loss += loss_function(patent_emb, current_labels, hard_pairs)\n",
    "            #current_labels[current_labels==0] = -1\n",
    "            #loss += loss_function(patent_emb, label_emb, current_labels)\n",
    "        \n",
    "        running_loss += loss\n",
    "        avg_loss = running_loss / (batch_no+1)\n",
    "        \n",
    "        print(f\"step_no: {step}, epoch: {epoch}, batch_no: {batch_no}, loss: {loss}, avg_loss: {avg_loss}\")\n",
    "        writer.add_scalar('loss/embedding', loss, step)\n",
    "        loss.backward()\n",
    "        \n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()   \n",
    "        \n",
    "#         predictions = model.predict_classes(patent_emb)\n",
    "#         classifier_loss = classifier_function(labels, predictions)\n",
    "#         writer.add_scalar('loss/classifier', classifier_loss, step)\n",
    "#         classifier_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()       \n",
    "        step = step + 1\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            model.eval()\n",
    "            f1s = []\n",
    "            with torch.no_grad():\n",
    "                for val_batch_no, val_batch in enumerate(testing_loader):\n",
    "                    raw_predictions = torch.sigmoid(model.predict_classes(patent_emb)).cpu()\n",
    "                    #print(f\"raw_predictions: {raw_predictions.shape}\")\n",
    "                    predictions = raw_predictions > 0.5\n",
    "                    #print(f\"predictions: {predictions.shape}: {predictions}\")\n",
    "                    #print(f\"val_batch: {val_batch['labels'].shape}: {val_batch['labels']}\")\n",
    "                    if val_batch['labels'].shape[0] == 3:\n",
    "                        continue\n",
    "                    f1s.append(f1_score(val_batch['labels'], predictions, average='samples'))\n",
    "            f1_avg = np.mean(f1s)\n",
    "            writer.add_scalar('f1/valid', f1_avg, step)\n",
    "            print(f1_avg)\n",
    "            model.train()\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "f1s = []\n",
    "with torch.no_grad():\n",
    "    for batch_no, batch in enumerate(testing_loader):\n",
    "        predictions = torch.sigmoid(model.predict_classes(patent_emb)).cpu() > 0.5\n",
    "        f1s.append(f1_score(batch['labels'], predictions, average='samples'))\n",
    "f1_avg = np.mean(f1)\n",
    "print(f1_avg)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "#criterion = contrib.nn.criterion.LovaszLossMultiLabel()\n",
    "#scheduler = contrib.nn.OneCycleLRWithWarmup(optimizer, num_steps=500, lr_range=(1e-4, 1e-8), init_lr=1e-9, warmup_fraction=0.2)\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [2])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "#lrfinder = dl.LRFinder(final_lr=1)\n",
    "\n",
    "runner = dl.SupervisedRunner(input_key=(\"abstract\", \"claims\", \"embedded_cpc\", \"people\", \"citing\"))\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    logdir=logdir,\n",
    "    num_epochs=EPOCHS,\n",
    "    callbacks=[\n",
    "               dl.MultiLabelAccuracyCallback(threshold=PRED_THRES, activation=\"None\"),\n",
    "               dl.EarlyStoppingCallback(patience=3, metric=\"multi_label_accuracy\", minimize=False),\n",
    "               dl.TensorboardLogger(),\n",
    "               #dl.CheckpointCallback(),\n",
    "               dl.OptimizerCallback(accumulation_steps=ACCUM_STEPS),\n",
    "               dl.ValidationManagerCallback(),\n",
    "               ],\n",
    "               #dl.MetricManagerCallback(num_classes=len(subset), )],\n",
    "    \n",
    "    fp16=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.vstack(list(map(\n",
    "    lambda x: x[\"logits\"].cpu().numpy(), \n",
    "    runner.predict_loader(loader=loaders[\"valid\"], resume=f\"{logdir}/checkpoints/best.pth\" )\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_predictions = torch.sigmoid(torch.from_numpy(predictions)) > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "print(classification_report(testing_set[subset].astype(int), binary_predictions, target_names=subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision    recall  f1-score   support\n",
    "\n",
    "     SpecificationofUse_JointReplacement       0.21      0.32      0.25        44\n",
    "                                 Imaging       0.55      1.00      0.71       133\n",
    "                          SurgicalMethod       0.00      0.00      0.00        40\n",
    "     Manufacturing_AdditiveManufacturing       0.00      0.00      0.00        38\n",
    "                      Imaging_Ultrasound       0.00      0.00      0.00        32\n",
    "                             Imaging_MRI       0.34      0.20      0.26        59\n",
    "AnatomicalTarget_UpperExtremity_Shoulder       0.00      0.00      0.00        23\n",
    "              SpecificationofUse_Disease       0.00      0.00      0.00        30\n",
    "             PersonalizedProduct_Implant       0.51      1.00      0.68       124\n",
    "                           Manufacturing       0.34      0.90      0.49        83\n",
    "         AnatomicalTarget_UpperExtremity       0.00      0.00      0.00        31\n",
    "                     AnalysisAndModeling       0.36      0.96      0.52        84\n",
    "         AnatomicalTarget_LowerExtremity       0.47      1.00      0.63       113\n",
    "                      SpecificationofUse       0.34      0.99      0.50        79\n",
    "                        AnatomicalTarget       0.67      1.00      0.81       164\n",
    "           PersonalizedProduct_Guide/Jig       0.49      1.00      0.66       120\n",
    "            AnatomicalTarget_Torso_Spine       0.00      0.00      0.00        21\n",
    "                              Imaging_CT       0.29      0.31      0.30        59\n",
    "          AnalysisAndModeling_3DModeling       0.30      0.93      0.46        71\n",
    "    AnatomicalTarget_LowerExtremity_Knee       0.34      0.78      0.48        82\n",
    "                  AnatomicalTarget_Torso       0.00      0.00      0.00        35\n",
    "     AnatomicalTarget_LowerExtremity_Hip       0.00      0.00      0.00        40\n",
    "\n",
    "                               micro avg       0.43      0.71      0.54      1505\n",
    "                               macro avg       0.24      0.47      0.31      1505\n",
    "                            weighted avg       0.36      0.71      0.47      1505\n",
    "                             samples avg       0.43      0.74      0.52      1505"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longformer base (claims + abstract)\n",
    "                                            precision    recall  f1-score   support\n",
    "\n",
    "                     AnalysisAndModeling       0.35      1.00      0.51        84\n",
    "                        AnatomicalTarget       0.67      1.00      0.81       164\n",
    "            AnatomicalTarget_Torso_Spine       0.00      0.00      0.00        21\n",
    "     AnatomicalTarget_LowerExtremity_Hip       0.00      0.00      0.00        40\n",
    "                             Imaging_MRI       0.00      0.00      0.00        59\n",
    "                                 Imaging       0.55      1.00      0.71       133\n",
    "                           Manufacturing       0.34      0.99      0.50        83\n",
    "             PersonalizedProduct_Implant       0.51      1.00      0.68       124\n",
    "              SpecificationofUse_Disease       0.00      0.00      0.00        30\n",
    "                      SpecificationofUse       0.34      0.89      0.49        79\n",
    "     SpecificationofUse_JointReplacement       0.00      0.00      0.00        44\n",
    "                  AnatomicalTarget_Torso       0.00      0.00      0.00        35\n",
    "         AnatomicalTarget_UpperExtremity       0.00      0.00      0.00        31\n",
    "                      Imaging_Ultrasound       0.00      0.00      0.00        32\n",
    "                              Imaging_CT       0.32      0.25      0.28        59\n",
    "          AnalysisAndModeling_3DModeling       0.28      0.80      0.42        71\n",
    "                          SurgicalMethod       0.00      0.00      0.00        40\n",
    "AnatomicalTarget_UpperExtremity_Shoulder       0.00      0.00      0.00        23\n",
    "    AnatomicalTarget_LowerExtremity_Knee       0.34      1.00      0.51        82\n",
    "           PersonalizedProduct_Guide/Jig       0.49      1.00      0.66       120\n",
    "         AnatomicalTarget_LowerExtremity       0.47      1.00      0.63       113\n",
    "     Manufacturing_AdditiveManufacturing       0.00      0.00      0.00        38\n",
    "\n",
    "                               micro avg       0.44      0.69      0.54      1505\n",
    "                               macro avg       0.21      0.45      0.28      1505\n",
    "                            weighted avg       0.34      0.69      0.45      1505\n",
    "                             samples avg       0.44      0.73      0.52      1505"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Albert base w/ 256 length sequences (claims + abstract)                \n",
    "    \n",
    "    precision    recall  f1-score   support\n",
    "\n",
    "         AnatomicalTarget_LowerExtremity       0.47      1.00      0.63       113\n",
    "     Manufacturing_AdditiveManufacturing       0.67      0.05      0.10        38\n",
    "                                 Imaging       0.55      1.00      0.71       133\n",
    "                          SurgicalMethod       0.00      0.00      0.00        40\n",
    "AnatomicalTarget_UpperExtremity_Shoulder       0.18      0.13      0.15        23\n",
    "              SpecificationofUse_Disease       0.00      0.00      0.00        30\n",
    "    AnatomicalTarget_LowerExtremity_Knee       0.45      0.40      0.43        82\n",
    "                      SpecificationofUse       0.35      0.95      0.52        79\n",
    "         AnatomicalTarget_UpperExtremity       0.00      0.00      0.00        31\n",
    "            AnatomicalTarget_Torso_Spine       0.00      0.00      0.00        21\n",
    "             PersonalizedProduct_Implant       0.51      1.00      0.68       124\n",
    "                     AnalysisAndModeling       0.38      0.65      0.48        84\n",
    "          AnalysisAndModeling_3DModeling       0.33      0.68      0.44        71\n",
    "                  AnatomicalTarget_Torso       0.00      0.00      0.00        35\n",
    "     SpecificationofUse_JointReplacement       0.18      0.68      0.28        44\n",
    "                        AnatomicalTarget       0.67      1.00      0.81       164\n",
    "                           Manufacturing       0.32      0.87      0.47        83\n",
    "                             Imaging_MRI       0.26      0.15      0.19        59\n",
    "                      Imaging_Ultrasound       0.20      0.12      0.15        32\n",
    "                              Imaging_CT       0.32      0.34      0.33        59\n",
    "     AnatomicalTarget_LowerExtremity_Hip       0.14      0.03      0.04        40\n",
    "           PersonalizedProduct_Guide/Jig       0.50      1.00      0.66       120\n",
    "\n",
    "                               micro avg       0.43      0.67      0.52      1505\n",
    "                               macro avg       0.29      0.46      0.32      1505\n",
    "                            weighted avg       0.39      0.67      0.47      1505\n",
    "                             samples avg       0.43      0.70      0.51      1505"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamming_loss(testing_set[subset], binary_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
