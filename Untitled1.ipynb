{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import LightningLoggerBase, TensorBoardLogger\n",
    "from torchnlp.random import set_seed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from transformers import AutoModel\n",
    "import pytorch_lightning as pl\n",
    "from torchnlp.encoders import LabelEncoder\n",
    "from torchnlp.utils import collate_tensors, lengths_to_mask\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from torchnlp.encoders import Encoder\n",
    "from torchnlp.encoders.text import stack_and_pad_tensors\n",
    "from torchnlp.encoders.text.text_encoder import TextEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(TextEncoder):\n",
    "    \"\"\"\n",
    "    Wrapper arround BERT tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_model) -> None:\n",
    "        self.enforce_reversible = False\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "        self.itos = self.tokenizer.ids_to_tokens\n",
    "\n",
    "    @property\n",
    "    def unk_index(self) -> int:\n",
    "        \"\"\" Returns the index used for the unknown token. \"\"\"\n",
    "        return self.tokenizer.unk_token_id\n",
    "\n",
    "    @property\n",
    "    def bos_index(self) -> int:\n",
    "        \"\"\" Returns the index used for the begin-of-sentence token. \"\"\"\n",
    "        return self.tokenizer.cls_token_id\n",
    "\n",
    "    @property\n",
    "    def eos_index(self) -> int:\n",
    "        \"\"\" Returns the index used for the end-of-sentence token. \"\"\"\n",
    "        return self.tokenizer.sep_token_id\n",
    "\n",
    "    @property\n",
    "    def padding_index(self) -> int:\n",
    "        \"\"\" Returns the index used for padding. \"\"\"\n",
    "        return self.tokenizer.pad_token_id\n",
    "\n",
    "    @property\n",
    "    def vocab(self) -> list:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            list: List of tokens in the dictionary.\n",
    "        \"\"\"\n",
    "        return self.tokenizer.vocab\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            int: Number of tokens in the dictionary.\n",
    "        \"\"\"\n",
    "        return len(self.itos)\n",
    "\n",
    "    def encode(self, sequence: str) -> torch.Tensor:\n",
    "        \"\"\" Encodes a 'sequence'.\n",
    "        :param sequence: String 'sequence' to encode.\n",
    "        \n",
    "        :return: torch.Tensor with Encoding of the `sequence`.\n",
    "        \"\"\"\n",
    "        sequence = TextEncoder.encode(self, sequence)\n",
    "        return self.tokenizer(sequence, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "\n",
    "    def batch_encode(self, sentences: list) -> (torch.Tensor, torch.Tensor):\n",
    "        \"\"\"\n",
    "        :param iterator (iterator): Batch of text to encode.\n",
    "        :param **kwargs: Keyword arguments passed to 'encode'.\n",
    "            \n",
    "        Returns\n",
    "            torch.Tensor, torch.Tensor: Encoded and padded batch of sequences; Original lengths of\n",
    "                sequences.\n",
    "        \"\"\"\n",
    "        tokenizer_output = self.tokenizer(\n",
    "            sentences, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            return_length=True, \n",
    "            return_token_type_ids=False, \n",
    "            return_attention_mask=False,\n",
    "            truncation=\"only_first\",\n",
    "            max_length=512\n",
    "        )\n",
    "        return tokenizer_output[\"input_ids\"], tokenizer_output[\"length\"]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def mask_fill(\n",
    "    fill_value: float,\n",
    "    tokens: torch.tensor,\n",
    "    embeddings: torch.tensor,\n",
    "    padding_index: int,\n",
    ") -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Function that masks embeddings representing padded elements.\n",
    "    :param fill_value: the value to fill the embeddings belonging to padded tokens.\n",
    "    :param tokens: The input sequences [bsz x seq_len].\n",
    "    :param embeddings: word embeddings [bsz x seq_len x hiddens].\n",
    "    :param padding_index: Index of the padding token.\n",
    "    \"\"\"\n",
    "    padding_mask = tokens.eq(padding_index).unsqueeze(-1)\n",
    "    return embeddings.float().masked_fill_(padding_mask, fill_value).type_as(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Classifier(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Sample model to show how to use a Transformer model to classify sentences.\n",
    "    \n",
    "    :param hparams: ArgumentParser containing the hyperparameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    class DataModule(pl.LightningDataModule):\n",
    "        def __init__(self, classifier_instance):\n",
    "            super().__init__()\n",
    "            self.hparams = classifier_instance.hparams\n",
    "            self.classifier = classifier_instance\n",
    "            # Label Encoder\n",
    "            self.label_encoder = LabelEncoder(\n",
    "                pd.read_csv(self.hparams.train_csv).label.astype(str).unique().tolist(), \n",
    "                reserved_labels=[]\n",
    "            )\n",
    "            self.label_encoder.unknown_index = None\n",
    "\n",
    "        def read_csv(self, path: str) -> list:\n",
    "            \"\"\" Reads a comma separated value file.\n",
    "            :param path: path to a csv file.\n",
    "            \n",
    "            :return: List of records as dictionaries\n",
    "            \"\"\"\n",
    "            df = pd.read_csv(path)\n",
    "            df = df[[\"text\", \"label\"]]\n",
    "            df[\"text\"] = df[\"text\"].astype(str)\n",
    "            df[\"label\"] = df[\"label\"].astype(str)\n",
    "            return df.to_dict(\"records\")\n",
    "\n",
    "        def train_dataloader(self) -> DataLoader:\n",
    "            \"\"\" Function that loads the train set. \"\"\"\n",
    "            self._train_dataset = self.read_csv(self.hparams.train_csv)\n",
    "            return DataLoader(\n",
    "                dataset=self._train_dataset,\n",
    "                sampler=RandomSampler(self._train_dataset),\n",
    "                batch_size=self.hparams.batch_size,\n",
    "                collate_fn=self.classifier.prepare_sample,\n",
    "                num_workers=self.hparams.loader_workers,\n",
    "            )\n",
    "\n",
    "        def val_dataloader(self) -> DataLoader:\n",
    "            \"\"\" Function that loads the validation set. \"\"\"\n",
    "            self._dev_dataset = self.read_csv(self.hparams.dev_csv)\n",
    "            return DataLoader(\n",
    "                dataset=self._dev_dataset,\n",
    "                batch_size=self.hparams.batch_size,\n",
    "                collate_fn=self.classifier.prepare_sample,\n",
    "                num_workers=self.hparams.loader_workers,\n",
    "            )\n",
    "\n",
    "        def test_dataloader(self) -> DataLoader:\n",
    "            \"\"\" Function that loads the test set. \"\"\"\n",
    "            self._test_dataset = self.read_csv(self.hparams.test_csv)\n",
    "            return DataLoader(\n",
    "                dataset=self._test_dataset,\n",
    "                batch_size=self.hparams.batch_size,\n",
    "                collate_fn=self.classifier.prepare_sample,\n",
    "                num_workers=self.hparams.loader_workers,\n",
    "            )\n",
    "\n",
    "    def __init__(self, hparams: Namespace) -> None:\n",
    "        super(Classifier, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.batch_size = hparams.batch_size\n",
    "\n",
    "        # Build Data module\n",
    "        self.data = self.DataModule(self)\n",
    "        \n",
    "        # build model\n",
    "        self.__build_model()\n",
    "\n",
    "        # Loss criterion initialization.\n",
    "        self.__build_loss()\n",
    "\n",
    "        if hparams.nr_frozen_epochs > 0:\n",
    "            self.freeze_encoder()\n",
    "        else:\n",
    "            self._frozen = False\n",
    "        self.nr_frozen_epochs = hparams.nr_frozen_epochs\n",
    "\n",
    "    def __build_model(self) -> None:\n",
    "        \"\"\" Init BERT model + tokenizer + classification head.\"\"\"\n",
    "        self.bert = AutoModel.from_pretrained(\n",
    "            self.hparams.encoder_model, output_hidden_states=True\n",
    "        )\n",
    "\n",
    "        # set the number of features our encoder model will return...\n",
    "        if self.hparams.encoder_model == \"google/bert_uncased_L-2_H-128_A-2\":\n",
    "            self.encoder_features = 128\n",
    "        else:\n",
    "            self.encoder_features = 768\n",
    "\n",
    "        # Tokenizer\n",
    "        self.tokenizer = Tokenizer(\"bert-base-uncased\")\n",
    "\n",
    "        # Classification head\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(self.encoder_features, self.encoder_features * 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.encoder_features * 2, self.encoder_features),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.encoder_features, self.data.label_encoder.vocab_size),\n",
    "        )\n",
    "\n",
    "    def __build_loss(self):\n",
    "        \"\"\" Initializes the loss function/s. \"\"\"\n",
    "        self._loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def unfreeze_encoder(self) -> None:\n",
    "        \"\"\" un-freezes the encoder layer. \"\"\"\n",
    "        if self._frozen:\n",
    "            log.info(f\"\\n-- Encoder model fine-tuning\")\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = True\n",
    "            self._frozen = False\n",
    "\n",
    "    def freeze_encoder(self) -> None:\n",
    "        \"\"\" freezes the encoder layer. \"\"\"\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        self._frozen = True\n",
    "\n",
    "    def predict(self, sample: dict) -> dict:\n",
    "        \"\"\" Predict function.\n",
    "        :param sample: dictionary with the text we want to classify.\n",
    "        Returns:\n",
    "            Dictionary with the input text and the predicted label.\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            self.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_input, _ = self.prepare_sample([sample], prepare_target=False)\n",
    "            model_out = self.forward(**model_input)\n",
    "            logits = model_out[\"logits\"].numpy()\n",
    "            predicted_labels = [\n",
    "                self.data.label_encoder.index_to_token[prediction]\n",
    "                for prediction in np.argmax(logits, axis=1)\n",
    "            ]\n",
    "            sample[\"predicted_label\"] = predicted_labels[0]\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def forward(self, tokens, lengths):\n",
    "        \"\"\" Usual pytorch forward function.\n",
    "        :param tokens: text sequences [batch_size x src_seq_len]\n",
    "        :param lengths: source lengths [batch_size]\n",
    "        Returns:\n",
    "            Dictionary with model outputs (e.g: logits)\n",
    "        \"\"\"\n",
    "        tokens = tokens[:, : lengths.max()]\n",
    "        # When using just one GPU this should not change behavior\n",
    "        # but when splitting batches across GPU the tokens have padding\n",
    "        # from the entire original batch\n",
    "        mask = lengths_to_mask(lengths, device=tokens.device)\n",
    "\n",
    "        # Run BERT model.\n",
    "        word_embeddings = self.bert(tokens, mask)[0]\n",
    "\n",
    "        # Average Pooling\n",
    "        word_embeddings = mask_fill(\n",
    "            0.0, tokens, word_embeddings, self.tokenizer.padding_index\n",
    "        )\n",
    "        sentemb = torch.sum(word_embeddings, 1)\n",
    "        sum_mask = mask.unsqueeze(-1).expand(word_embeddings.size()).float().sum(1)\n",
    "        sentemb = sentemb / sum_mask\n",
    "\n",
    "        return {\"logits\": self.classification_head(sentemb)}\n",
    "\n",
    "    def loss(self, predictions: dict, targets: dict) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Computes Loss value according to a loss function.\n",
    "        :param predictions: model specific output. Must contain a key 'logits' with\n",
    "            a tensor [batch_size x 1] with model predictions\n",
    "        :param labels: Label values [batch_size]\n",
    "        Returns:\n",
    "            torch.tensor with loss value.\n",
    "        \"\"\"\n",
    "        return self._loss(predictions[\"logits\"], targets[\"labels\"])\n",
    "\n",
    "    def prepare_sample(self, sample: list, prepare_target: bool = True) -> (dict, dict):\n",
    "        \"\"\"\n",
    "        Function that prepares a sample to input the model.\n",
    "        :param sample: list of dictionaries.\n",
    "        \n",
    "        Returns:\n",
    "            - dictionary with the expected model inputs.\n",
    "            - dictionary with the expected target labels.\n",
    "        \"\"\"\n",
    "        sample = collate_tensors(sample)\n",
    "        tokens, lengths = self.tokenizer.batch_encode(sample[\"text\"])\n",
    "\n",
    "        inputs = {\"tokens\": tokens, \"lengths\": lengths}\n",
    "\n",
    "        if not prepare_target:\n",
    "            return inputs, {}\n",
    "\n",
    "        # Prepare target:\n",
    "        try:\n",
    "            targets = {\"labels\": self.data.label_encoder.batch_encode(sample[\"label\"])}\n",
    "            return inputs, targets\n",
    "        except RuntimeError:\n",
    "            raise Exception(\"Label encoder found an unknown label.\")\n",
    "\n",
    "    def training_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\n",
    "        \"\"\" \n",
    "        Runs one training step. This usually consists in the forward function followed\n",
    "            by the loss function.\n",
    "        \n",
    "        :param batch: The output of your dataloader. \n",
    "        :param batch_nb: Integer displaying which batch this is\n",
    "        Returns:\n",
    "            - dictionary containing the loss and the metrics to be added to the lightning logger.\n",
    "        \"\"\"\n",
    "        inputs, targets = batch\n",
    "        model_out = self.forward(**inputs)\n",
    "        loss_val = self.loss(model_out, targets)\n",
    "\n",
    "        # in DP mode (default) make sure if result is scalar, there's another dim in the beginning\n",
    "        if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "            loss_val = loss_val.unsqueeze(0)\n",
    "\n",
    "        tqdm_dict = {\"train_loss\": loss_val}\n",
    "        output = OrderedDict(\n",
    "            {\"loss\": loss_val, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict}\n",
    "        )\n",
    "\n",
    "        # can also return just a scalar instead of a dict (return loss_val)\n",
    "        return output\n",
    "\n",
    "    def validation_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\n",
    "        \"\"\" Similar to the training step but with the model in eval mode.\n",
    "        Returns:\n",
    "            - dictionary passed to the validation_end function.\n",
    "        \"\"\"\n",
    "        inputs, targets = batch\n",
    "        model_out = self.forward(**inputs)\n",
    "        loss_val = self.loss(model_out, targets)\n",
    "\n",
    "        y = targets[\"labels\"]\n",
    "        y_hat = model_out[\"logits\"]\n",
    "\n",
    "        # acc\n",
    "        labels_hat = torch.argmax(y_hat, dim=1)\n",
    "        val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\n",
    "        val_acc = torch.tensor(val_acc)\n",
    "\n",
    "        if self.on_gpu:\n",
    "            val_acc = val_acc.cuda(loss_val.device.index)\n",
    "\n",
    "        # in DP mode (default) make sure if result is scalar, there's another dim in the beginning\n",
    "        if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "            loss_val = loss_val.unsqueeze(0)\n",
    "            val_acc = val_acc.unsqueeze(0)\n",
    "\n",
    "        output = OrderedDict({\"val_loss\": loss_val, \"val_acc\": val_acc,})\n",
    "\n",
    "        # can also return just a scalar instead of a dict (return loss_val)\n",
    "        return output\n",
    "\n",
    "    def validation_end(self, outputs: list) -> dict:\n",
    "        \"\"\" Function that takes as input a list of dictionaries returned by the validation_step\n",
    "        function and measures the model performance accross the entire validation set.\n",
    "        \n",
    "        Returns:\n",
    "            - Dictionary with metrics to be added to the lightning logger.  \n",
    "        \"\"\"\n",
    "        val_loss_mean = 0\n",
    "        val_acc_mean = 0\n",
    "        for output in outputs:\n",
    "            val_loss = output[\"val_loss\"]\n",
    "\n",
    "            # reduce manually when using dp\n",
    "            if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "                val_loss = torch.mean(val_loss)\n",
    "            val_loss_mean += val_loss\n",
    "\n",
    "            # reduce manually when using dp\n",
    "            val_acc = output[\"val_acc\"]\n",
    "            if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "                val_acc = torch.mean(val_acc)\n",
    "\n",
    "            val_acc_mean += val_acc\n",
    "\n",
    "        val_loss_mean /= len(outputs)\n",
    "        val_acc_mean /= len(outputs)\n",
    "        tqdm_dict = {\"val_loss\": val_loss_mean, \"val_acc\": val_acc_mean}\n",
    "        result = {\n",
    "            \"progress_bar\": tqdm_dict,\n",
    "            \"log\": tqdm_dict,\n",
    "            \"val_loss\": val_loss_mean,\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\" Sets different Learning rates for different parameter groups. \"\"\"\n",
    "        parameters = [\n",
    "            {\"params\": self.classification_head.parameters()},\n",
    "            {\n",
    "                \"params\": self.bert.parameters(),\n",
    "                \"lr\": self.hparams.encoder_learning_rate,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = optim.Adam(parameters, lr=self.hparams.learning_rate)\n",
    "        return [optimizer], []\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\" Pytorch lightning hook \"\"\"\n",
    "        if self.current_epoch + 1 >= self.nr_frozen_epochs:\n",
    "            self.unfreeze_encoder()\n",
    "    \n",
    "    @classmethod\n",
    "    def add_model_specific_args(\n",
    "        cls, parser: ArgumentParser\n",
    "    ) -> ArgumentParser:\n",
    "        \"\"\" Parser for Estimator specific arguments/hyperparameters. \n",
    "        :param parser: argparse.ArgumentParser\n",
    "        Returns:\n",
    "            - updated parser\n",
    "        \"\"\"\n",
    "        parser.add_argument(\n",
    "            \"--encoder_model\",\n",
    "            default=\"bert-base-uncased\",\n",
    "            type=str,\n",
    "            help=\"Encoder model to be used.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--encoder_learning_rate\",\n",
    "            default=1e-05,\n",
    "            type=float,\n",
    "            help=\"Encoder specific learning rate.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--learning_rate\",\n",
    "            default=3e-05,\n",
    "            type=float,\n",
    "            help=\"Classification head learning rate.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--nr_frozen_epochs\",\n",
    "            default=1,\n",
    "            type=int,\n",
    "            help=\"Number of epochs we want to keep the encoder model frozen.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--train_csv\",\n",
    "            default=\"data/imdb_reviews_train.csv\",\n",
    "            type=str,\n",
    "            help=\"Path to the file containing the train data.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--dev_csv\",\n",
    "            default=\"data/imdb_reviews_test.csv\",\n",
    "            type=str,\n",
    "            help=\"Path to the file containing the dev data.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--test_csv\",\n",
    "            default=\"data/imdb_reviews_test.csv\",\n",
    "            type=str,\n",
    "            help=\"Path to the file containing the dev data.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--loader_workers\",\n",
    "            default=8,\n",
    "            type=int,\n",
    "            help=\"How many subprocesses to use for data loading. 0 means that \\\n",
    "                the data will be loaded in the main process.\",\n",
    "        )\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"save_top_k\": 1,\n",
    "    \"monitor\": \"val_acc\",\n",
    "    \"metric_mode\": \"max\",\n",
    "    \"patience\": 3,\n",
    "    \"min_epochs\": 1,\n",
    "    \"max_epochs\": 20,\n",
    "    \"batch_size\": 16,\n",
    "    \"accumulate_grad_batches\": 2,\n",
    "    \"gpus\": 1,\n",
    "    \"val_check_interval\": 1.0\n",
    "}\n",
    "\n",
    "model = Classifier(hparams)\n",
    "early_stop_callback = EarlyStopping(\n",
    "        monitor=hparams.monitor,\n",
    "        min_delta=0.0,\n",
    "        patience=hparams.patience,\n",
    "        verbose=True,\n",
    "        mode=hparams.metric_mode,\n",
    "    )\n",
    "\n",
    "tb_logger = TensorBoardLogger(\n",
    "        save_dir=\"experiments/\",\n",
    "        version=\"version_\" + datetime.now().strftime(\"%d-%m-%Y--%H-%M-%S\"),\n",
    "        name=\"\",\n",
    "    )\n",
    "\n",
    "ckpt_path = os.path.join(\n",
    "        \"experiments/\", tb_logger.version, \"checkpoints\",\n",
    "    )\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=ckpt_path,\n",
    "        save_top_k=hparams.save_top_k,\n",
    "        verbose=True,\n",
    "        monitor=hparams.monitor,\n",
    "        period=1,\n",
    "        mode=hparams.metric_mode,\n",
    "        save_weights_only=True\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "        logger=tb_logger,\n",
    "        checkpoint_callback=True,\n",
    "        early_stop_callback=early_stop_callback,\n",
    "        gradient_clip_val=1.0,\n",
    "        gpus=hparams.gpus,\n",
    "        log_gpu_memory=\"all\",\n",
    "        deterministic=True,\n",
    "        check_val_every_n_epoch=1,\n",
    "        fast_dev_run=False,\n",
    "        accumulate_grad_batches=hparams.accumulate_grad_batches,\n",
    "        max_epochs=hparams.max_epochs,\n",
    "        min_epochs=hparams.min_epochs,\n",
    "        val_check_interval=hparams.val_check_interval,\n",
    "        distributed_backend=\"dp\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, model.data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
