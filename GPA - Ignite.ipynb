{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from catalyst import dl\n",
    "from catalyst import dl, utils\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import Accuracy, Loss, RunningAverage\n",
    "from ignite.handlers import ModelCheckpoint, EarlyStopping\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import logging\n",
    "from util import *\n",
    "import torch.nn.functional as F\n",
    "from pytorch_metric_learning import miners, losses\n",
    "import catalyst.contrib as contrib\n",
    "from datetime import datetime\n",
    "import pprint\n",
    "from ignite.engine import _prepare_batch\n",
    "from ignite.engine import create_supervised_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_json(\"training_set.json.gz\", lines=True, orient=\"records\")\n",
    "testing_set = pd.read_json(\"testing_set.json.gz\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = list(sorted(set(all_tiers_100)-set([\"PersonalizedProduct\"])))\n",
    "nice_subset = [tier_translations[x] for x in subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"/home/martin/IdeaProjects/phenetics/bertForPatents/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModel.from_pretrained(model_name, gradient_checkpointing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['labels']=training_set[subset].astype(int).values.tolist()\n",
    "testing_set['labels']=testing_set[subset].astype(int).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpc_embeddings = np.fromfile(\"/home/martin/patentmark/cpc.node2vec.emb.32d.bin\", dtype=np.float32).reshape((-1,32))\n",
    "\n",
    "# import joblib\n",
    "# cpc_labelizer = joblib.load('./node2id.joblib')\n",
    "# cpc_lookup = {c: n for n, c in enumerate(cpc_labelizer.classes_)}\n",
    "\n",
    "# @f.collecting\n",
    "# def convert_cpc_codes(codes):\n",
    "#     for code in codes:\n",
    "#         if code in cpc_lookup:\n",
    "#             yield cpc_lookup[code]\n",
    "    \n",
    "# def embed_cpc_codes(codes):\n",
    "#     embedding = np.zeros(32)\n",
    "#     converted = convert_cpc_codes(codes)\n",
    "    \n",
    "#     if not converted:\n",
    "#         return embedding\n",
    "    \n",
    "#     for code_id in converted:\n",
    "#         embedding = embedding + cpc_embeddings[code_id]\n",
    "        \n",
    "#     return embedding / len(converted)\n",
    "\n",
    "# training_set['embedded_cpc'] = training_set.cpc_codes.apply(embed_cpc_codes)\n",
    "# testing_set['embedded_cpc'] = testing_set.cpc_codes.apply(embed_cpc_codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cpc_coder = CountVectorizer(analyzer=cpc_split, min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set['citing'] = training_set[['citations', 'cited_by']].apply(\n",
    "#         lambda row: list(set(row['citations']+row['cited_by'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_set['citing'] = testing_set[['citations', 'cited_by']].apply(\n",
    "#         lambda row: list(set(row['citations']+row['cited_by'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set['people'] = training_set[['assignees', 'inventors']].apply(lambda row: list(set(row['assignees']+row['inventors'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_set['people'] = testing_set[['assignees', 'inventors']].apply(lambda row: list(set(row['assignees']+row['inventors'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format(t):\n",
    "#     CORP_TYPES = set(\n",
    "#         [\n",
    "#             \"INC\",\n",
    "#             \"LLC\" \"CORP\",\n",
    "#             \"KK\",\n",
    "#             \"SA\",\n",
    "#             \"SRL\",\n",
    "#             \"LTD\",\n",
    "#             \"NL\",\n",
    "#             \"PTY\",\n",
    "#             \"AG\",\n",
    "#             \"GMBH\",\n",
    "#             \"KG\",\n",
    "#             \"OG\",\n",
    "#             \"LIMITED\",\n",
    "#             \"SARL\",\n",
    "#             \"BM\",\n",
    "#             \"PLC\",\n",
    "#             \"LP\",\n",
    "#             \"IP\",\n",
    "#             \"DBA\",\n",
    "#             \"CORP\",\n",
    "#             \"CO\",\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     tokenized = strip_non_alphanum(strip_punctuation(t)).upper().split(\" \")\n",
    "#     cleaned = [t for t in tokenized if t not in CORP_TYPES]\n",
    "#     return \"\".join(cleaned)\n",
    "\n",
    "\n",
    "# people_coder = CountVectorizer(analyzer=lambda x: map(format, x), min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# citing_coder = CountVectorizer(analyzer=lambda x: x, min_df=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# citing_coder.fit(training_set.citing)\n",
    "# len(citing_coder.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# people_coder.fit(training_set.people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(people_coder.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #training_set['cpc_vec'] = list(cpc_coder.transform(training_set.cpc_codes).todense())\n",
    "# #testing_set['cpc_vec'] = list(cpc_coder.transform(testing_set.cpc_codes).todense())\n",
    "# training_set['people_vec'] = list(np.array(people_coder.transform(training_set.people).todense()))\n",
    "# testing_set['people_vec'] = list(np.array(people_coder.transform(testing_set.people).todense()))\n",
    "# training_set['citing_vec'] = list(np.array(citing_coder.transform(training_set.citing).todense()))\n",
    "# testing_set['citing_vec'] = list(np.array(citing_coder.transform(testing_set.citing).todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set = training_set.explode('labels').reset_index()\n",
    "# testing_set = testing_set.explode('labels').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_set.labels.str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#possible_labels = set([tier_translations[x] for x in subset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokenizer, text, max_len):\n",
    "        text = str(text)\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            f\"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            f\"attention_mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            f\"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "        }\n",
    "    \n",
    "def tokenize_list(tokenizer, text_list, max_len):\n",
    "        \n",
    "        inputs = tokenizer.batch_encode_plus(\n",
    "            text_list,\n",
    "            #None,\n",
    "            #add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            f\"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            f\"attention_mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            f\"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset_tokenized = tokenize_list(tokenizer, nice_subset, 56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fse\n",
    "# import gensim.downloader as api\n",
    "# glove = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fse import IndexedList, SplitIndexedList\n",
    "# from fse.models import uSIF\n",
    "# s = SplitIndexedList(nice_subset)\n",
    "# label_model = uSIF(glove, workers=32, lang_freq=\"en\")\n",
    "# label_model.train(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset_embeddings = label_model.infer(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        \n",
    "        self.claims = dataframe.claims\n",
    "        self.abstracts = dataframe.abstract\n",
    "        \n",
    "        self.labels = dataframe.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        abstract = tokenize(tokenizer, self.abstracts[index], max_len=MAX_LEN_ABSTRACT)\n",
    "        claims = tokenize(tokenizer, self.claims[index], MAX_LEN_CLAIMS)\n",
    "        \n",
    "        labels = torch.tensor(np.array(self.labels[index]), dtype=torch.float)\n",
    "        \n",
    "        #people = torch.tensor(np.array(self.data.people_vec[index]), dtype=torch.float)\n",
    "        #citing = torch.tensor(np.array(self.data.citing_vec[index]), dtype=torch.float)\n",
    "        #embedded_cpc = torch.tensor(np.array(self.data.embedded_cpc[index]), dtype=torch.float)        \n",
    "                \n",
    "        return {\"abstract\": abstract, \n",
    "                \"claims\": claims, \n",
    "                \n",
    "                #'cpcs': cpcs,\n",
    "                # 'people': people,\n",
    "                # 'citing': citing,\n",
    "                # 'embedded_cpc': embedded_cpc,\n",
    "                 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = MultiLabelDataset(training_set, tokenizer)\n",
    "testing_dataset = MultiLabelDataset(testing_set, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN_CLAIMS = 512\n",
    "MAX_LEN_ABSTRACT = 160\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-5\n",
    "SEED = 17\n",
    "PRED_THRES = 0.4\n",
    "ACCUM_STEPS = 8\n",
    "NUM_LABELS = len(nice_subset)\n",
    "\n",
    "device = utils.get_device()\n",
    "logdir=\"/var/patentmark/logdir/fit2/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_dataset, **train_params)\n",
    "testing_loader = DataLoader(testing_dataset, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PatentModel(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.text_embedder = AutoModel.from_pretrained(model_name, gradient_checkpointing=True)\n",
    "        \n",
    "#         #self.people_embedder = torch.nn.Linear(len(people_coder.vocabulary_), 64)\n",
    "#         #self.citing_embedder = torch.nn.Linear(len(citing_coder.vocabulary_), 64)\n",
    "        \n",
    "#         total_embedding_size = self.text_embedder.pooler.dense.out_features*2 #+32+64*2\n",
    "#         output_size = 768 #self.text_embedder.pooler.dense.out_features\n",
    "        \n",
    "#         self.dropout1 = nn.Dropout(0.1)\n",
    "        \n",
    "#         self.dense1 = nn.Linear(total_embedding_size, output_size)\n",
    "#         self.dense1label = nn.Linear(self.text_embedder.pooler.dense.out_features, output_size)\n",
    "        \n",
    "#         self.categorizer = nn.Linear(output_size, NUM_LABELS)\n",
    "\n",
    "    \n",
    "#     def encode_label(self, label):\n",
    "#         label_emb = self.text_embedder(input_ids=label[\"input_ids\"].to(device), attention_mask=label[\"attention_mask\"].to(device))\n",
    "#         label_emb = label_emb[0][:,0]\n",
    "        \n",
    "#         x = self.dropout1(label_emb)\n",
    "#         x = F.elu(self.dense1label(x))\n",
    "#         return x\n",
    "    \n",
    "#     def predict_classes(self, embeddings):\n",
    "#         #x = self.dropout1(embeddings)\n",
    "#         x = embeddings\n",
    "#         x = self.categorizer(x)\n",
    "#         return x\n",
    "        \n",
    "        \n",
    "#     def encode_patent(self, abstract, claims): #, embedded_cpc, people, citing):\n",
    "        \n",
    "#         abstract_emb = self.text_embedder(input_ids=abstract[\"input_ids\"].to(device), \n",
    "#                                           attention_mask=abstract[\"attention_mask\"].to(device))\n",
    "#         abstract_emb = abstract_emb[0][:, 0]\n",
    "        \n",
    "#         claim_emb = self.text_embedder(input_ids=claims[\"input_ids\"].to(device), \n",
    "#                                        attention_mask=claims[\"attention_mask\"].to(device))\n",
    "#         claim_emb = claim_emb[0][:, 0]\n",
    "        \n",
    "# #         people_emb = F.elu(self.people_embedder(people.to(device)))\n",
    "# #         citing_emb = F.elu(self.citing_embedder(citing.to(device)))\n",
    "    \n",
    "#         x = torch.cat((abstract_emb, claim_emb), 1)\n",
    "#                        #, embedded_cpc.to(device), people_emb, citing_emb), 1)\n",
    "#         x = self.dropout1(x)\n",
    "#         x = F.elu(self.dense1(x))\n",
    "        \n",
    "#         return x\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.text_embedder = AutoModel.from_pretrained(model_name, gradient_checkpointing=True)\n",
    "        \n",
    "        total_embedding_size = self.text_embedder.pooler.dense.out_features*2\n",
    "        output_size = NUM_LABELS\n",
    "        bottleneck = 768\n",
    "    \n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dense1 = nn.Linear(total_embedding_size, bottleneck)\n",
    "        self.classifier = nn.Linear(bottleneck, output_size)\n",
    "\n",
    "            \n",
    "    def forward(self, abstract, claims):\n",
    "        abstract_emb = self.text_embedder(input_ids=abstract[\"input_ids\"], attention_mask=abstract[\"attention_mask\"])\n",
    "        abstract_emb = abstract_emb[0][:, 0]\n",
    "        \n",
    "        claim_emb = self.text_embedder(input_ids=claims[\"input_ids\"], attention_mask=claims[\"attention_mask\"])\n",
    "        claim_emb = claim_emb[0][:, 0]\n",
    "           \n",
    "        x = torch.cat((abstract_emb, claim_emb), 1)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.elu(self.dense1(x))\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "#model = PatentModel().to(device)\n",
    "model = BasicModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "#miner = miners.MultiSimilarityMiner()\n",
    "#loss_function = losses.TripletMarginLoss()\n",
    "#classifier_function = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# from ignite.contrib.handlers.tensorboard_logger import *\n",
    "# tb_logger = TensorboardLogger(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_train_step(engine, batch):\n",
    "    model.train()\n",
    "    \n",
    "    abstract = batch['abstract'].to(device)\n",
    "    claims = batch['claims'].to(device)\n",
    "    #people = batch['people']#.to(device)\n",
    "    #citing = batch['citing']#.to(device)\n",
    "    #embedded_cpc = batch['embedded_cpc']#.to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "\n",
    "    #label_embeddings = model.encode_label(subset_tokenized)\n",
    "    \n",
    "#     loss = 0\n",
    "#     for label_idx in range(NUM_LABELS):\n",
    "#         current_labels = labels[:, label_idx]\n",
    "#         hard_pairs = miner(patent_emb, current_labels)\n",
    "#         loss += loss_function(patent_emb, current_labels, hard_pairs)      \n",
    "\n",
    "    patent_emb = model.encode_patent(abstract=abstract, claims=claims)\n",
    "                                     #, embedded_cpc=embedded_cpc, people=people, citing=citing)    \n",
    "    predictions = model.predict_classes(patent_emb)\n",
    "    loss = classifier_function(predictions, labels)\n",
    "    loss.backward()\n",
    "    \n",
    "    if engine.state.iteration % ACCUM_STEPS == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "trainer = Engine(train_step)\n",
    "\n",
    "\n",
    "def validation_step(engine, batch):\n",
    "    model.eval()\n",
    "    abstract = batch['abstract']#.to(device)\n",
    "    claims = batch['claims']#.to(device)\n",
    "    #people = batch['people']#.to(device)\n",
    "    #citing = batch['citing']#.to(device)\n",
    "    #embedded_cpc = batch['embedded_cpc']#.to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        patent_emb = model.encode_patent(abstract=abstract, claims=claims)\n",
    "                                         #, embedded_cpc=embedded_cpc, people=people, citing=citing)\n",
    "        raw_predictions = model.predict_classes(patent_emb)\n",
    "        return raw_predictions, labels\n",
    "\n",
    "def thresholded_output_transform(output):\n",
    "    y_pred, y = output\n",
    "    y_pred = torch.round(torch.sigmoid(y_pred))\n",
    "    return y_pred, y\n",
    "\n",
    "#train_evaluator = Engine(validation_step)\n",
    "test_evaluator = Engine(validation_step)\n",
    "\n",
    "#Accuracy(is_multilabel=True, output_transform=thresholded_output_transform).attach(train_evaluator, 'train_accuracy')\n",
    "#Loss(classifier_function).attach(train_evaluator, 'train_loss')\n",
    "Accuracy(is_multilabel=True, output_transform=thresholded_output_transform).attach(test_evaluator, 'val_accuracy')\n",
    "Loss(classifier_function).attach(test_evaluator, 'val_loss')\n",
    "\n",
    "RunningAverage(output_transform=lambda x: x).attach(trainer, 'loss')\n",
    "pbar = ProgressBar(persist=True, bar_format=\"\")\n",
    "pbar.attach(trainer, ['loss'])\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED(every=100))\n",
    "def log_validation_results(engine):\n",
    "    test_evaluator.run(testing_loader)\n",
    "    metrics = test_evaluator.state.metrics\n",
    "    pbar.log_message(\n",
    "        \"Val Results - Epoch: {} \\nMetrics\\n{}\"\n",
    "        .format(engine.state.epoch, pprint.pformat(metrics)))\n",
    "\n",
    "trainer.run(training_loader, max_epochs=1)\n",
    "    \n",
    "# @trainer.on(Events.ITERATION_COMPLETED(every=log_interval))\n",
    "# def log_training_loss(trainer):\n",
    "#     print(\"Epoch[{}] Loss: {:.2f}\".format(trainer.state.epoch, trainer.state.output))\n",
    "\n",
    "\n",
    "# def log_validation_results(engine):\n",
    "#     validation_evaluator.run(valid_iterator)\n",
    "#     metrics = validation_evaluator.state.metrics\n",
    "#     avg_accuracy = metrics['accuracy']\n",
    "#     avg_bce = metrics['bce']\n",
    "#     pbar.log_message(\n",
    "#         \"Validation Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\"\n",
    "#         .format(engine.state.epoch, avg_accuracy, avg_bce))\n",
    "#     pbar.n = pbar.last_print_n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# log_interval = 10\n",
    "# @trainer.on(Events.ITERATION_COMPLETED(every=log_interval))\n",
    "# def log_validation_results(trainer):\n",
    "#     print(\"running validation\")\n",
    "#     evaluator.run(testing_loader)\n",
    "#     metrics = evaluator.state.metrics\n",
    "#     print(\"Validation Results - Epoch: {}  Avg accuracy: {:.2f}\"\n",
    "#           .format(trainer.state.epoch, metrics[\"accuracy\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
