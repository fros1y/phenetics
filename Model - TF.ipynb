{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum, strip_punctuation\n",
    "import torch\n",
    "import torchtext\n",
    "from sklearn.model_selection import *\n",
    "from torch import nn\n",
    "import gensim.downloader as api\n",
    "\n",
    "from torchtext.data import Field, LabelField\n",
    "from torchtext.data import BucketIterator\n",
    "from torchtext.datasets import IMDB\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import Freezer\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.pipeline import * \n",
    "from sklearn.compose import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.neural_network import *\n",
    "from sklearn.preprocessing import *\n",
    "from skorch.callbacks import ProgressBar\n",
    "from util import *\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import fse\n",
    "from fse.models import uSIF\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.utils import CallbackIOWrapper\n",
    "from skopt.space import Real, Integer\n",
    "from skopt import BayesSearchCV\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, TFAutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, do_lower_case=True, add_special_tokens=True,\n",
    "                                                max_length=128, pad_to_max_length=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences, tokenizer):\n",
    "    input_ids, input_masks, input_segments = [],[],[]\n",
    "    for sentence in tqdm(sentences):\n",
    "        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=128, pad_to_max_length=True, \n",
    "                                             return_attention_mask=True, return_token_type_ids=True)\n",
    "        input_ids.append(inputs['input_ids'])\n",
    "        input_masks.append(inputs['attention_mask'])\n",
    "        input_segments.append(inputs['token_type_ids'])        \n",
    "        \n",
    "    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig(dropout=0.2, attention_dropout=0.2)\n",
    "config.output_hidden_states = False\n",
    "transformer_model = TFAutoModel.from_pretrained(model, config = config)\n",
    "\n",
    "input_ids_in = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n",
    "input_masks_in = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32') \n",
    "\n",
    "embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\n",
    "X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n",
    "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
    "X = tf.keras.layers.Dense(50, activation='relu')(X)\n",
    "X = tf.keras.layers.Dropout(0.2)(X)\n",
    "X = tf.keras.layers.Dense(6, activation='sigmoid')(X)\n",
    "model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
    "\n",
    "for layer in model.layers[:3]:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "usif = uSIF(glove, workers=32, lang_freq=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_json(\"training_set.json.gz\", lines=True, orient=\"records\")\n",
    "testing_set = pd.read_json(\"testing_set.json.gz\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AnatomicalTarget',\n",
       " 'AnalysisAndModeling',\n",
       " 'Manufacturing',\n",
       " 'Imaging',\n",
       " 'SurgicalMethod',\n",
       " 'SpecificationofUse']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = list((set(tier1)-set([\"PersonalizedProduct\"]))&set(all_tiers_100))\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels = training_set[subset]\n",
    "testing_labels = testing_set[subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_default_settings = {\n",
    "    'lowercase': True, \n",
    "    'strip_accents': 'ascii',\n",
    "    #'stop_words' : stopwords,\n",
    "    'min_df': 5,\n",
    "    'max_df': 0.5,\n",
    "    #'ngram_range': (1,3)\n",
    "}\n",
    "\n",
    "transformer = ColumnTransformer([\n",
    "     ('top_terms', CountVectorizer(analyzer=iden, binary=True, min_df=2), 'top_terms'),\n",
    "     ('cited_by', CountVectorizer(analyzer=iden, binary=True, min_df=2), 'cited_by'),\n",
    "     ('inventors', CountVectorizer(analyzer=iden, binary=True, min_df=2), 'inventors'),\n",
    "     ('citations', CountVectorizer(analyzer=citations_split, binary=True, min_df=2), 'citations'),\n",
    "     ('similar_patents', CountVectorizer(analyzer=iden, binary=True, min_df=2), 'similar_patents'),\n",
    "     ('cpc', CountVectorizer(analyzer=cpc_split, binary=True, min_df=2), 'cpc_codes'),\n",
    "     ('embedding_v1', Extract(), 'embedding_v1'),\n",
    "     ('usif_abstract', Embedder(usif), 'abstract'),\n",
    "     ('usif_claims', Embedder(usif), 'claims'),\n",
    "     ('usif_description', Embedder(usif), 'description'),\n",
    "     ('abstract_tfidf', TfidfVectorizer(**tfidf_default_settings), 'abstract'),\n",
    "     ('claims_tfidf', TfidfVectorizer(**tfidf_default_settings), 'claims'),\n",
    "     ('description_tfidf', TfidfVectorizer(**tfidf_default_settings), 'description')\n",
    "    ], verbose=False, n_jobs=-1)\n",
    "\n",
    "\n",
    "\n",
    "transformer_grid = {\n",
    "#     'top_terms__min_df': (1, 5),\n",
    "#     'top_terms__max_df': Real(0.1, 1.0),\n",
    "#     'cited_by__min_df': (1, 5),\n",
    "#     'cited_by__max_df': Real(0.1, 1.0),\n",
    "#     'inventors__min_df': (1, 20),\n",
    "#     'inventors__max_df': Real(0.1, 1.0),\n",
    "#     'citations__min_df': (1, 20),\n",
    "#     'citations__max_df': Real(0.1, 1.0),\n",
    "#     'similar_patents__min_df': (1, 20),\n",
    "#     'similar_patents__max_df': Real(0.1, 1.0),\n",
    "#     'cpc__min_df': (1, 20),\n",
    "#     'cpc__max_df': Real(0.1, 1.0),\n",
    "#     'abstract_tfidf__min_df': (1, 20),\n",
    "#     'abstract_tfidf__max_df': Real(0.1, 1.0),\n",
    "#     'claims_tfidf__min_df': (1, 20),\n",
    "#     'claims_tfidf__max_df': Real(0.1, 1.0),\n",
    "#     'description_tfidf__min_df': (1, 20),\n",
    "#     'description_tfidf__max_df': Real(0.1, 1.0),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('transformer', transformer),\n",
    "                       ('svd', TruncatedSVD(random_state=42, n_components=1024)),\n",
    "                       #('dummy', OneVsRestClassifier(DummyClassifier()))\n",
    "                       #('svc', OneVsRestClassifier(SVC(random_state=42), n_jobs=-1))\n",
    "                       ('rf',  RandomForestClassifier(n_jobs=-1, random_state=42))\n",
    "                       #('lr', OneVsRestClassifier(LogisticRegression(n_jobs=-1), n_jobs=-1))\n",
    "                       #('cat', OneVsRestClassifier(CatBoostClassifier(verbose=True)))\n",
    "                       #('knn', KNeighborsClassifier(n_jobs=-1))\n",
    "                      ], \n",
    "                verbose=True,\n",
    "                memory=\"cachedir/\")\n",
    "\n",
    "model_grid = {\n",
    "    'svd__n_components': (64, 1024*16),\n",
    "    'rf__max_depth': (10, 1000), #[10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 125, 150, 175, 200, 250, 300, 350, 400, None],\n",
    "    'rf__min_samples_leaf': (1, 12),\n",
    "    'rf__min_samples_split': (2, 12),\n",
    "    'rf__n_estimators': (5, 1000)          \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.fit(training_set, training_labels)\n",
    "# predictions = pipe.predict(testing_set)\n",
    "# print(classification_report(testing_labels, predictions, target_names=subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svd__n_components': (64, 16384),\n",
       " 'rf__max_depth': (10, 1000),\n",
       " 'rf__min_samples_leaf': (1, 12),\n",
       " 'rf__min_samples_split': (2, 12),\n",
       " 'rf__n_estimators': (5, 1000)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {**model_grid, **({f\"transformer__{k}\": v for k,v in transformer_grid.items()})}\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ada00b7bfb149f98dc40f6abd2a0b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 31 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "search = BayesSearchCV(pipe, param_grid, n_iter=50, n_points=3, pre_dispatch=36, refit=True, cv=3, verbose=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "with tqdm(total=search.total_iterations) as pbar:\n",
    "    def on_step(optim_result):\n",
    "        print(optim_result)\n",
    "        pbar.update(9)\n",
    "        return False\n",
    "    search.fit(training_set, training_labels, callback=on_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untuned RF with uSIF (Glove)\n",
    "\n",
    "#      SurgicalMethod       1.00      0.03      0.05        40\n",
    "#    AnatomicalTarget       0.68      0.95      0.79       164\n",
    "#             Imaging       0.55      0.78      0.64       133\n",
    "#  SpecificationofUse       0.50      0.04      0.07        79\n",
    "# AnalysisAndModeling       0.20      0.01      0.02        84\n",
    "#       Manufacturing       0.57      0.10      0.16        83\n",
    "\n",
    "#           micro avg       0.61      0.47      0.53       583\n",
    "#           macro avg       0.58      0.32      0.29       583\n",
    "#        weighted avg       0.56      0.47      0.41       583\n",
    "#         samples avg       0.63      0.50      0.52       583"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
